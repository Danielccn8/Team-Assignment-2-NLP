{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32yCsRUo8H33"
   },
   "source": [
    "# 2025 CITS4012 Project\n",
    "\n",
    "_Make sure you change the file name with your group id._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "CUDA version: None\n",
      "Number of GPUs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import copy\n",
    "import random\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "if not nltk.download('punkt'):\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "if not nltk.download('stopwords'):\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "# Stemmer and Lemmatizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "if not nltk.download('wordnet'):\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sys.path.append(os.path.abspath('..')) \n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "sys.path.append(os.path.abspath('../')) # Points to the current folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "# Helper functions for training\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **We build a Configuration class to have all the hyperparameters together**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class construct for hyperparameters \n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.min_word_len = 3\n",
    "        self.max_word_len = 64\n",
    "        self.batch_size = 64\n",
    "        self.embedding_dim = 64\n",
    "        self.hidden_dim = 64\n",
    "        self.max_combined_len = 130 #to join premise + hypothesis + cls + sep \n",
    "        self.patience = 5\n",
    "        #Embedding\n",
    "        self.embedding_method = \"\"\n",
    "        self.emb_lr = 0.0005\n",
    "        self.emb_epoch = 3\n",
    "        self.emb_window_size = 5\n",
    "        # Models\n",
    "        self.n_heads = 2\n",
    "        self.n_layers = 1\n",
    "        self.dropout = 0.3\n",
    "        self.num_classes = 2\n",
    "        self.lr = 0.01\n",
    "        self.wd = 0.005\n",
    "        \n",
    "        # self.train_num_epoch = 1\n",
    "        # self.train_num_epoch = 5 # took 3 -4 hrs\n",
    "        self.train_num_epoch = 3 #10 too long\n",
    "        \n",
    "        self.label_smoothing = 0.05\n",
    "\n",
    "    def display(self): # Display all the config parameters \n",
    "        print(\"=\" * 60)\n",
    "        print(\"CONFIGURATION PARAMETERS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nEmbedding Parameters:\")\n",
    "        print(f\"  Method: {self.embedding_method}\")\n",
    "        print(f\"  Max length: {self.max_word_len}\")\n",
    "        print(f\"  Learning rate: {self.emb_lr}\")\n",
    "        print(f\"  Embedding size: {self.embedding_dim}\")\n",
    "        print(f\"  Epochs: {self.emb_epoch}\")\n",
    "        print(f\"  Window size: {self.emb_window_size}\")\n",
    "        \n",
    "        print(\"\\nTransformer Parameters:\")\n",
    "        print(f\"  Batch size: {self.batch_size}\")\n",
    "        print(f\"  Number of heads: {self.n_heads}\")\n",
    "        print(f\"  Number of layers: {self.n_layers}\")\n",
    "        print(f\"  Dropout: {self.dropout}\")\n",
    "        print(f\"  Number of classes: {self.num_classes}\")\n",
    "        print(f\"  Learning rate: {self.lr}\")\n",
    "        print(f\"  Weight decay: {self.wd}\")\n",
    "        print(f\"  Training epochs: {self.train_num_epoch}\")\n",
    "\n",
    "        print(f\"  Label smoothing: {self.label_smoothing}\")\n",
    "        print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCybYoGz8YWQ"
   },
   "source": [
    "# Readme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6po98qVA8bJD"
   },
   "source": [
    "# 1.Dataset Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# These are common English contractions.\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
    "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "# Helper function for lemmatization with POS tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no match\n",
    "    \n",
    "# Allowed POS tags for filtering (example: nouns, verbs, adjectives, adverbs)\n",
    "allowed_pos_tags = {'NN', 'NNS', 'NNP', 'NNPS',   # Nouns\n",
    "                    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Verbs\n",
    "                    'JJ', 'JJR', 'JJS',           # Adjectives\n",
    "                    'RB', 'RBR', 'RBS'}           # Adverbs\n",
    "\n",
    "# Filter the desired POS tags\n",
    "def filter_tokens_by_pos(tokens):\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    filtered = [word for word, tag in tagged_tokens if tag in allowed_pos_tags]\n",
    "    return filtered\n",
    "\n",
    "def clean_dataset(dataset, min_word_len, max_word_len, method=['stem, lemmatize'], pos_filter=False, stop_w=False, clean_nums = False):\n",
    "    cleaned_premises = []\n",
    "    cleaned_hypotheses = []\n",
    "    cleaned_labels = []\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        premise = row['premise']\n",
    "        hypothesis = row['hypothesis']\n",
    "        label = row['label']\n",
    "\n",
    "        # Lowercase\n",
    "        premise = premise.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "        # Expand contractions\n",
    "        for contraction, full_form in contraction_dict.items():\n",
    "            premise = premise.replace(contraction, full_form)\n",
    "            hypothesis = hypothesis.replace(contraction, full_form)\n",
    "\n",
    "        # Remove punctuation/special chars\n",
    "        premise = re.sub(r'[^a-zA-Z0-9\\s.-]', ' ', premise)\n",
    "        hypothesis = re.sub(r'[^a-zA-Z0-9\\s.-]', ' ', hypothesis)\n",
    "\n",
    "        # Replace underscores/hyphens with spaces, then normalize whitespace\n",
    "        premise = re.sub(r'[-–—_]+', ' ', premise)\n",
    "        hypothesis = re.sub(r'[-–—_]+', ' ', hypothesis)\n",
    "\n",
    "        # Normalize whitespace\n",
    "        premise = re.sub(r'\\s+', ' ', premise).strip()\n",
    "        hypothesis = re.sub(r'\\s+', ' ', hypothesis).strip()\n",
    "\n",
    "        # Tokenization\n",
    "        premise_tokens = word_tokenize(premise)\n",
    "        hypothesis_tokens = word_tokenize(hypothesis)\n",
    "\n",
    "        # Drop . that are not decimals\n",
    "        premise_tokens = [t for t in premise_tokens if not (t == '…' or re.fullmatch(r'\\.+', t))]\n",
    "        hypothesis_tokens = [t for t in hypothesis_tokens if not (t == '…' or re.fullmatch(r'\\.+', t))]\n",
    "\n",
    "        # # Replace numbers with '<NUM>'\n",
    "        if clean_nums is True:\n",
    "            premise_tokens = ['[NUM]' if any(char.isdigit() for char in word) else word for word in premise_tokens]\n",
    "            hypothesis_tokens = ['[NUM]' if any(char.isdigit() for char in word) else word for word in hypothesis_tokens]\n",
    "\n",
    "\n",
    "        # Stemming/Lemmatization\n",
    "        if 'stem' in method:\n",
    "            stemmer = PorterStemmer()\n",
    "            premise_tokens = [stemmer.stem(word) for word in premise_tokens]\n",
    "            hypothesis_tokens = [stemmer.stem(word) for word in hypothesis_tokens]\n",
    "        elif 'lemmatize' in method:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            premise_pos_tags = pos_tag(premise_tokens)\n",
    "            hypothesis_pos_tags = pos_tag(hypothesis_tokens)\n",
    "            premise_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in premise_pos_tags]\n",
    "            hypothesis_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in hypothesis_pos_tags]\n",
    "\n",
    "        # POS Filtering\n",
    "        if pos_filter:\n",
    "            premise_tokens = filter_tokens_by_pos(premise_tokens)\n",
    "            hypothesis_tokens = filter_tokens_by_pos(hypothesis_tokens)\n",
    "\n",
    "        # Remove stop words\n",
    "        if stop_w:\n",
    "            premise_tokens = [word for word in premise_tokens if word not in stop_words]\n",
    "            hypothesis_tokens = [word for word in hypothesis_tokens if word not in stop_words]\n",
    "\n",
    "        # Now check token length AFTER cleaning\n",
    "        if (min_word_len <= len(premise_tokens) <= max_word_len and\n",
    "            min_word_len <= len(hypothesis_tokens) <= max_word_len):\n",
    "            cleaned_premises.append(premise_tokens)\n",
    "            cleaned_hypotheses.append(hypothesis_tokens)\n",
    "            cleaned_labels.append(label)\n",
    "        # else: skip row\n",
    "\n",
    "    # Build DataFrame from all cleaned token lists\n",
    "    new_dataset = pd.DataFrame({\n",
    "        'premise': cleaned_premises,\n",
    "        'hypothesis': cleaned_hypotheses,\n",
    "        'label': cleaned_labels\n",
    "    })\n",
    "\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Make the Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [pluto, rotates, axis, every, 6.39, earth, day...\n",
       "1    [glenn, per, day, earth, rotates, axis, earth,...\n",
       "2    [geysers, periodic, gush, hot, water, surface,...\n",
       "3    [facts, liquid, water, droplets, changed, invi...\n",
       "4    [comparison, earth, rotates, axis, per, day, r...\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data\n",
    "train_df = pd.read_json('train.json')\n",
    "test_df = pd.read_json('test.json')\n",
    "validation_df = pd.read_json('validation.json')\n",
    "\n",
    "# Clean datasets. MAX_WORD_LENGTH set to 64 to remove very long texts\n",
    "clean_train_dataset = clean_dataset(train_df, hyperparameters.min_word_len, hyperparameters.max_word_len, stop_w=True, method=[], pos_filter=False)\n",
    "clean_test_dataset = clean_dataset(test_df, hyperparameters.min_word_len, hyperparameters.max_word_len,stop_w=True, method=[], pos_filter= False )\n",
    "clean_validation_dataset = clean_dataset(validation_df, hyperparameters.min_word_len, hyperparameters.max_word_len,stop_w=True, method=[], pos_filter= False )\n",
    "\n",
    "# Combine clean premises and hypotheses\n",
    "clean_t_dataset = clean_train_dataset['premise'] + clean_train_dataset['hypothesis']\n",
    "clean_t_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique word list from the cleaned dataset\n",
    "unique_words = set()\n",
    "for sentence in clean_t_dataset: # Both Premise and Hypothesis \n",
    "    for word in sentence:\n",
    "        unique_words.add(word)\n",
    "        \n",
    "unique_words_list = sorted(list(unique_words))\n",
    "\n",
    "# Make dictionary of words and indices\n",
    "word2id = {w:i for i,w in enumerate(unique_words_list)}\n",
    "id2word = {i:w for i,w in enumerate(unique_words_list)}\n",
    "\n",
    "\n",
    "# Add special tokens to use later\n",
    "SPECIAL_TOKENS = ['[PAD]','[UNK]','[CLS]','[SEP]'] \n",
    "for tok in SPECIAL_TOKENS:\n",
    "    if tok not in word2id:\n",
    "        idx = len(word2id)\n",
    "        word2id[tok] = idx\n",
    "        id2word[idx] = tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_ID = word2id['[SEP]']\n",
    "UNK_ID = word2id['[UNK]']\n",
    "PAD_ID = word2id['[PAD]']\n",
    "CLS_ID = word2id['[CLS]']\n",
    "\n",
    "\n",
    "def prepare_indexed_data(df, word2id, max_len):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    label_map = {'neutral': 0, 'entails': 1}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        premise_toks = row['premise']\n",
    "        hypothesis_toks = row['hypothesis']\n",
    "        label = row['label']\n",
    "\n",
    "        # Add special tokens \n",
    "        tokens = [CLS_ID] \\\n",
    "                + [word2id.get(w,UNK_ID) for w in premise_toks] \\\n",
    "                + [SEP_ID] \\\n",
    "                + [word2id.get(w,UNK_ID) for w in hypothesis_toks] \n",
    "        # Truncate\n",
    "        tokens = tokens[:max_len] #to combine len of hypothesis and premise, plus cls and sep  \n",
    "\n",
    "        # Attention mask \n",
    "        attn = [1] * len(tokens)\n",
    "\n",
    "        # Pad\n",
    "        pad_len = max_len - len(tokens) # To fill the [PAD]\n",
    "        if pad_len > 0:\n",
    "            tokens += [PAD_ID] * pad_len\n",
    "            attn += [0] * pad_len # FLag positions as padding \n",
    "\n",
    "        input_ids.append(tokens)\n",
    "        attention_mask.append(attn)\n",
    "        labels.append(label_map[label])\n",
    "\n",
    "    return (torch.LongTensor(input_ids),\n",
    "            np.array(input_ids),\n",
    "            torch.LongTensor(attention_mask),\n",
    "            torch.LongTensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: torch.Size([22659, 130])\n",
      "Shape of train_masks: torch.Size([22659, 130])\n",
      "Shape of y_train: torch.Size([22659])\n",
      "\n",
      "--- Example ---\n",
      "First example real length (mask sum): 15\n",
      "First example PAD count: 115\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build train/test\n",
    "x_train, plain_train, train_masks, y_train = prepare_indexed_data(clean_train_dataset, word2id, hyperparameters.max_combined_len)\n",
    "x_test,  plain_test, test_masks,  y_test  = prepare_indexed_data(clean_test_dataset,  word2id, hyperparameters.max_combined_len)\n",
    "x_valid,  plain_valid, valid_masks,  y_valid  = prepare_indexed_data(clean_validation_dataset,  word2id, hyperparameters.max_combined_len)\n",
    "\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of train_masks:\", train_masks.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "\n",
    "# Sanity Check \n",
    "i = 0\n",
    "print(\"\\n--- Example ---\")\n",
    "print(\"First example real length (mask sum):\", int(train_masks[i].sum()))\n",
    "print(\"First example PAD count:\", int((train_masks[i]==0).sum()))\n",
    "\n",
    "# Check we didn't cut off too much\n",
    "sns.histplot(np.sum(train_masks.numpy(), axis=1), bins = 16)\n",
    "plt.axvline(hyperparameters.max_word_len, color='red')\n",
    "plt.show()\n",
    "\n",
    "# Prepare the datasets for model training\n",
    "train_ds = TensorDataset(x_train, train_masks, y_train)\n",
    "test_ds = TensorDataset(x_test, test_masks,y_test)\n",
    "valid_ds = TensorDataset(x_valid, valid_masks,y_valid)\n",
    "\n",
    "# Dataloaders (shuffle for train)\n",
    "train_loader = DataLoader(train_ds, batch_size=hyperparameters.batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=hyperparameters.batch_size, shuffle=False, num_workers=0)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=hyperparameters.batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Make an embedding model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Skip Gram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip gram model with two embedding layers, for center words and context words\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        # Embedding layer for the center \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, sparse=True)\n",
    "        # Embedding layer for the context \n",
    "        self.output = nn.Embedding(vocab_size, embedding_size, sparse=True)\n",
    "\n",
    "    def forward(self, center, context, negative):\n",
    "        v_c = self.embeddings(center)    \n",
    "        v_o = self.output(context)       \n",
    "        v_neg = self.output(negative)    \n",
    "        # Positive Score\n",
    "        pos_logits = (v_c * v_o).sum(dim=1)\n",
    "\n",
    "        # Negative score\n",
    "        neg_logits = (v_c.unsqueeze(1) * v_neg).sum(dim=2)\n",
    "\n",
    "        return pos_logits, neg_logits\n",
    "    \n",
    "\n",
    "class SkipGramPairsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        return int(center), int(context)\n",
    "        \n",
    "\n",
    "def create_skip_gram_pairs(corpus, word2id, window_size):\n",
    "\n",
    "    skip_grams = []\n",
    "    for sentence in corpus:\n",
    "        sentence_len = len(sentence)\n",
    "        if sentence_len < 2:\n",
    "            continue\n",
    "\n",
    "        for i, center_word in enumerate(sentence):\n",
    "            if center_word not in word2id:\n",
    "                continue\n",
    "            center_id = word2id[center_word]\n",
    "\n",
    "            # Dynamic window size\n",
    "            dynamic_window = random.randint(1, window_size)\n",
    "            start = max(0, i - dynamic_window)\n",
    "            end = min(sentence_len, i + dynamic_window + 1)\n",
    "\n",
    "            for j in range(start, end):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                context_word = sentence[j]\n",
    "                if context_word in word2id:\n",
    "                    context_id = word2id[context_word]\n",
    "                    skip_grams.append((center_id, context_id))\n",
    "    return skip_grams\n",
    "\n",
    "# Main training process for the Skip-Gram \n",
    "def train_skip_gram(dataset, word2id, id2word, vector_size, window, epochs, batch_size, lr, neg_samples=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    vocab_size = len(word2id)\n",
    "\n",
    "    # Create pairs \n",
    "    skip_gram_pairs = create_skip_gram_pairs(dataset, word2id, window)\n",
    "    sg_dataset = SkipGramPairsDataset(skip_gram_pairs)\n",
    "    loader = DataLoader(sg_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    sg_model = SkipGramModel(vocab_size, vector_size).to(device)\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SparseAdam(sg_model.parameters(), lr=lr)\n",
    "\n",
    "    # Calculate word frequencies for the unigram distribution\n",
    "    word_counts = Counter(word for sentence in dataset for word in sentence if word in word2id)\n",
    "    counts_array = np.array([word_counts[id2word[i]] for i in range(vocab_size)], dtype=np.float32)\n",
    "\n",
    "    sampling_probs = torch.from_numpy(counts_array ** 0.75).to(device)\n",
    "    sampling_probs /= sampling_probs.sum()\n",
    "\n",
    "    # Train \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (centers, contexts) in enumerate(loader):\n",
    "            centers, contexts = centers.to(device), contexts.to(device)\n",
    "            batch_len = centers.size(0)\n",
    "\n",
    "            neg_context_ids = torch.multinomial(sampling_probs, num_samples=batch_len * neg_samples, replacement=True).view(batch_len, neg_samples)\n",
    "\n",
    "            pos_logits, neg_logits = sg_model(centers, contexts, neg_context_ids)\n",
    "\n",
    "            pos_labels = torch.ones_like(pos_logits, device=device)\n",
    "            neg_labels = torch.zeros_like(neg_logits, device=device)\n",
    "\n",
    "            loss = bce_loss(pos_logits, pos_labels) + bce_loss(neg_logits, neg_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "    print(\"Custom Skip-Gram training complete.\")\n",
    "\n",
    "    trained_embeddings = sg_model.embeddings.weight.cpu().detach().numpy()\n",
    "    \n",
    "    # Create a KeyedVectors instance as gensim \n",
    "    kv = KeyedVectors(vector_size=vector_size)\n",
    "    kv.add_vectors([id2word[i] for i in range(vocab_size)], trained_embeddings)\n",
    "    \n",
    "    return kv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim FastTesxt Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 5.1185\n",
      "Epoch 2/3, Loss: 3.7711\n",
      "Epoch 3/3, Loss: 3.1209\n",
      "Custom Skip-Gram training complete.\n",
      "Embedding matrix created with shape: (20189, 64)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fast_text_model = FastText(clean_t_dataset,\n",
    "                           vector_size=hyperparameters.embedding_dim,\n",
    "                           window=hyperparameters.emb_window_size,\n",
    "                           sg=1,\n",
    "                           epochs=hyperparameters.emb_epoch\n",
    "                           )\n",
    "\n",
    "skip_gram_model = train_skip_gram(clean_t_dataset,\n",
    "                                  word2id,\n",
    "                                  id2word,\n",
    "                                  hyperparameters.embedding_dim,\n",
    "                                  window=hyperparameters.emb_window_size,\n",
    "                                  epochs=hyperparameters.emb_epoch,\n",
    "                                  batch_size=hyperparameters.batch_size,\n",
    "                                  lr=hyperparameters.emb_lr)\n",
    "\n",
    "#if skip gram model \n",
    "#fast_text_model.most_similar('saturn', topn=10)\n",
    "#If Fast Text model \n",
    "fast_text_model.wv.most_similar('saturn', topn=10)\n",
    "\n",
    "def build_embedding_matrix(word2id, pretrained_vectors, embedding_size):\n",
    "    vocab_size = len(word2id)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size), dtype=np.float32)\n",
    "\n",
    "    # Fill the matrix with the pre-trained vectors\n",
    "    for word, idx in word2id.items():\n",
    "        if word == '[PAD]':# If [PAD] ignore \n",
    "            continue\n",
    "        try:\n",
    "            vec = pretrained_vectors[word] #Checks the size of the vector of each word\n",
    "            if vec.shape[0] == embedding_size:\n",
    "                embedding_matrix[idx] = vec.astype(np.float32)\n",
    "            else:\n",
    "                # fallback if dims don’t match\n",
    "                embedding_matrix[idx] = np.random.normal(0.0, 0.02, size=(embedding_size,)).astype(np.float32)\n",
    "        except KeyError:\n",
    "            # special tokens or OOV start them with random values \n",
    "            embedding_matrix[idx] = np.random.normal(0.0, 0.02, size=(embedding_size,)).astype(np.float32)\n",
    "\n",
    "    print(\"Embedding matrix created with shape:\", embedding_matrix.shape)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "#If Skip gram model remove .wv\n",
    "#embedding_matrix = build_embedding_matrix(word2id, fast_text_model, hyperparameters.embedding_dim)\n",
    "embedding_matrix = build_embedding_matrix(word2id, fast_text_model.wv, hyperparameters.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " idx  token                 first 5 dims\n",
      "--------------------------------------------------------------------------------\n",
      "19020  typical               [0.1117967  0.17780373 0.2747581  0.12648955 0.14168796]\n",
      "3158  cancers               [ 0.1956963   0.02108056  0.5093111   0.5973254  -0.47045377]\n",
      "19550  view                  [-0.16676852 -0.04592191  0.24660091  0.3629931  -0.30779716]\n",
      "12242  multistep             [ 0.0232805  -0.0611108   0.13324408  0.4321485  -0.35658216]\n",
      "16522  sexuality             [ 0.12530969 -0.09491722  0.309708    0.41617957 -0.27061027]\n"
     ]
    }
   ],
   "source": [
    "# Look at embedding matrix with random words\n",
    "candidates = [i for i in range(embedding_matrix.shape[0]) \n",
    "              if id2word.get(i) not in (None, \"[PAD]\")]\n",
    "\n",
    "rng = np.random\n",
    "chosen = rng.choice(candidates, size=min(5, len(candidates)), replace=False)\n",
    "\n",
    "print(f\"{'idx':>4}  {'token':<20}  first 5 dims\")\n",
    "print(\"-\" * 80)\n",
    "for i in chosen:\n",
    "    token = id2word[i]\n",
    "    print(f\"{i:>4}  {token:<20}  {embedding_matrix[i][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FA2ao2l8hOg"
   },
   "source": [
    "# 2. Model Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we have collected following parameters from the pre-processing:\n",
    "\n",
    "- VOCAB_SIZE\n",
    "- MAX_SEQ_LEN\n",
    "- BATCH_SIZE\n",
    "- embedding_dim\n",
    "  We have collected the objects:\n",
    "- embedding_matrix\n",
    "- fast_text_model\n",
    "- clean_train_dataset, clean_test_dataset, clean_validation_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We build a Configuration class to have all the hyperparameters together**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model A: RNN with Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LSTM With Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=2, dropout=0.3, num_layers = 2):\n",
    "        super(LSTMAttentionModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None, return_weights = False):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        context = self.dropout(context)\n",
    "\n",
    "        logits = self.fc(context)\n",
    "        if return_weights:\n",
    "            return logits, attn_weights\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gru with Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, unk_prob=0.1, num_layers = 2):\n",
    "        super(GRUAttentionModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 2)\n",
    "        self.unk_id = UNK_ID\n",
    "        self.unk_prob = unk_prob\n",
    "\n",
    "    def forward(self, x, mask=None, return_weights = False):\n",
    "        # Only replace tokens during training\n",
    "        if self.training and self.unk_prob > 0:\n",
    "            mask = (x != PAD_ID) & (x != SEP_ID) & (x != CLS_ID) & (x != UNK_ID)\n",
    "            replace_mask = (torch.rand_like(x.float()) < self.unk_prob) & mask\n",
    "            x = torch.where(replace_mask, torch.full_like(x, self.unk_id), x)\n",
    "        # Rest of the forward pass...\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        gru_out = self.norm(gru_out)\n",
    "        attn_weights = torch.softmax(self.attention(gru_out), dim=1)\n",
    "        context = torch.sum(attn_weights * gru_out, dim=1)\n",
    "        context = self.dropout(context)\n",
    "\n",
    "        logits = self.fc(context)\n",
    "        # Possibiity to return the attention weights \n",
    "        if return_weights:\n",
    "            return logits, attn_weights\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model B: Transformer with Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding for the transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,max_len = 512, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0,max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.pe[:, :x.size(1), :] \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_classes, n_heads, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.d_model = embed_dim\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False, padding_idx=word2id['[PAD]'])\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=hyperparameters.max_combined_len, dropout=hyperparameters.dropout)\n",
    "\n",
    "        # Transformer Encoder Layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=n_heads,\n",
    "            dropout=hyperparameters.dropout,\n",
    "            dim_feedforward=(embed_dim*4),\n",
    "            batch_first=True \n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=n_layers)\n",
    "\n",
    "        # Final Classification Head\n",
    "        #self.classifier = nn.Linear(embed_dim,num_classes)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        padding_mask = (src_mask == 0)\n",
    "\n",
    "        # Apply embedding and positional encoding\n",
    "        embedded = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "\n",
    "        # Pass trough the transformer encoder\n",
    "        encoded = self.transformer_encoder(pos_encoded,src_key_padding_mask = padding_mask)\n",
    "\n",
    "        # Use the output of the [CLS] token for classification\n",
    "        cls_output = encoded[:,0,:]\n",
    "\n",
    "        # Get final logits from the classifier\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "    \n",
    "    # Get attention from the first layer\n",
    "    def get_attention_weights(self, src, src_mask):\n",
    "        # Get embeddings and positional encoding\n",
    "        padding_mask = (src_mask == 0)\n",
    "        embedded = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "\n",
    "        # Access the FIRST encoder layer \n",
    "        first_encoder_layer = self.transformer_encoder.layers[0]\n",
    "\n",
    "        # Call its self-attention module with need_weights=True\n",
    "        _, attn_weights = first_encoder_layer.self_attn(\n",
    "            pos_encoded, pos_encoded, pos_encoded,\n",
    "            key_padding_mask=padding_mask,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=False\n",
    "        )\n",
    "\n",
    "        return attn_weights.cpu().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model C: Wide and Deep RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model seeks to build on Model A by splitting into three components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUThreeStreamNLI(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, wide_layers=2, prem_layers=2, hypo_layers=2, num_fc_layers=2, dropout=0.3, unk_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Encoders\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True,\n",
    "                          bidirectional=True, num_layers=wide_layers, dropout=dropout if wide_layers > 1 else 0.0)\n",
    "        self.prem_encoder = nn.GRU(\n",
    "            embedding_dim, hidden_dim, num_layers=prem_layers,\n",
    "            batch_first=True, bidirectional=True, dropout=dropout if prem_layers > 1 else 0.0\n",
    "        )\n",
    "        self.hypo_encoder = nn.GRU(\n",
    "            embedding_dim, hidden_dim, num_layers=hypo_layers,\n",
    "            batch_first=True, bidirectional=True, dropout=dropout if hypo_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
    "\n",
    "\n",
    "        # Self-attention for premise and hypothesis\n",
    "        self.self_attn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        # Cross-attention for the wide stream\n",
    "        self.cross_attn_query = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.cross_attn_key = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.cross_attn_value = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "        # Fusion and classification layers\n",
    "        fusion_dim = hidden_dim * 5\n",
    "        fc_layers = []\n",
    "        in_dim = fusion_dim\n",
    "        out_dim = hidden_dim * 2\n",
    "\n",
    "        for i in range(num_fc_layers):\n",
    "            fc_layers.extend([\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            # for next layer\n",
    "            in_dim = out_dim\n",
    "            # Optionally shrink layer width gradually:\n",
    "            out_dim = max(hidden_dim, out_dim // 2)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "        self.classifier = nn.Linear(in_dim, 2)\n",
    "        self.unk_id = UNK_ID\n",
    "        self.unk_prob = unk_prob\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Token corruption (word dropout)\n",
    "        if self.training and self.unk_prob > 0:\n",
    "            token_mask = (x != PAD_ID) & (x != SEP_ID) & (x != CLS_ID) & (x != UNK_ID)\n",
    "            replace_mask = (torch.rand_like(x.float()) < self.unk_prob) & token_mask\n",
    "            x = torch.where(replace_mask, torch.full_like(x, self.unk_id), x)\n",
    "\n",
    "        # Split into premise/hypothesis \n",
    "        sep_positions = (x == SEP_ID).int().argmax(dim=1)\n",
    "        batch_idx = torch.arange(x.size(0), device=x.device)\n",
    "        token_range = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        premise_mask = token_range < sep_positions.unsqueeze(1)\n",
    "        hypothesis_mask = token_range > sep_positions.unsqueeze(1)\n",
    "        premise = torch.where(premise_mask, x, PAD_ID)\n",
    "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
    "\n",
    "        #  Embeddings\n",
    "        embedded = self.embedding(x)\n",
    "        emb_prem = self.embedding(premise)\n",
    "        emb_hypo = self.embedding(hypothesis)\n",
    "\n",
    "        # Encode all three streams\n",
    "        full_out, _ = self.gru(embedded)\n",
    "        full_out = self.norm(full_out)\n",
    "        prem_out, _ = self.prem_encoder(emb_prem)\n",
    "        prem_out = self.norm(prem_out)\n",
    "        hypo_out, _ = self.hypo_encoder(emb_hypo)\n",
    "        hypo_out = self.norm(hypo_out)\n",
    "\n",
    "        # Wide stream (cross-attention over full sequence)\n",
    "        Q = self.cross_attn_query(full_out)\n",
    "        K = self.cross_attn_key(full_out)\n",
    "        V = self.cross_attn_value(full_out)\n",
    "        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / (Q.size(-1) ** 0.5)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        cross_context = torch.bmm(attn_weights, V).mean(dim=1)  # (B, hidden_dim)\n",
    "\n",
    "        # Deep streams (self-attention pooling)\n",
    "        def self_attention_pooling(out):\n",
    "            weights = F.softmax(self.self_attn(out), dim=1)  # (B, L, 1)\n",
    "            return torch.sum(weights * out, dim=1)           # (B, 2H)\n",
    "\n",
    "        prem_context = self_attention_pooling(prem_out)\n",
    "        hypo_context = self_attention_pooling(hypo_out)\n",
    "\n",
    "        # Fusion\n",
    "        fused = torch.cat([cross_context, prem_context, hypo_context], dim=1)\n",
    "        fused = self.fc_layers(fused)\n",
    "        logits = self.classifier(fused)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contain_UNK = np.zeros(len(plain_valid), dtype=bool)\n",
    "for i, sent in enumerate(plain_valid):\n",
    "    if UNK_ID in sent:\n",
    "        contain_UNK[i] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),  \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence_output, query=None):\n",
    "        if query is None:\n",
    "            query = sequence_output.mean(dim=1)  \n",
    "        \n",
    "        query_expanded = query.unsqueeze(1).expand(-1, sequence_output.size(1), -1)\n",
    "        \n",
    "        combined = torch.cat([sequence_output, query_expanded], dim=-1)\n",
    "        \n",
    "        scores = self.attention_net(combined)  \n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.sum(weights * sequence_output, dim=1)\n",
    "        \n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cross_attention(source_seq, target_seq):\n",
    "    scores = torch.bmm(source_seq, target_seq.transpose(1, 2))\n",
    "    scores = scores / math.sqrt(source_seq.size(-1))\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    context = torch.bmm(weights, target_seq)\n",
    "    return context, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence, padding_mask=None):\n",
    "        output, weights = self.mha(\n",
    "            sequence, sequence, sequence,\n",
    "            key_padding_mask=padding_mask\n",
    "        )\n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimented different Attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate to GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUAdditiveAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, \n",
    "                         bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "        # Call additive attention external function\n",
    "        self.attention = AdditiveAttention(hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None, return_weights=False):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        context, attn_weights = self.attention(gru_out)\n",
    "        context = self.dropout(context)\n",
    "        logits = self.fc(context)\n",
    "        \n",
    "        if return_weights:\n",
    "            return logits, attn_weights\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCrossAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.premise_gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, \n",
    "                                 bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.hypothesis_gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, \n",
    "                                    bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 4, 2)  # hypo_pooled + cross_pooled\n",
    "    \n",
    "    def forward(self, x, mask=None, return_weights=False):\n",
    "        # Split input using existing SEP_ID logic\n",
    "        sep_positions = (x == SEP_ID).int().argmax(dim=1)\n",
    "        batch_idx = torch.arange(x.size(0), device=x.device)\n",
    "        token_range = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        premise_mask = token_range < sep_positions.unsqueeze(1)\n",
    "        hypothesis_mask = token_range > sep_positions.unsqueeze(1)\n",
    "        \n",
    "        premise = torch.where(premise_mask, x, PAD_ID)\n",
    "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
    "        \n",
    "        # Encode separately\n",
    "        prem_emb = self.embedding(premise)\n",
    "        hypo_emb = self.embedding(hypothesis)\n",
    "        \n",
    "        prem_encoded, _ = self.premise_gru(prem_emb)\n",
    "        hypo_encoded, _ = self.hypothesis_gru(hypo_emb)\n",
    "        \n",
    "        # Call cross_attention external function\n",
    "        cross_context, cross_weights = apply_cross_attention(hypo_encoded, prem_encoded)\n",
    "        \n",
    "        # Pool and combine\n",
    "        hypo_pooled = hypo_encoded.mean(dim=1)\n",
    "        cross_pooled = cross_context.mean(dim=1)\n",
    "        combined = torch.cat([hypo_pooled, cross_pooled], dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        logits = self.fc(combined)\n",
    "        \n",
    "        if return_weights:\n",
    "            return logits, cross_weights\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, num_layers=2, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, \n",
    "                         bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.multihead_attn = MultiHeadSelfAttention(hidden_dim * 2, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None, return_weights=False):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        \n",
    "        # Create padding mask from existing PAD_ID\n",
    "        padding_mask = (x == PAD_ID)\n",
    "        attn_out, attn_weights = self.multihead_attn(gru_out, padding_mask=padding_mask)\n",
    "        \n",
    "        context = attn_out.mean(dim=1)\n",
    "        context = self.dropout(context)\n",
    "        logits = self.fc(context)\n",
    "        \n",
    "        if return_weights:\n",
    "            return logits, attn_weights\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAdditiveAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, num_layers=2):\n",
    "        super().__init__()\n",
    "        # Use same structure as existing LSTMAttentionModel\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, \n",
    "                           bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.attention = AdditiveAttention(hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None, return_weights=False):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        context, attn_weights = self.attention(lstm_out)\n",
    "        context = self.dropout(context)\n",
    "        logits = self.fc(context)\n",
    "        \n",
    "        if return_weights:\n",
    "            return logits, attn_weights\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCrossAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.premise_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, \n",
    "                                   bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.hypothesis_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, \n",
    "                                      bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 4, 2)  # hypo_pooled + cross_pooled\n",
    "    \n",
    "    def forward(self, x, mask=None, return_weights=False):\n",
    "        # Split input using existing SEP_ID logic\n",
    "        sep_positions = (x == SEP_ID).int().argmax(dim=1)\n",
    "        batch_idx = torch.arange(x.size(0), device=x.device)\n",
    "        token_range = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        premise_mask = token_range < sep_positions.unsqueeze(1)\n",
    "        hypothesis_mask = token_range > sep_positions.unsqueeze(1)\n",
    "        \n",
    "        premise = torch.where(premise_mask, x, PAD_ID)\n",
    "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
    "        \n",
    "        # Encode separately\n",
    "        prem_emb = self.embedding(premise)\n",
    "        hypo_emb = self.embedding(hypothesis)\n",
    "        \n",
    "        prem_encoded, _ = self.premise_lstm(prem_emb)\n",
    "        hypo_encoded, _ = self.hypothesis_lstm(hypo_emb)\n",
    "        \n",
    "        # Cross attention (hypothesis attends to premise)\n",
    "        cross_context, cross_weights = apply_cross_attention(hypo_encoded, prem_encoded)\n",
    "        \n",
    "        # Pool and combine\n",
    "        hypo_pooled = hypo_encoded.mean(dim=1)\n",
    "        cross_pooled = cross_context.mean(dim=1)\n",
    "        combined = torch.cat([hypo_pooled, cross_pooled], dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        logits = self.fc(combined)\n",
    "        \n",
    "        if return_weights:\n",
    "            return logits, cross_weights\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, num_layers=2, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, \n",
    "                           bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.multihead_attn = MultiHeadSelfAttention(hidden_dim * 2, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None, return_weights=False):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # Create padding mask from existing PAD_ID\n",
    "        padding_mask = (x == PAD_ID)\n",
    "        attn_out, attn_weights = self.multihead_attn(lstm_out, padding_mask=padding_mask)\n",
    "        \n",
    "        context = attn_out.mean(dim=1)\n",
    "        context = self.dropout(context)\n",
    "        logits = self.fc(context)\n",
    "        \n",
    "        if return_weights:\n",
    "            return logits, attn_weights\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate to Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAdditiveAttention(nn.Module):\n",
    "    def __init__(self, embedding_matrix, n_heads, n_layers, dropout=0.3):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.d_model = embed_dim\n",
    "        \n",
    "        # Use same embedding and transformer setup as existing TransformerClassifier\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), \n",
    "                                                     freeze=False, padding_idx=PAD_ID)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=hyperparameters.max_combined_len, dropout=dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dropout=dropout,\n",
    "            dim_feedforward=embed_dim*4, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Additive attention on transformer outputs\n",
    "        self.additive_attention = AdditiveAttention(embed_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim // 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        padding_mask = (src_mask == 0)\n",
    "        embedded = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "        encoded = self.transformer_encoder(pos_encoded, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Apply additive attention to all tokens (not just CLS)\n",
    "        context, attn_weights = self.additive_attention(encoded)\n",
    "        logits = self.classifier(context)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerCrossAttention(nn.Module):\n",
    "    def __init__(self, embedding_matrix, n_heads, n_layers, dropout=0.3):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.d_model = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), \n",
    "                                                     freeze=False, padding_idx=PAD_ID)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=hyperparameters.max_combined_len, dropout=dropout)\n",
    "        \n",
    "        # Separate encoders for premise and hypothesis\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dropout=dropout,\n",
    "            dim_feedforward=embed_dim*4, batch_first=True\n",
    "        )\n",
    "        self.premise_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.hypothesis_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Cross-attention layer\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim * 2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # Split into premise and hypothesis using existing SEP_ID logic\n",
    "        sep_positions = (src == SEP_ID).int().argmax(dim=1)\n",
    "        batch_idx = torch.arange(src.size(0), device=src.device)\n",
    "        token_range = torch.arange(src.size(1), device=src.device).unsqueeze(0)\n",
    "        premise_mask = token_range < sep_positions.unsqueeze(1)\n",
    "        hypothesis_mask = token_range > sep_positions.unsqueeze(1)\n",
    "        \n",
    "        premise = torch.where(premise_mask, src, PAD_ID)\n",
    "        hypothesis = torch.where(hypothesis_mask, src, PAD_ID)\n",
    "        \n",
    "        # Create masks\n",
    "        prem_padding_mask = (premise == PAD_ID)\n",
    "        hypo_padding_mask = (hypothesis == PAD_ID)\n",
    "        \n",
    "        # Encode separately\n",
    "        prem_emb = self.embedding(premise) * math.sqrt(self.d_model)\n",
    "        hypo_emb = self.embedding(hypothesis) * math.sqrt(self.d_model)\n",
    "        \n",
    "        prem_encoded = self.pos_encoder(prem_emb)\n",
    "        hypo_encoded = self.pos_encoder(hypo_emb)\n",
    "        \n",
    "        prem_output = self.premise_encoder(prem_encoded, src_key_padding_mask=prem_padding_mask)\n",
    "        hypo_output = self.hypothesis_encoder(hypo_encoded, src_key_padding_mask=hypo_padding_mask)\n",
    "        \n",
    "        # Cross attention: hypothesis attends to premise\n",
    "        cross_context, cross_weights = self.cross_attention(\n",
    "            hypo_output, prem_output, prem_output,\n",
    "            key_padding_mask=prem_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Use CLS token from hypothesis and cross-context\n",
    "        hypo_cls = hypo_output[:, 0, :]\n",
    "        cross_cls = cross_context[:, 0, :]\n",
    "        \n",
    "        combined = torch.cat([hypo_cls, cross_cls], dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMultiHead(nn.Module):\n",
    "    def __init__(self, embedding_matrix, n_heads, n_layers, dropout=0.3):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.d_model = embed_dim\n",
    "        \n",
    "        # Same base setup as existing TransformerClassifier\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), \n",
    "                                                     freeze=False, padding_idx=PAD_ID)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=hyperparameters.max_combined_len, dropout=dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dropout=dropout,\n",
    "            dim_feedforward=embed_dim*4, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Additional multi-head attention layer\n",
    "        self.extra_multihead = MultiHeadSelfAttention(embed_dim, n_heads)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim // 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        padding_mask = (src_mask == 0)\n",
    "        embedded = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "        encoded = self.transformer_encoder(pos_encoded, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Apply additional multi-head attention\n",
    "        enhanced_encoded, _ = self.extra_multihead(encoded, padding_mask=padding_mask)\n",
    "        \n",
    "        # Use CLS token for classification\n",
    "        cls_output = enhanced_encoded[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate with GRUThreeStream i.e. MONSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeStreamGRUAdditive(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, wide_layers=2, prem_layers=2, \n",
    "                 hypo_layers=2, num_fc_layers=2, dropout=0.3, unk_prob=0.1):\n",
    "        super().__init__()\n",
    "        # Same base setup as existing GRUThreeStreamNLI\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, \n",
    "                         num_layers=wide_layers, dropout=dropout if wide_layers > 1 else 0.0)\n",
    "        self.prem_encoder = nn.GRU(embedding_dim, hidden_dim, num_layers=prem_layers,\n",
    "                                 batch_first=True, bidirectional=True, dropout=dropout if prem_layers > 1 else 0.0)\n",
    "        self.hypo_encoder = nn.GRU(embedding_dim, hidden_dim, num_layers=hypo_layers,\n",
    "                                 batch_first=True, bidirectional=True, dropout=dropout if hypo_layers > 1 else 0.0)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        \n",
    "        # Additive attention for each stream\n",
    "        self.wide_attention = AdditiveAttention(hidden_dim * 2)\n",
    "        self.prem_attention = AdditiveAttention(hidden_dim * 2)\n",
    "        self.hypo_attention = AdditiveAttention(hidden_dim * 2)\n",
    "        \n",
    "        # Fusion (same as original)\n",
    "        fusion_dim = hidden_dim * 6  # 3 streams * 2 (bidirectional)\n",
    "        fc_layers = []\n",
    "        in_dim = fusion_dim\n",
    "        out_dim = hidden_dim * 2\n",
    "        \n",
    "        for i in range(num_fc_layers):\n",
    "            fc_layers.extend([nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            in_dim = out_dim\n",
    "            out_dim = max(hidden_dim, out_dim // 2)\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "        self.classifier = nn.Linear(in_dim, 2)\n",
    "        self.unk_id = UNK_ID\n",
    "        self.unk_prob = unk_prob\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if self.training and self.unk_prob > 0:\n",
    "            token_mask = (x != PAD_ID) & (x != SEP_ID) & (x != CLS_ID) & (x != UNK_ID)\n",
    "            replace_mask = (torch.rand_like(x.float()) < self.unk_prob) & token_mask\n",
    "            x = torch.where(replace_mask, torch.full_like(x, self.unk_id), x)\n",
    "            \n",
    "        sep_positions = (x == SEP_ID).int().argmax(dim=1)\n",
    "        batch_idx = torch.arange(x.size(0), device=x.device)\n",
    "        token_range = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        premise_mask = token_range < sep_positions.unsqueeze(1)\n",
    "        hypothesis_mask = token_range > sep_positions.unsqueeze(1)\n",
    "        premise = torch.where(premise_mask, x, PAD_ID)\n",
    "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
    "        \n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        emb_prem = self.embedding(premise)\n",
    "        emb_hypo = self.embedding(hypothesis)\n",
    "        \n",
    "        full_out, _ = self.gru(embedded)\n",
    "        full_out = self.norm(full_out)\n",
    "        prem_out, _ = self.prem_encoder(emb_prem)\n",
    "        prem_out = self.norm(prem_out)\n",
    "        hypo_out, _ = self.hypo_encoder(emb_hypo)\n",
    "        hypo_out = self.norm(hypo_out)\n",
    "        \n",
    "        wide_context, _ = self.wide_attention(full_out)\n",
    "        prem_context, _ = self.prem_attention(prem_out)\n",
    "        hypo_context, _ = self.hypo_attention(hypo_out)\n",
    "        \n",
    "        # Fusion\n",
    "        fused = torch.cat([wide_context, prem_context, hypo_context], dim=1)\n",
    "        fused = self.fc_layers(fused)\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeStreamGRUCross(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, wide_layers=2, prem_layers=2, \n",
    "                 hypo_layers=2, num_fc_layers=2, dropout=0.3, unk_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, \n",
    "                         num_layers=wide_layers, dropout=dropout if wide_layers > 1 else 0.0)\n",
    "        self.prem_encoder = nn.GRU(embedding_dim, hidden_dim, num_layers=prem_layers,\n",
    "                                 batch_first=True, bidirectional=True, dropout=dropout if prem_layers > 1 else 0.0)\n",
    "        self.hypo_encoder = nn.GRU(embedding_dim, hidden_dim, num_layers=hypo_layers,\n",
    "                                 batch_first=True, bidirectional=True, dropout=dropout if hypo_layers > 1 else 0.0)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        \n",
    "        # Cross-attention between streams\n",
    "        self.prem_to_hypo_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2, num_heads=2, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.hypo_to_prem_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2, num_heads=2, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        fusion_dim = hidden_dim * 2 * 5  # 5 streams * (hidden_dim * 2 for bidirectional)\n",
    "        \n",
    "        fc_layers = []\n",
    "        in_dim = fusion_dim\n",
    "        out_dim = hidden_dim * 2  # Start with same size as hidden_dim * 2\n",
    "        \n",
    "        for i in range(num_fc_layers):\n",
    "            fc_layers.extend([nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            in_dim = out_dim\n",
    "            out_dim = max(hidden_dim, out_dim // 2)  # Gradually reduce size\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "        self.classifier = nn.Linear(in_dim, 2)\n",
    "        self.unk_id = UNK_ID\n",
    "        self.unk_prob = unk_prob\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Word dropout\n",
    "        if self.training and self.unk_prob > 0:\n",
    "            token_mask = (x != PAD_ID) & (x != SEP_ID) & (x != CLS_ID) & (x != UNK_ID)\n",
    "            replace_mask = (torch.rand_like(x.float()) < self.unk_prob) & token_mask\n",
    "            x = torch.where(replace_mask, torch.full_like(x, self.unk_id), x)\n",
    "            \n",
    "        # Split premise and hypothesis\n",
    "        sep_positions = (x == SEP_ID).int().argmax(dim=1)\n",
    "        batch_idx = torch.arange(x.size(0), device=x.device)\n",
    "        token_range = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        premise_mask = token_range < sep_positions.unsqueeze(1)\n",
    "        hypothesis_mask = token_range > sep_positions.unsqueeze(1)\n",
    "        premise = torch.where(premise_mask, x, PAD_ID)\n",
    "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
    "        \n",
    "        # Three-stream processing\n",
    "        embedded = self.embedding(x)\n",
    "        emb_prem = self.embedding(premise)\n",
    "        emb_hypo = self.embedding(hypothesis)\n",
    "        \n",
    "        full_out, _ = self.gru(embedded)\n",
    "        full_out = self.norm(full_out)\n",
    "        prem_out, _ = self.prem_encoder(emb_prem)\n",
    "        prem_out = self.norm(prem_out)\n",
    "        hypo_out, _ = self.hypo_encoder(emb_hypo)\n",
    "        hypo_out = self.norm(hypo_out)\n",
    "        \n",
    "        # Cross-attention between premise and hypothesis\n",
    "        prem_cross, _ = self.prem_to_hypo_attention(prem_out, hypo_out, hypo_out)\n",
    "        hypo_cross, _ = self.hypo_to_prem_attention(hypo_out, prem_out, prem_out)\n",
    "        \n",
    "        # Pool all streams (mean pooling)\n",
    "        wide_pooled = full_out.mean(dim=1)  # shape: [batch_size, hidden_dim * 2]\n",
    "        prem_pooled = prem_out.mean(dim=1)  # shape: [batch_size, hidden_dim * 2]\n",
    "        hypo_pooled = hypo_out.mean(dim=1)  # shape: [batch_size, hidden_dim * 2]\n",
    "        prem_cross_pooled = prem_cross.mean(dim=1)  # shape: [batch_size, hidden_dim * 2]\n",
    "        hypo_cross_pooled = hypo_cross.mean(dim=1)  # shape: [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Fusion - concatenate all 5 streams\n",
    "        fused = torch.cat([wide_pooled, prem_pooled, hypo_pooled, prem_cross_pooled, hypo_cross_pooled], dim=1)\n",
    "        fused = self.fc_layers(fused)\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeStreamGRUMultiHead(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, wide_layers=2, prem_layers=2, \n",
    "                 hypo_layers=2, num_fc_layers=2, dropout=0.3, unk_prob=0.1, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, \n",
    "                         num_layers=wide_layers, dropout=dropout if wide_layers > 1 else 0.0)\n",
    "        self.prem_encoder = nn.GRU(embedding_dim, hidden_dim, num_layers=prem_layers,\n",
    "                                 batch_first=True, bidirectional=True, dropout=dropout if prem_layers > 1 else 0.0)\n",
    "        self.hypo_encoder = nn.GRU(embedding_dim, hidden_dim, num_layers=hypo_layers,\n",
    "                                 batch_first=True, bidirectional=True, dropout=dropout if hypo_layers > 1 else 0.0)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        \n",
    "        # Multi-head self-attention for each stream\n",
    "        self.wide_multihead = MultiHeadSelfAttention(hidden_dim * 2, num_heads)\n",
    "        self.prem_multihead = MultiHeadSelfAttention(hidden_dim * 2, num_heads)\n",
    "        self.hypo_multihead = MultiHeadSelfAttention(hidden_dim * 2, num_heads)\n",
    "        \n",
    "        # FIXED: Calculate correct fusion dimension\n",
    "        # 3 streams * (hidden_dim * 2 for bidirectional) = 384\n",
    "        fusion_dim = hidden_dim * 2 * 3\n",
    "        \n",
    "        fc_layers = []\n",
    "        in_dim = fusion_dim\n",
    "        out_dim = hidden_dim * 2  # Start with same size as hidden_dim * 2\n",
    "        \n",
    "        for i in range(num_fc_layers):\n",
    "            fc_layers.extend([nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            in_dim = out_dim\n",
    "            out_dim = max(hidden_dim, out_dim // 2)  # Gradually reduce size\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "        self.classifier = nn.Linear(in_dim, 2)\n",
    "        self.unk_id = UNK_ID\n",
    "        self.unk_prob = unk_prob\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Word dropout\n",
    "        if self.training and self.unk_prob > 0:\n",
    "            token_mask = (x != PAD_ID) & (x != SEP_ID) & (x != CLS_ID) & (x != UNK_ID)\n",
    "            replace_mask = (torch.rand_like(x.float()) < self.unk_prob) & token_mask\n",
    "            x = torch.where(replace_mask, torch.full_like(x, self.unk_id), x)\n",
    "            \n",
    "        # Split premise and hypothesis\n",
    "        sep_positions = (x == SEP_ID).int().argmax(dim=1)\n",
    "        batch_idx = torch.arange(x.size(0), device=x.device)\n",
    "        token_range = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        premise_mask = token_range < sep_positions.unsqueeze(1)\n",
    "        hypothesis_mask = token_range > sep_positions.unsqueeze(1)\n",
    "        premise = torch.where(premise_mask, x, PAD_ID)\n",
    "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
    "        \n",
    "        # Three-stream processing with multi-head attention\n",
    "        embedded = self.embedding(x)\n",
    "        emb_prem = self.embedding(premise)\n",
    "        emb_hypo = self.embedding(hypothesis)\n",
    "        \n",
    "        full_out, _ = self.gru(embedded)\n",
    "        full_out = self.norm(full_out)\n",
    "        prem_out, _ = self.prem_encoder(emb_prem)\n",
    "        prem_out = self.norm(prem_out)\n",
    "        hypo_out, _ = self.hypo_encoder(emb_hypo)\n",
    "        hypo_out = self.norm(hypo_out)\n",
    "        \n",
    "        # Apply multi-head self-attention to each stream\n",
    "        padding_mask = (x == PAD_ID)\n",
    "        prem_padding_mask = (premise == PAD_ID)\n",
    "        hypo_padding_mask = (hypothesis == PAD_ID)\n",
    "        \n",
    "        wide_attended, _ = self.wide_multihead(full_out, padding_mask=padding_mask)\n",
    "        prem_attended, _ = self.prem_multihead(prem_out, padding_mask=prem_padding_mask)\n",
    "        hypo_attended, _ = self.hypo_multihead(hypo_out, padding_mask=hypo_padding_mask)\n",
    "        \n",
    "        # Pool each stream (mean pooling)\n",
    "        wide_pooled = wide_attended.mean(dim=1)  # shape: [batch_size, hidden_dim * 2]\n",
    "        prem_pooled = prem_attended.mean(dim=1)  # shape: [batch_size, hidden_dim * 2]\n",
    "        hypo_pooled = hypo_attended.mean(dim=1)  # shape: [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        # Fusion - concatenate all 3 streams\n",
    "        fused = torch.cat([wide_pooled, prem_pooled, hypo_pooled], dim=1)\n",
    "        fused = self.fc_layers(fused)\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instatiate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_attention_models(gru=False, lstm=False, transformer=False, three_stream=False):\n",
    "    models = {}\n",
    "    \n",
    "    # GRU Attention Variants\n",
    "    if gru:\n",
    "        models.update({\n",
    "            'GRU_Additive': GRUAdditiveAttention(\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_dim=hyperparameters.embedding_dim,\n",
    "                hidden_dim=hyperparameters.hidden_dim,\n",
    "                dropout=hyperparameters.dropout,\n",
    "                num_layers=hyperparameters.n_layers\n",
    "            ).to(device),\n",
    "            'GRU_Cross': GRUCrossAttention(\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_dim=hyperparameters.embedding_dim,\n",
    "                hidden_dim=hyperparameters.hidden_dim,\n",
    "                dropout=hyperparameters.dropout,\n",
    "                num_layers=hyperparameters.n_layers\n",
    "            ).to(device),\n",
    "            'GRU_MultiHead': GRUMultiHeadAttention(\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_dim=hyperparameters.embedding_dim,\n",
    "                hidden_dim=hyperparameters.hidden_dim,\n",
    "                dropout=hyperparameters.dropout,\n",
    "                num_layers=hyperparameters.n_layers,\n",
    "                num_heads=hyperparameters.n_heads\n",
    "            ).to(device)\n",
    "        })\n",
    "    \n",
    "    # LSTM Attention Variants\n",
    "    if lstm:\n",
    "        models.update({\n",
    "            'LSTM_Additive': LSTMAdditiveAttention(\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_dim=hyperparameters.embedding_dim,\n",
    "                hidden_dim=hyperparameters.hidden_dim,\n",
    "                dropout=hyperparameters.dropout,\n",
    "                num_layers=hyperparameters.n_layers\n",
    "            ).to(device),\n",
    "            'LSTM_Cross': LSTMCrossAttention(\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_dim=hyperparameters.embedding_dim,\n",
    "                hidden_dim=hyperparameters.hidden_dim,\n",
    "                dropout=hyperparameters.dropout,\n",
    "                num_layers=hyperparameters.n_layers\n",
    "            ).to(device),\n",
    "            'LSTM_MultiHead': LSTMMultiHeadAttention(\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_dim=hyperparameters.embedding_dim,\n",
    "                hidden_dim=hyperparameters.hidden_dim,\n",
    "                dropout=hyperparameters.dropout,\n",
    "                num_layers=hyperparameters.n_layers,\n",
    "                num_heads=hyperparameters.n_heads\n",
    "            ).to(device)\n",
    "        })\n",
    "    \n",
    "    # Transformer Attention Variants\n",
    "    if transformer:\n",
    "        models.update({\n",
    "            'Transformer_Additive': TransformerAdditiveAttention(\n",
    "                embedding_matrix=embedding_matrix,\n",
    "                n_heads=hyperparameters.n_heads,\n",
    "                n_layers=hyperparameters.n_layers,\n",
    "                dropout=hyperparameters.dropout\n",
    "            ).to(device),\n",
    "            'Transformer_Cross': TransformerCrossAttention(\n",
    "                embedding_matrix=embedding_matrix,\n",
    "                n_heads=hyperparameters.n_heads,\n",
    "                n_layers=hyperparameters.n_layers,\n",
    "                dropout=hyperparameters.dropout\n",
    "            ).to(device),\n",
    "            'Transformer_MultiHead': TransformerMultiHead(\n",
    "                embedding_matrix=embedding_matrix,\n",
    "                n_heads=hyperparameters.n_heads,\n",
    "                n_layers=hyperparameters.n_layers,\n",
    "                dropout=hyperparameters.dropout\n",
    "            ).to(device)\n",
    "        })\n",
    "    \n",
    "    # Three-Stream GRU Variants\n",
    "    if three_stream:\n",
    "        models.update({\n",
    "            'ThreeStream_Additive': ThreeStreamGRUAdditive(\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_dim=hyperparameters.embedding_dim,\n",
    "                hidden_dim=hyperparameters.hidden_dim,\n",
    "                wide_layers=2, prem_layers=4, hypo_layers=2,\n",
    "                num_fc_layers=2, dropout=0.5, unk_prob=0.1\n",
    "            ).to(device),\n",
    "            'ThreeStream_Cross': ThreeStreamGRUCross(\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_dim=hyperparameters.embedding_dim,\n",
    "                hidden_dim=hyperparameters.hidden_dim,\n",
    "                wide_layers=2, prem_layers=4, hypo_layers=2,\n",
    "                num_fc_layers=2, dropout=0.5, unk_prob=0.1\n",
    "            ).to(device),\n",
    "            'ThreeStream_MultiHead': ThreeStreamGRUMultiHead(\n",
    "                vocab_size=vocab_size,\n",
    "                embedding_dim=hyperparameters.embedding_dim,\n",
    "                hidden_dim=hyperparameters.hidden_dim,\n",
    "                wide_layers=2, prem_layers=4, hypo_layers=2,\n",
    "                num_fc_layers=2, dropout=0.5, unk_prob=0.1,\n",
    "                num_heads=hyperparameters.n_heads\n",
    "            ).to(device)\n",
    "        })\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2id)  # This should already exist from your vocabulary building\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Device definition\n",
    "\n",
    "# print(f\"📊 Model Configuration:\")\n",
    "# print(f\"   Vocabulary size: {vocab_size}\")\n",
    "# print(f\"   Device: {device}\")\n",
    "# print(f\"   Embedding dim: {hyperparameters.embedding_dim}\")\n",
    "# print(f\"   Hidden dim: {hyperparameters.hidden_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 BUILDING ATTENTION MODELS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msi\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 12 attention models: ['GRU_Additive', 'GRU_Cross', 'GRU_MultiHead', 'LSTM_Additive', 'LSTM_Cross', 'LSTM_MultiHead', 'Transformer_Additive', 'Transformer_Cross', 'Transformer_MultiHead', 'ThreeStream_Additive', 'ThreeStream_Cross', 'ThreeStream_MultiHead']\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 BUILDING ATTENTION MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "attention_models = instantiate_attention_models(\n",
    "    gru=True, lstm=True, transformer=True, three_stream=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Created {len(attention_models)} attention models: {list(attention_models.keys())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for xb, mb, yb in loader: # x_train, train_mask, y_train\n",
    "        xb,mb,yb = xb.to(device), mb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(xb, mb)\n",
    "        # Handle models that return tuples vs single output\n",
    "        logits = output[0] if isinstance(output, tuple) else output\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)# Gradient clipping, prevents gradients to become to large \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        total_correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return total_loss/total, total_correct/total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Global Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for xb, mb, yb in loader:\n",
    "        xb, mb, yb = xb.to(device), mb.to(device), yb.to(device).long()\n",
    "        out = model(xb, mb)\n",
    "        # Handle models that return tuples vs single output\n",
    "        logits = out[0] if isinstance(out, tuple) else out\n",
    "\n",
    "        loss = criterion(logits, yb)\n",
    "        batch_size = yb.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == yb).sum().item()\n",
    "        total += batch_size\n",
    "\n",
    "    return total_loss / total, total_correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def train_models(models_to_train):\n",
    "    trained_models = {}\n",
    "    histories = {}\n",
    "\n",
    "    # Calculate class weights\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "    weights = compute_class_weight('balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float).to(device))\n",
    "\n",
    "    # Iterate through the models that were passed in\n",
    "    for name, model in models_to_train.items():\n",
    "        print(f\"--- Training {name.upper()} Model ---\")\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparameters.lr, weight_decay=hyperparameters.wd)\n",
    "        \n",
    "        trained_model, history = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            valid_loader, \n",
    "            optimizer, \n",
    "            n_epochs=hyperparameters.train_num_epoch, \n",
    "            patience=5, \n",
    "            criterion=criterion\n",
    "        )\n",
    "        \n",
    "        trained_models[name] = trained_model\n",
    "        histories[name] = history\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return trained_models, histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW CODE: Train 1 model. Rename later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, optimizer, n_epochs=10, patience=3,criterion=nn.CrossEntropyLoss()):\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer,criterion)\n",
    "        val_loss, val_acc = evaluate_model(model, valid_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - \"\n",
    "              f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f} - \"\n",
    "              f\"Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW CODE: Train multiples models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(models_to_train):\n",
    "    trained_models = {}\n",
    "    histories = {}\n",
    "\n",
    "    # Calculate class weights\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "    weights = compute_class_weight('balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float).to(device))\n",
    "\n",
    "    # Iterate through the models that were passed in\n",
    "    for name, model in models_to_train.items():\n",
    "        print(f\"--- Training {name.upper()} Model ---\")\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparameters.lr, weight_decay=hyperparameters.wd)\n",
    "        \n",
    "        trained_model, history = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            valid_loader, \n",
    "            optimizer, \n",
    "            n_epochs=hyperparameters.train_num_epoch, \n",
    "            patience=5, \n",
    "            criterion=criterion\n",
    "        )\n",
    "        \n",
    "        trained_models[name] = trained_model\n",
    "        histories[name] = history\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return trained_models, histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_unk_performance(model, data_loader, plain_data):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Get all predictions and labels\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks, labels in data_loader:\n",
    "            outputs = model(inputs.to(device), masks.to(device))\n",
    "            # Handle models that return tuples vs single output\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            preds = torch.max(outputs, dim=1)[1]\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    correct = (predictions == all_labels)\n",
    "\n",
    "    # Check which original sentences contain the UNK_ID\n",
    "    contain_UNK = np.array([UNK_ID in sent for sent in plain_data])\n",
    "\n",
    "    # Calculate accuracy for sentences with UNK tokens\n",
    "    unk_correct = correct[contain_UNK]\n",
    "    unk_accuracy = unk_correct.mean() if len(unk_correct) > 0 else 0\n",
    "\n",
    "    # Calculate accuracy for sentences without UNK tokens\n",
    "    no_unk_correct = correct[~contain_UNK]\n",
    "    no_unk_accuracy = no_unk_correct.mean() if len(no_unk_correct) > 0 else 0\n",
    "\n",
    "    print(\"--- UNK Token Analysis ---\")\n",
    "    print(f\"Accuracy on samples WITH [UNK] tokens:   {unk_accuracy:.4f} ({len(unk_correct)} samples)\")\n",
    "    print(f\"Accuracy on samples WITHOUT [UNK] tokens: {no_unk_accuracy:.4f} ({len(no_unk_correct)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def model_validation(model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in test_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            # Handle models that return tuples vs single output\n",
    "            if isinstance(logits, tuple):\n",
    "                logits = logits[0]\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"Accuracy:\", round(accuracy_score(all_labels, all_preds), 2))\n",
    "    print(f\"F1 Score: {f1_score(all_labels, all_preds):.2f}\")\n",
    "    print(\"Precision:\", round(precision_score(all_labels, all_preds), 2))\n",
    "    print(\"Recall:\", round(recall_score(all_labels, all_preds), 2))\n",
    "    print(\"-\" * 40)\n",
    "    print(classification_report(all_labels, all_preds, target_names=[\"neutral\", \"entails\"]))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    analyze_unk_performance(model, test_loader, plain_test)\n",
    "    print(\"\\n\")\n",
    "    hyperparameters.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models(trained_models):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL MODEL EVALUATION ON TEST SET\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    for model_name, model in trained_models.items():\n",
    "        print(f\"--- Evaluating {model_name.upper()} Model ---\")\n",
    "        model_validation(model)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 TRAINING ATTENTION MODELS\n",
      "================================================================================\n",
      "--- Training GRU_ADDITIVE Model ---\n",
      "Epoch 1/3 - Train loss: 0.5601, Train acc: 0.7193 - Val loss: 0.6284, Val acc: 0.6241\n",
      "Epoch 2/3 - Train loss: 0.4213, Train acc: 0.8277 - Val loss: 0.7293, Val acc: 0.6434\n",
      "Epoch 3/3 - Train loss: 0.3339, Train acc: 0.8704 - Val loss: 0.7448, Val acc: 0.6449\n",
      "\n",
      "\n",
      "--- Training GRU_CROSS Model ---\n",
      "Epoch 1/3 - Train loss: 0.5381, Train acc: 0.7315 - Val loss: 0.6860, Val acc: 0.6633\n"
     ]
    }
   ],
   "source": [
    "# Now you can proceed with training\n",
    "print(\"\\n🎯 TRAINING ATTENTION MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trained_models, histories = train_models(attention_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models that were trained\n",
    "evaluate_all_models(trained_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss vs the validation loss\n",
    "def plot_training_histories(histories):\n",
    "    if not histories:\n",
    "        print(\"No histories to plot.\")\n",
    "        return\n",
    "\n",
    "    for model_name, history in histories.items():\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(history['train_loss'], label='Training Loss', color='blue', marker='o')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss', color='orange', marker='o')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training & Validation Loss for {model_name.upper()} Model')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_histories(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Attention of the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rnn_attention(tokens, attention_weights):\n",
    "    # 1. Remove padding from tokens for a cleaner plot\n",
    "    try:\n",
    "        real_len = tokens.index('[PAD]')\n",
    "        tokens_no_padding = tokens[:real_len]\n",
    "        weights_no_padding = attention_weights.squeeze().cpu().numpy()[:real_len]\n",
    "    except ValueError:\n",
    "        tokens_no_padding = tokens\n",
    "        weights_no_padding = attention_weights.squeeze().cpu().numpy()\n",
    "\n",
    "    # 2. Create the bar plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=tokens_no_padding, y=weights_no_padding, palette='viridis')\n",
    "    plt.title('RNN Attention Weights')\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Attention Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific Plot for transformer\n",
    "def plot_attention_head(attention_weights, tokens, head_index):\n",
    "    \"\"\"Plots a readable heatmap for a single attention head, ignoring padding.\"\"\"\n",
    "    \n",
    "    # 1. Find the actual length of the sentence by finding the first [PAD] token\n",
    "    try:\n",
    "        real_len = tokens.index('[PAD]')\n",
    "    except ValueError:\n",
    "        real_len = len(tokens)\n",
    "\n",
    "    # 2. Slice the tokens and weights to remove padding\n",
    "    tokens_no_padding = tokens[:real_len]\n",
    "    head_weights = attention_weights[0, head_index][:real_len, :real_len]\n",
    "    \n",
    "    # 3. Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        head_weights, \n",
    "        xticklabels=tokens_no_padding, \n",
    "        yticklabels=tokens_no_padding, \n",
    "        cmap='viridis', \n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'Attention Head #{head_index}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master function to visualize attention\n",
    "def visualize_attention(model_name, model, sample_token_ids, sample_mask):\n",
    "    print(f\"--- Visualizing Attention for {model_name.upper()} Model ---\")\n",
    "    \n",
    "    # Convert token IDs back to words for plotting\n",
    "    plot_tokens = [id2word[int(token_id)] for token_id in sample_token_ids]\n",
    "    \n",
    "    model.eval() # Set model to evaluation mode\n",
    "    \n",
    "    # Add a batch dimension for model input\n",
    "    sample_token_ids = sample_token_ids.unsqueeze(0).to(device)\n",
    "    sample_mask = sample_mask.unsqueeze(0).to(device)\n",
    "\n",
    "    # Extended model name matching for attention variants\n",
    "    if model_name in ['gru', 'lstm', 'GRU_Additive', 'GRU_Cross', 'GRU_MultiHead', \n",
    "                     'LSTM_Additive', 'LSTM_Cross', 'LSTM_MultiHead']:\n",
    "        # For RNNs, try to call forward with return_weights=True\n",
    "        try:\n",
    "            output = model(sample_token_ids, sample_mask, return_weights=True)\n",
    "            if isinstance(output, tuple) and len(output) == 2:\n",
    "                _, attention_weights = output\n",
    "                plot_rnn_attention(plot_tokens, attention_weights.detach())\n",
    "            else:\n",
    "                print(f\"Attention weights not available for {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attention visualization failed for {model_name}: {e}\")\n",
    "            \n",
    "    elif model_name in ['transformer', 'Transformer_Additive', 'Transformer_Cross', 'Transformer_MultiHead']:\n",
    "        # For Transformer variants\n",
    "        try:\n",
    "            if hasattr(model, 'get_attention_weights'):\n",
    "                attention_weights = model.get_attention_weights(sample_token_ids, sample_mask)\n",
    "                # Plot the attention for each head\n",
    "                num_heads = attention_weights.shape[1]\n",
    "                for i in range(num_heads):\n",
    "                    plot_attention_head(attention_weights.detach().cpu().numpy(), plot_tokens, head_index=i)\n",
    "            else:\n",
    "                print(f\"get_attention_weights method not available for {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attention visualization failed for {model_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"Attention visualization for model type '{model_name}' is not supported yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample to visualize (e.g., the 5th sentence in the training set)\n",
    "sample_idx = 5\n",
    "sample_tokens = x_train[sample_idx]\n",
    "sample_mask = train_masks[sample_idx]\n",
    "\n",
    "print(\"👁️ VISUALIZING ATTENTION FOR ALL TRAINED MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize attention for ALL trained models (both original and new attention variants)\n",
    "for model_name, model in trained_models.items():\n",
    "    print(f\"\\n📊 Visualizing: {model_name}\")\n",
    "    try:\n",
    "        visualize_attention(model_name, model, sample_tokens, sample_mask)\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  Could not visualize {model_name}: {e}\")\n",
    "\n",
    "# Or if you want to visualize specific model types:\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎯 VISUALIZING BY MODEL CATEGORIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# GRU variants\n",
    "gru_models = [name for name in trained_models.keys() if 'gru' in name.lower() and 'threestream' not in name.lower()]\n",
    "print(f\"🔍 GRU Models: {gru_models}\")\n",
    "for model_name in gru_models:\n",
    "    visualize_attention(model_name, trained_models[model_name], sample_tokens, sample_mask)\n",
    "\n",
    "# LSTM variants  \n",
    "lstm_models = [name for name in trained_models.keys() if 'lstm' in name.lower() and 'threestream' not in name.lower()]\n",
    "print(f\"🔍 LSTM Models: {lstm_models}\")\n",
    "for model_name in lstm_models:\n",
    "    visualize_attention(model_name, trained_models[model_name], sample_tokens, sample_mask)\n",
    "\n",
    "# Transformer variants\n",
    "transformer_models = [name for name in trained_models.keys() if 'transformer' in name.lower()]\n",
    "print(f\"🔍 Transformer Models: {transformer_models}\")\n",
    "for model_name in transformer_models:\n",
    "    visualize_attention(model_name, trained_models[model_name], sample_tokens, sample_mask)\n",
    "\n",
    "# Three-Stream variants\n",
    "threestream_models = [name for name in trained_models.keys() if 'threestream' in name.lower()]\n",
    "print(f\"🔍 Three-Stream Models: {threestream_models}\")\n",
    "for model_name in threestream_models:\n",
    "    visualize_attention(model_name, trained_models[model_name], sample_tokens, sample_mask)\n",
    "\n",
    "# Or visualize the best model from each category\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🏆 VISUALIZING BEST MODELS FROM EACH CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_best_model_by_category(trained_models, histories):\n",
    "    \"\"\"Get the best model from each category based on validation accuracy\"\"\"\n",
    "    best_models = {}\n",
    "    \n",
    "    # Group models by category\n",
    "    categories = {\n",
    "        'GRU': [name for name in trained_models.keys() if 'gru' in name.lower() and 'threestream' not in name.lower()],\n",
    "        'LSTM': [name for name in trained_models.keys() if 'lstm' in name.lower() and 'threestream' not in name.lower()],\n",
    "        'Transformer': [name for name in trained_models.keys() if 'transformer' in name.lower()],\n",
    "        'ThreeStream': [name for name in trained_models.keys() if 'threestream' in name.lower()]\n",
    "    }\n",
    "    \n",
    "    for category, model_names in categories.items():\n",
    "        if model_names:\n",
    "            # Find model with highest validation accuracy\n",
    "            best_acc = 0\n",
    "            best_model_name = None\n",
    "            for name in model_names:\n",
    "                if name in histories and histories[name]['val_acc']:\n",
    "                    max_acc = max(histories[name]['val_acc'])\n",
    "                    if max_acc > best_acc:\n",
    "                        best_acc = max_acc\n",
    "                        best_model_name = name\n",
    "            \n",
    "            if best_model_name:\n",
    "                best_models[category] = best_model_name\n",
    "                print(f\"🎯 Best {category}: {best_model_name} (Val Acc: {best_acc:.4f})\")\n",
    "    \n",
    "    return best_models\n",
    "\n",
    "# Visualize best models\n",
    "best_models = get_best_model_by_category(trained_models, histories)\n",
    "for category, model_name in best_models.items():\n",
    "    print(f\"\\n🏅 Visualizing Best {category}: {model_name}\")\n",
    "    visualize_attention(model_name, trained_models[model_name], sample_tokens, sample_mask)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
