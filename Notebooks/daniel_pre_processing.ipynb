{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38257243",
   "metadata": {},
   "source": [
    "# Daniel's Pre-Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423eb4d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Daniel's Branch Created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24fff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d41667",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('../Dataset/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6793498",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e37ae",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917129ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "if not nltk.download('punkt'):\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "if not nltk.download('stopwords'):\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are just common English contractions. We used it in Lab 5 before!\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
    "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ade6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer and Lemmatizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "if not nltk.download('wordnet'):\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b71d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for lemmatization with POS tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowed POS tags for filtering (example: nouns, verbs, adjectives, adverbs)\n",
    "allowed_pos_tags = {'NN', 'NNS', 'NNP', 'NNPS',   # Nouns\n",
    "                    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Verbs\n",
    "                    'JJ', 'JJR', 'JJS',           # Adjectives\n",
    "                    'RB', 'RBR', 'RBS'}           # Adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec143704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the desired POS tags\n",
    "def filter_tokens_by_pos(tokens):\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    filtered = [word for word, tag in tagged_tokens if tag in allowed_pos_tags]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae31e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "min_word_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97386b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset, method='none', pos_filter=False, stop_w=False):\n",
    "    cleaned_premises = []\n",
    "    cleaned_hypotheses = []\n",
    "    labels = []\n",
    "    new_dataset = pd.DataFrame()\n",
    "    for index, row in dataset.iterrows():\n",
    "        premise = row['premise']\n",
    "        hypothesis = row['hypothesis']\n",
    "        label = row['label']\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        premise = premise.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "        # Expand contractions\n",
    "        for contraction, full_form in contraction_dict.items():\n",
    "            premise = premise.replace(contraction, full_form)\n",
    "            hypothesis = hypothesis.replace(contraction, full_form)\n",
    "        \n",
    "        # Remove punctuation and special characters\n",
    "        premise = re.sub(r'[^\\w\\s]', '', premise)\n",
    "        hypothesis = re.sub(r'[^\\w\\s]', '', hypothesis)\n",
    "\n",
    "        # Normalize whitespace after removing punctuation\n",
    "        premise = re.sub(r'\\s+', ' ', premise).strip()\n",
    "        hypothesis = re.sub(r'\\s+', ' ', hypothesis).strip()\n",
    "        \n",
    "        # Tokenize\n",
    "        premise_tokens = word_tokenize(premise)\n",
    "        hypothesis_tokens = word_tokenize(hypothesis)\n",
    "\n",
    "        if method == 'stem':\n",
    "            # Stemming\n",
    "            stemmer = PorterStemmer()\n",
    "            premise_tokens = [stemmer.stem(word) for word in premise_tokens]\n",
    "            hypothesis_tokens = [stemmer.stem(word) for word in hypothesis_tokens]\n",
    "        elif method == 'lemmatize':\n",
    "            # Lemmatization with POS tagging\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            premise_pos_tags = pos_tag(premise_tokens)\n",
    "            hypothesis_pos_tags = pos_tag(hypothesis_tokens)\n",
    "            premise_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in premise_pos_tags]\n",
    "            hypothesis_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in hypothesis_pos_tags]\n",
    "        \n",
    "        #POS Filtering\n",
    "        if pos_filter:\n",
    "            premise_tokens = filter_tokens_by_pos(premise_tokens)\n",
    "            hypothesis_tokens = filter_tokens_by_pos(hypothesis_tokens)\n",
    "\n",
    "        #Remove stop words\n",
    "        if stop_w:\n",
    "            premise_tokens = [word for word in premise_tokens if word not in stop_words]\n",
    "            hypothesis_tokens = [word for word in hypothesis_tokens if word not in stop_words]\n",
    "\n",
    "        # Remove short words  EX. row 146\n",
    "        if len(premise_tokens) > min_word_length and len(hypothesis_tokens) > min_word_length:    \n",
    "            cleaned_premises.append(' '.join(premise_tokens))\n",
    "            cleaned_hypotheses.append(' '.join(hypothesis_tokens))\n",
    "            labels.append(label)\n",
    "\n",
    "    new_dataset['premise'] = cleaned_premises\n",
    "    new_dataset['hypothesis'] = cleaned_hypotheses\n",
    "    new_dataset['label'] = labels\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_dataset = clean_dataset(train_df, method='stem', pos_filter=True, stop_w=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_train_dataset['premise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique word list from the cleaned dataset\n",
    "unique_words = set()\n",
    "for sentence in clean_train_dataset['premise']:\n",
    "    for word in sentence.split():\n",
    "        unique_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759cdfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_words)\n",
    "#No lemmatization or stemming count = 22555\n",
    "#Lemmatization count = 18986\n",
    "#Stemming count = 15954"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
