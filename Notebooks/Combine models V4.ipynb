{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 CITS4012 Project\n",
        "\n",
        "_Make sure you change the file name with your group id._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "CUDA version: 12.1\n",
            "Number of GPUs: 1\n",
            "GPU name: NVIDIA GeForce MX450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\DCCN9\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\DCCN9\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\DCCN9\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import copy\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "if not nltk.download('punkt'):\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "if not nltk.download('stopwords'):\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Stemmer and Lemmatizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "if not nltk.download('wordnet'):\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('omw-1.4')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sys.path.append(os.path.abspath('..')) \n",
        "import math\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "sys.path.append(os.path.abspath('../')) # Points to the current folder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "# Helper functions for training\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **We build a Configuration class to have all the hyperparameters together**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class construct for hyperparameters \n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.min_word_len = 3\n",
        "        self.max_word_len = 64\n",
        "        self.batch_size = 256\n",
        "        self.embedding_dim = 200\n",
        "        self.hidden_dim = 128\n",
        "        #Embedding\n",
        "        self.embedding_method = \"\"\n",
        "        self.max_pre_pros_len = 128\n",
        "        self.emb_lr = 0.0005\n",
        "        self.emb_epoch = 3\n",
        "        self.emb_window_size = 5\n",
        "        # Transformer\n",
        "        self.n_heads = 2\n",
        "        self.n_layers = 2\n",
        "        self.dropout = 0.2\n",
        "        self.num_classes = 2\n",
        "        self.lr = 0.00005\n",
        "        self.wd = 0.00005\n",
        "        self.train_num_epoch = 7\n",
        "        self.max_trans_len = 130\n",
        "        self.label_smoothing = 0.0\n",
        "\n",
        "    def display(self): # Display all the config parameters \n",
        "        print(\"=\" * 60)\n",
        "        print(\"CONFIGURATION PARAMETERS\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"\\nEmbedding Parameters:\")\n",
        "        print(f\"  Method: {self.embedding_method}\")\n",
        "        print(f\"  Max length: {self.max_pre_pros_len}\")\n",
        "        print(f\"  Learning rate: {self.emb_lr}\")\n",
        "        print(f\"  Embedding size: {self.emb_size}\")\n",
        "        print(f\"  Epochs: {self.emb_epoch}\")\n",
        "        print(f\"  Window size: {self.emb_window_size}\")\n",
        "        \n",
        "        print(\"\\nTransformer Parameters:\")\n",
        "        print(f\"  Batch size: {self.batch_size}\")\n",
        "        print(f\"  Number of heads: {self.n_heads}\")\n",
        "        print(f\"  Number of layers: {self.n_layers}\")\n",
        "        print(f\"  Dropout: {self.dropout}\")\n",
        "        print(f\"  Number of classes: {self.num_classes}\")\n",
        "        print(f\"  Learning rate: {self.lr}\")\n",
        "        print(f\"  Weight decay: {self.wd}\")\n",
        "        print(f\"  Training epochs: {self.train_num_epoch}\")\n",
        "        print(f\"  Max sequence length: {self.max_trans_len}\")\n",
        "        print(f\"  Label smoothing: {self.label_smoothing}\")\n",
        "        print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparameters = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.Dataset Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# These are common English contractions.\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "# Helper function for lemmatization with POS tagging\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if no match\n",
        "    \n",
        "# Allowed POS tags for filtering (example: nouns, verbs, adjectives, adverbs)\n",
        "allowed_pos_tags = {'NN', 'NNS', 'NNP', 'NNPS',   # Nouns\n",
        "                    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Verbs\n",
        "                    'JJ', 'JJR', 'JJS',           # Adjectives\n",
        "                    'RB', 'RBR', 'RBS'}           # Adverbs\n",
        "\n",
        "# Filter the desired POS tags\n",
        "def filter_tokens_by_pos(tokens):\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    filtered = [word for word, tag in tagged_tokens if tag in allowed_pos_tags]\n",
        "    return filtered\n",
        "\n",
        "def clean_dataset(dataset, min_word_len, max_word_len, method=['stem, lemmatize'], pos_filter=False, stop_w=False, clean_nums = False):\n",
        "    cleaned_premises = []\n",
        "    cleaned_hypotheses = []\n",
        "    cleaned_labels = []\n",
        "\n",
        "    for index, row in dataset.iterrows():\n",
        "        premise = row['premise']\n",
        "        hypothesis = row['hypothesis']\n",
        "        label = row['label']\n",
        "\n",
        "        # Lowercase\n",
        "        premise = premise.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "        # Expand contractions\n",
        "        for contraction, full_form in contraction_dict.items():\n",
        "            premise = premise.replace(contraction, full_form)\n",
        "            hypothesis = hypothesis.replace(contraction, full_form)\n",
        "\n",
        "        # Remove punctuation/special chars\n",
        "        premise = re.sub(r'[^a-zA-Z0-9\\s.-]', ' ', premise)\n",
        "        hypothesis = re.sub(r'[^a-zA-Z0-9\\s.-]', ' ', hypothesis)\n",
        "\n",
        "        # Replace underscores/hyphens with spaces, then normalize whitespace\n",
        "        premise = re.sub(r'[-–—_]+', ' ', premise)\n",
        "        hypothesis = re.sub(r'[-–—_]+', ' ', hypothesis)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        premise = re.sub(r'\\s+', ' ', premise).strip()\n",
        "        hypothesis = re.sub(r'\\s+', ' ', hypothesis).strip()\n",
        "\n",
        "        # Tokenization\n",
        "        premise_tokens = word_tokenize(premise)\n",
        "        hypothesis_tokens = word_tokenize(hypothesis)\n",
        "\n",
        "        # # Replace numbers with '<NUM>'\n",
        "        if clean_nums is True:\n",
        "            premise_tokens = ['[NUM]' if any(char.isdigit() for char in word) else word for word in premise_tokens]\n",
        "            hypothesis_tokens = ['[NUM]' if any(char.isdigit() for char in word) else word for word in hypothesis_tokens]\n",
        "\n",
        "\n",
        "        # Stemming/Lemmatization\n",
        "        if 'stem' in method:\n",
        "            stemmer = PorterStemmer()\n",
        "            premise_tokens = [stemmer.stem(word) for word in premise_tokens]\n",
        "            hypothesis_tokens = [stemmer.stem(word) for word in hypothesis_tokens]\n",
        "        elif 'lemmatize' in method:\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            premise_pos_tags = pos_tag(premise_tokens)\n",
        "            hypothesis_pos_tags = pos_tag(hypothesis_tokens)\n",
        "            premise_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in premise_pos_tags]\n",
        "            hypothesis_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in hypothesis_pos_tags]\n",
        "\n",
        "        # POS Filtering\n",
        "        if pos_filter:\n",
        "            premise_tokens = filter_tokens_by_pos(premise_tokens)\n",
        "            hypothesis_tokens = filter_tokens_by_pos(hypothesis_tokens)\n",
        "\n",
        "        # Remove stop words\n",
        "        if stop_w:\n",
        "            premise_tokens = [word for word in premise_tokens if word not in stop_words]\n",
        "            hypothesis_tokens = [word for word in hypothesis_tokens if word not in stop_words]\n",
        "\n",
        "        # Now check token length AFTER cleaning\n",
        "        if (min_word_len <= len(premise_tokens) <= max_word_len and\n",
        "            min_word_len <= len(hypothesis_tokens) <= max_word_len):\n",
        "            cleaned_premises.append(premise_tokens)\n",
        "            cleaned_hypotheses.append(hypothesis_tokens)\n",
        "            cleaned_labels.append(label)\n",
        "        # else: skip row\n",
        "\n",
        "    # Build DataFrame from all cleaned token lists\n",
        "    new_dataset = pd.DataFrame({\n",
        "        'premise': cleaned_premises,\n",
        "        'hypothesis': cleaned_hypotheses,\n",
        "        'label': cleaned_labels\n",
        "    })\n",
        "\n",
        "    return new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Make the Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [pluto, rotates, axis, every, 6.39, earth, day...\n",
              "1    [glenn, per, day, earth, rotates, axis, ., ear...\n",
              "2    [geysers, periodic, gush, hot, water, surface,...\n",
              "3    [facts, liquid, water, droplets, changed, invi...\n",
              "4    [comparison, earth, rotates, axis, per, day, r...\n",
              "dtype: object"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load raw data\n",
        "train_df = pd.read_json('../Dataset/train.json')\n",
        "test_df = pd.read_json('../Dataset/test.json')\n",
        "validation_df = pd.read_json('../Dataset/validation.json')\n",
        "\n",
        "# Clean datasets. MAX_WORD_LENGTH set to 64 to remove very long texts\n",
        "clean_train_dataset = clean_dataset(train_df, hyperparameters.min_word_len, hyperparameters.max_word_len, stop_w=True, method=[], pos_filter=False)\n",
        "clean_test_dataset = clean_dataset(test_df, hyperparameters.min_word_len, hyperparameters.max_word_len,stop_w=True, method=[], pos_filter= False )\n",
        "clean_validation_dataset = clean_dataset(validation_df, hyperparameters.min_word_len, hyperparameters.max_word_len,stop_w=True, method=[], pos_filter= False )\n",
        "\n",
        "# Combine clean premises and hypotheses\n",
        "clean_t_dataset = clean_train_dataset['premise'] + clean_train_dataset['hypothesis']\n",
        "clean_t_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.36685724185724183"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(train_df['label']=='entails') / (sum(train_df['label']!='entails') + sum(train_df['label']=='entails'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5038343558282209"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(validation_df['label']=='entails') / (sum(validation_df['label']!='entails') + sum(validation_df['label']=='entails'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.39604891815616183"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(test_df['label']=='entails') / (sum(test_df['label']!='entails') + sum(test_df['label']=='entails'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a unique word list from the cleaned dataset\n",
        "unique_words = set()\n",
        "for sentence in clean_t_dataset: # Both Premise and Hypothesis \n",
        "    for word in sentence:\n",
        "        unique_words.add(word)\n",
        "        \n",
        "unique_words_list = sorted(list(unique_words))\n",
        "\n",
        "# Make dictionary of words and indices\n",
        "word2id = {w:i for i,w in enumerate(unique_words_list)}\n",
        "id2word = {i:w for i,w in enumerate(unique_words_list)}\n",
        "\n",
        "\n",
        "# Add special tokens to use later\n",
        "SPECIAL_TOKENS = ['[PAD]','[UNK]','[CLS]','[SEP]'] \n",
        "for tok in SPECIAL_TOKENS:\n",
        "    if tok not in word2id:\n",
        "        idx = len(word2id)\n",
        "        word2id[tok] = idx\n",
        "        id2word[idx] = tok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEP_ID = word2id['[SEP]']\n",
        "UNK_ID = word2id['[UNK]']\n",
        "PAD_ID = word2id['[PAD]']\n",
        "CLS_ID = word2id['[CLS]']\n",
        "\n",
        "\n",
        "def prepare_indexed_data(df, word2id, max_len):\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    labels = []\n",
        "\n",
        "    label_map = {'neutral': 0, 'entails': 1}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        premise_toks = row['premise']\n",
        "        hypothesis_toks = row['hypothesis']\n",
        "        label = row['label']\n",
        "\n",
        "        # Add special tokens \n",
        "        tokens = [CLS_ID] \\\n",
        "                + [word2id.get(w,UNK_ID) for w in premise_toks] \\\n",
        "                + [SEP_ID] \\\n",
        "                + [word2id.get(w,UNK_ID) for w in hypothesis_toks] \n",
        "        # Truncate\n",
        "        tokens = tokens[:max_len * 2 + 2] #to combine len of hypothesis and premise, plus cls and sep  \n",
        "\n",
        "        # Attention mask \n",
        "        attn = [1] * len(tokens)\n",
        "\n",
        "        # Pad\n",
        "        pad_len = max_len- len(tokens) # To fill the [PAD]\n",
        "        if pad_len > 0:\n",
        "            tokens += [PAD_ID] * pad_len\n",
        "            attn += [0] * pad_len # FLag positions as padding \n",
        "\n",
        "        input_ids.append(tokens)\n",
        "        attention_mask.append(attn)\n",
        "        labels.append(label_map[label])\n",
        "\n",
        "    return (torch.LongTensor(input_ids),\n",
        "            np.array(input_ids),\n",
        "            torch.LongTensor(attention_mask),\n",
        "            torch.LongTensor(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of x_train: torch.Size([22900, 64])\n",
            "Shape of train_masks: torch.Size([22900, 64])\n",
            "Shape of y_train: torch.Size([22900])\n",
            "\n",
            "--- Example ---\n",
            "First example real length (mask sum): 16\n",
            "First example PAD count: 48\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGdCAYAAADzOWwgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJL5JREFUeJzt3QlwlEXex/F/IAHCkYQQciCZGBdNghwRUGQ9liMSES1Y2C0tDaYWcBcWkGMLkFrk8nWhUECUaETk2FIWYUuUS65E8CCcwnKFrO4Gk4JADJgDyAWZt7qtmc1wRpnJM0l/P1WPk3meTk9PGzK/9NP9PD52u90uAAAABmtgdQMAAACsRiACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABjP1+oG1AVVVVVy+vRpadGihfj4+FjdHAAAUAPq2tMlJSXSpk0badDg5mNABKIaUGEoMjLS6mYAAIBfIDc3V9q2bXvTMgSiGlAjQ44ODQgIsLo5AADUHxcvirRp89PXp0+LNGvmtqqLi4v1gIbjc/xmCEQ14DhNpsIQgQgAADdq2PB/X6vPWDcGIoeaTHdhUjUAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA43G3e3i9nJwcKSgo8OhrhISEiM1m8+hrAAC8F4EIXh+GYmPjpLT0kkdfx9+/qZw4kUkoAgBDEYjg1dTIkApD3YdOl4CIOz3yGsV5J2XP0pn6tQhEAGAmAhHqBBWGgm0xVjcDAFBPMakaAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4/la3QDAW2RmZnqs7pCQELHZbB6rHwBwewhEMF5p0TkR8ZGkpCSPvYa/f1M5cSKTUAQAXopABONVXioREbvEPztZWkfHur3+4ryTsmfpTCkoKCAQAYCX8po5RHPmzBEfHx8ZN26cc19ZWZmMGjVKWrVqJc2bN5fBgwfL2bNnXb4vJydH+vfvL02bNpXQ0FCZOHGiXL582aXMjh07pEuXLtK4cWNp166dLF++vNbeF+qO5qE2CbbFuH0LiLjT6rcGAKgLgWjfvn3y7rvvSqdOnVz2jx8/XtavXy9r1qyRnTt3yunTp2XQoEHO41euXNFhqKKiQnbt2iUrVqzQYWfatGnOMtnZ2bpMr1695NChQzpwDR8+XLZs2VKr7xEAAHgvywPRhQsX5LnnnpP33ntPWrZs6dxfVFQk77//vsyfP1969+4tXbt2lWXLlungs3v3bl1m69atcvz4cfnggw8kPj5e+vXrJ6+88oqkpKTokKSkpqZKdHS0zJs3T+Li4mT06NHyu9/9ThYsWGDZewYAAN7F8kCkTompEZyEhASX/QcOHJDKykqX/bGxsXoORkZGhn6uHjt27ChhYWHOMomJiVJcXCzHjh1zlrm6blXGUQcAAIClk6pXrVol33zzjT5ldrUzZ85Io0aNJCgoyGW/Cj/qmKNM9TDkOO44drMyKjSVlpaKv7//Na9dXl6uNwdVFgAA1F+WjRDl5ubK2LFj5cMPP5QmTZqIN5k9e7YEBgY6t8jISKubBAAA6mMgUqfE8vPz9eovX19fvamJ02+++ab+Wo3iqHlAhYWFLt+nVpmFh4frr9Xj1avOHM9vVSYgIOC6o0PKlClT9Bwmx6bCGwAAqL8sC0R9+vSRI0eO6JVfjq1bt256grXjaz8/P0lLS3N+T1ZWll5m36NHD/1cPao6VLBy2LZtmw477du3d5apXoejjKOO61HL81Ud1TcAAFB/WTaHqEWLFtKhQweXfc2aNdPXHHLsHzZsmEyYMEGCg4N1KBkzZowOMg8++KA+3rdvXx18hgwZInPnztXzhaZOnaonaqtQo4wYMUIWLVokkyZNkqFDh0p6erqsXr1aNm7caMG7BgAA3sirr1StlsY3aNBAX5BRTXJWq8Pefvtt5/GGDRvKhg0bZOTIkTooqUCVnJwss2bNcpZRS+5V+FHXNFq4cKG0bdtWlixZousCAADwukCkrihdnZpsra4ppLYbiYqKkk2bNt203p49e8rBgwfd1k4AAFC/eFUgQt2k5nWp+3TVtTvQAwDgQCDCbYeh2Ng4KS295NHXqSz/6crjAAB4AoEIt0WNDKkw1H3odI/cxDTvSIYcXbf4mhv2AgDgTgQiuIUKQ+rO7u5WnHfS7XUCAOB19zIDAACwGoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMJ6v1Q0ATJGZmemxukNCQsRms3msfgCo7whEgIeVFp0TER9JSkry2Gv4+zeVEycyCUUA8AsRiAAPq7xUIiJ2iX92srSOjnV7/cV5J2XP0plSUFBAIAKAX4hABNSS5qE2CbbFWN0MAMB1MKkaAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDxLA9E777wjnTp1koCAAL316NFDPvvsM+fxsrIyGTVqlLRq1UqaN28ugwcPlrNnz7rUkZOTI/3795emTZtKaGioTJw4US5fvuxSZseOHdKlSxdp3LixtGvXTpYvX15r7xEAAHg/SwNR27ZtZc6cOXLgwAHZv3+/9O7dWwYMGCDHjh3Tx8ePHy/r16+XNWvWyM6dO+X06dMyaNAg5/dfuXJFh6GKigrZtWuXrFixQoedadOmOctkZ2frMr169ZJDhw7JuHHjZPjw4bJlyxZL3jMAAPA+vla++FNPPeXy/NVXX9WjRrt379Zh6f3335eVK1fqoKQsW7ZM4uLi9PEHH3xQtm7dKsePH5ft27dLWFiYxMfHyyuvvCKTJ0+WGTNmSKNGjSQ1NVWio6Nl3rx5ug71/V999ZUsWLBAEhMTLXnfAADAu3jNHCI12rNq1Sq5ePGiPnWmRo0qKyslISHBWSY2NlZsNptkZGTo5+qxY8eOOgw5qJBTXFzsHGVSZarX4SjjqON6ysvLdR3VNwAAUH9ZHoiOHDmi5wep+T0jRoyQtWvXSvv27eXMmTN6hCcoKMilvAo/6piiHquHIcdxx7GblVEhp7S09Lptmj17tgQGBjq3yMhIt75nAADgXSwPRDExMXpuz549e2TkyJGSnJysT4NZacqUKVJUVOTccnNzLW0PAACox3OIFDUKpFZ+KV27dpV9+/bJwoUL5emnn9aTpQsLC11GidQqs/DwcP21ety7d69LfY5VaNXLXL0yTT1Xq9r8/f2v2yY1WqU2AABgBstHiK5WVVWl5/CocOTn5ydpaWnOY1lZWXqZvZpjpKhHdcotPz/fWWbbtm067KjTbo4y1etwlHHUAQAA4Gv1qal+/frpidIlJSV6RZm6ZpBaEq/m7gwbNkwmTJggwcHBOuSMGTNGBxm1wkzp27evDj5DhgyRuXPn6vlCU6dO1dcucozwqHlJixYtkkmTJsnQoUMlPT1dVq9eLRs3brTyrQMAAC9iaSBSIzvPP/+85OXl6QCkLtKowtBjjz2mj6ul8Q0aNNAXZFSjRmp12Ntvv+38/oYNG8qGDRv03CMVlJo1a6bnIM2aNctZRi25V+FHXdNInYpTy/mXLFnCknsAAOAdgUhdZ+hmmjRpIikpKXq7kaioKNm0adNN6+nZs6ccPHjwF7cTAADUb143hwgAAKC2EYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMN4vCkR33XWXnDt37pr9hYWF+hgAAEC9D0QnT56UK1euXLO/vLxcTp065Y52AQAA1Brfn1N43bp1zq+3bNkigYGBzucqIKWlpcmdd97p3hYCAAB4UyAaOHCgfvTx8ZHk5GSXY35+fjoMzZs3z70tBAAA8KZAVFVVpR+jo6Nl3759EhIS4ql2AQAAeGcgcsjOznZ/SwAAAOpSIFLUfCG15efnO0eOHJYuXeqOtgEAAHhvIJo5c6bMmjVLunXrJhEREXpOEQAAgFGBKDU1VZYvXy5Dhgxxf4sAAADqwnWIKioq5Ne//rX7WwMAAFBXAtHw4cNl5cqV7m8NAABAXTllVlZWJosXL5bt27dLp06d9DWIqps/f7672gcAAOCdgejw4cMSHx+vvz569KjLMSZYAwAAIwLR559/7v6WAAAA1KU5RAAAAGL6CFGvXr1uemosPT39dtoEAADg/YHIMX/IobKyUg4dOqTnE11901cAAIB6GYgWLFhw3f0zZsyQCxcu3G6bAAAA6sa9zK4nKSlJHnjgAXn99dfdWS2AGsjMzPRY3SEhIWKz2TxWPwDUq0CUkZEhTZo0cWeVAG6htOicuuCF/oPEU/z9m8qJE5mEIgD11i8KRIMGDXJ5brfbJS8vT/bv3y8vv/yyu9oGoAYqL5Wof4US/+xkaR0d6/b6i/NOyp6lM6WgoIBABKDe+kWBKDAw0OV5gwYNJCYmRmbNmiV9+/Z1V9sA/AzNQ20SbIuxuhkAYE4gWrZsmftbAgAAUBfnEB04cMA5kfPee++V++67z13tAgAA8O5AlJ+fL88884zs2LFDgoKC9L7CwkJ9wcZVq1ZJ69at3d1OAAAA77p1x5gxY6SkpESOHTsm58+f15u6KGNxcbG8+OKL7m8lAACAt40Qbd68WbZv3y5xcXHOfe3bt5eUlBQmVQMAADNGiKqqqsTPz++a/WqfOgYAAFDvA1Hv3r1l7Nixcvr0aee+U6dOyfjx46VPnz7ubB8AAIB3BqJFixbp+UJ33nmn/OpXv9JbdHS03vfWW2+5v5UAAADeNocoMjJSvvnmGz2P6MSJE3qfmk+UkJDg7vYBAAB41whRenq6njytRoJ8fHzkscce0yvO1Hb//ffraxF9+eWXnmstAACA1YHojTfekBdeeEECAgKuezuPP/3pTzJ//nx3tg8AAMC7AtG//vUvefzxx294XC25V1evBgAAqLeB6OzZs9ddbu/g6+srP/zwgzvaBQAA4J2B6I477tBXpL6Rw4cPS0REhDvaBQAA4J2B6IknnpCXX35ZysrKrjlWWloq06dPlyeffNKd7QMAAPCuZfdTp06Vjz/+WO655x4ZPXq0xMTE6P1q6b26bceVK1fkr3/9q6faCgAAYH0gCgsLk127dsnIkSNlypQpYrfb9X61BD8xMVGHIlUGAACgXl+YMSoqSjZt2iQ//vijfPfddzoU3X333dKyZUvPtBAAAMAbr1StqACkLsYIAABg5L3MAAAA6hMCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPEsD0ezZs/XVrlu0aCGhoaEycOBAycrKcilTVlYmo0aNklatWknz5s1l8ODBcvbsWZcyOTk50r9/f2natKmuZ+LEiXL58mWXMjt27JAuXbpI48aNpV27drJ8+fJaeY8AAMD7WRqIdu7cqcPO7t27Zdu2bVJZWSl9+/aVixcvOsuMHz9e1q9fL2vWrNHlT58+LYMGDXIev3Llig5DFRUV+sazK1as0GFn2rRpzjLZ2dm6TK9eveTQoUMybtw4GT58uGzZsqXW3zMAAKhH9zJzh82bN7s8V0FGjfAcOHBAHn30USkqKpL3339fVq5cKb1799Zlli1bJnFxcTpEPfjgg7J161Y5fvy4bN++XcLCwiQ+Pl5eeeUVmTx5ssyYMUMaNWokqampEh0dLfPmzdN1qO//6quvZMGCBZKYmGjJewcAAN7Dq+YQqQCkBAcH60cVjNSoUUJCgrNMbGys2Gw2ycjI0M/VY8eOHXUYclAhp7i4WI4dO+YsU70ORxlHHVcrLy/X3199AwAA9ZfXBKKqqip9Kuuhhx6SDh066H1nzpzRIzxBQUEuZVX4UcccZaqHIcdxx7GblVFBp7S09LpzmwIDA51bZGSkm98tAADwJl4TiNRcoqNHj8qqVausbopMmTJFj1Y5ttzcXKubBAAA6uscIofRo0fLhg0b5IsvvpC2bds694eHh+vJ0oWFhS6jRGqVmTrmKLN3716X+hyr0KqXuXplmnoeEBAg/v7+17RHrURTGwAAMIOlI0R2u12HobVr10p6erqe+Fxd165dxc/PT9LS0pz71LJ8tcy+R48e+rl6PHLkiOTn5zvLqBVrKuy0b9/eWaZ6HY4yjjoAAIDZfK0+TaZWkH366af6WkSOOT9q3o4auVGPw4YNkwkTJuiJ1irkjBkzRgcZtcJMUcv0VfAZMmSIzJ07V9cxdepUXbdjlGfEiBGyaNEimTRpkgwdOlSHr9WrV8vGjRutfPsAAMBLWDpC9M477+g5Oj179pSIiAjn9tFHHznLqKXxTz75pL4go1qKr05/ffzxx87jDRs21Kfb1KMKSklJSfL888/LrFmznGXUyJMKP2pUqHPnznr5/ZIlS1hyDwAArB8hUqfMbqVJkyaSkpKitxuJioqSTZs23bQeFboOHjz4i9oJAADqN69ZZQYAAGAVAhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADCer9UNgOfl5ORIQUGBR+rOzMz0SL0AANQmApEBYSg2Nk5KSy959HUqyys8Wj8AAJ5EIKrn1MiQCkPdh06XgIg73V5/3pEMObpusVy+fNntdQMAUFsIRIZQYSjYFuP2eovzTrq9TgAAahuTqgEAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADG87W6AQDqhszMTI/VHRISIjabzWP1A8CtEIgA3FRp0TkR8ZGkpCSPvYa/f1M5cSKTUATAMgQiADdVealEROwS/+xkaR0d6/b6i/NOyp6lM6WgoIBABMAyBCIANdI81CbBthirmwEAHsGkagAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxLA1EX3zxhTz11FPSpk0b8fHxkU8++cTluN1ul2nTpklERIT4+/tLQkKCfPvtty5lzp8/L88995wEBARIUFCQDBs2TC5cuOBS5vDhw/LII49IkyZNJDIyUubOnVsr7w8AANQNlgaiixcvSufOnSUlJeW6x1VwefPNNyU1NVX27NkjzZo1k8TERCkrK3OWUWHo2LFjsm3bNtmwYYMOWX/84x+dx4uLi6Vv374SFRUlBw4ckNdee01mzJghixcvrpX3CAAAvJ+vlS/er18/vV2PGh164403ZOrUqTJgwAC97+9//7uEhYXpkaRnnnlGMjMzZfPmzbJv3z7p1q2bLvPWW2/JE088Ia+//roeefrwww+loqJCli5dKo0aNZJ7771XDh06JPPnz3cJTgAAwFxeO4coOztbzpw5o0+TOQQGBkr37t0lIyNDP1eP6jSZIwwpqnyDBg30iJKjzKOPPqrDkIMaZcrKypIff/zxuq9dXl6uR5aqbwAAoP7y2kCkwpCiRoSqU88dx9RjaGioy3FfX18JDg52KXO9Oqq/xtVmz56tw5djU/OOAABA/eW1gchKU6ZMkaKiIueWm5trdZMAAICJgSg8PFw/nj171mW/eu44ph7z8/Ndjl++fFmvPKte5np1VH+NqzVu3FivWqu+AQCA+strA1F0dLQOLGlpac59ai6PmhvUo0cP/Vw9FhYW6tVjDunp6VJVVaXnGjnKqJVnlZWVzjJqRVpMTIy0bNmyVt8TAADwTpYGInW9ILXiS22OidTq65ycHH1donHjxsn//d//ybp16+TIkSPy/PPP65VjAwcO1OXj4uLk8ccflxdeeEH27t0rX3/9tYwePVqvQFPllGeffVZPqFbXJ1LL8z/66CNZuHChTJgwwcq3DgAAvIily+73798vvXr1cj53hJTk5GRZvny5TJo0SV+rSC2PVyNBDz/8sF5mry6w6KCW1asQ1KdPH726bPDgwfraRQ5qUvTWrVtl1KhR0rVrVwkJCdEXe2TJPQAA8IpA1LNnT329oRtRo0SzZs3S242oFWUrV6686et06tRJvvzyy9tqKwAAqL+8dg4RAABAbSEQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIzna3UDAEDJzMz0WN0hISFis9k8Vj+Auo9ABMBSpUXnRMRHkpKSPPYa/v5N5cSJTEIRgBsiEAGwVOWlEhGxS/yzk6V1dKzb6y/OOyl7ls6UgoICAhGAGyIQAfAKzUNtEmyLsboZAAzFpGoAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwnq/VDQCA2pCZmenR+kNCQsRms3n0NQB4DoEIQL1WWnRORHwkKSnJo6/j799UTpzIJBQBdRSBCEC9VnmpRETsEv/sZGkdHeuR1yjOOyl7ls6UgoICAhFQRxGIABiheahNgm0xVjcDgJdiUjUAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9bd3iBnJwcfQ+kuniHbwC18+8tJCSE+6QBHkQg8oIwFBsbJ6Wllzz6OpXlFR6tHzBZadE5EfGRpKQkj72Gv39TOXEik1AEeAiByGJqZEiFoe5Dp0tAxJ1urz/vSIYcXbdYLl++7Pa6Afyk8lKJiNgl/tnJ0jo61u31F+edlD1LZ+rfFwQiwDMIRF5ChSFP3Ilb/SIFUDuah9o88u8YgOcxqRoAABiPQAQAAIxHIAIAAMYjEAEAAOMxqRoA6giucwR4DoEIALwc1zkCPI9ABABejuscAZ5nVCBKSUmR1157Tc6cOSOdO3eWt956Sx544AGrmwUANcJ1jgDPMSYQffTRRzJhwgRJTU2V7t27yxtvvCGJiYmSlZUloaGhVjcPACzHHCWYzJhANH/+fHnhhRfkD3/4g36ugtHGjRtl6dKl8tJLL1ndPACwDHOUAEMCUUVFhRw4cECmTJni3NegQQNJSEiQjIyMa8qXl5frzaGoqEg/FhcXu71tFy5c0I/nv8+Sy+Wlbq+/OO97/Vh06lvx8/Whfgteg/rrd/218Rqerv/cf47qOUp39fy9BIa1dXv9l86flaxtK2XLli0SE+OZU37qd3pVVZVH6q6N+sPDw/VmpIsX//e1+py9csVtVTs+t+12+60L2w1w6tQp1RP2Xbt2ueyfOHGi/YEHHrim/PTp03V5NjY2NjY2NqnzW25u7i2zghEjRD+XGklS840c1F8F58+fl1atWomPj2f+wryd9BsZGSm5ubkSEBBgdXPqJPrw9tB/t48+vD303+0rrqd9qEaGSkpKpE2bNrcsa0QgUpP5GjZsKGfPnnXZr55fb4iycePGeqsuKChIvJn6Aa5PP8RWoA9vD/13++jD20P/3b6AetiHgYGBNSpnxK07GjVqJF27dpW0tDSXUR/1vEePHpa2DQAAWM+IESJFnQJLTk6Wbt266WsPqWX3Fy9edK46AwAA5jImED399NPyww8/yLRp0/SFGePj42Xz5s0SFhYmdZk6tTd9+vRrTvGh5ujD20P/3T768PbQf7evMX0oPmpmtdWNAAAAsJIRc4gAAABuhkAEAACMRyACAADGIxABAADjEYjqiC+++EKeeuopfbVNdbXsTz75xOW4mhuvVtBFRESIv7+/vk/bt99+a1l7vc3s2bPl/vvvlxYtWkhoaKgMHDhQsrKyXMqUlZXJqFGj9BXJmzdvLoMHD77mYp6meuedd6RTp07Oi7ap63d99tlnzuP03c83Z84c/W953Lhxzn30443NmDFD91f1LTY21nmcvquZU6dO6Zv4qn5SnxUdO3aU/fv3O4+b/FlCIKoj1DWTOnfuLCkpKdc9PnfuXHnzzTclNTVV9uzZI82aNZPExET9SwIiO3fu1L8sd+/eLdu2bZPKykrp27ev7leH8ePHy/r162XNmjW6/OnTp2XQoEGWtttbtG3bVn+Aq5skq1+evXv3lgEDBsixY8f0cfru59m3b5+8++67OmRWRz/e3L333it5eXnO7auvvnIeo+9u7ccff5SHHnpI/Pz89B80x48fl3nz5knLli2dZYz+LHHnTVRRO9T/trVr1zqfV1VV2cPDw+2vvfaac19hYaG9cePG9n/84x8WtdK75efn637cuXOns7/8/Pzsa9ascZbJzMzUZTIyMixsqfdq2bKlfcmSJfTdz1RSUmK/++677du2bbP/5je/sY8dO1bvpx9vTt10u3Pnztc9Rt/VzOTJk+0PP/zwDY9XGf5ZwghRPZCdna0vNqmGNqvfu6V79+6SkZFhadu8VVFRkX4MDg7Wj2rkQ40aVe9DNRxvs9now6tcuXJFVq1apUfX1Kkz+u7nUSOV/fv3d+kvhX68NXXqRk0buOuuu+S5556TnJwcvZ++q5l169bpuzX8/ve/11MH7rvvPnnvvfecx7MN/ywhENUD6gdYufqq2+q54xjE5T52at6GGjru0KGD3qf6Sd3z7uqb+NKH/3PkyBE9N0NdyXbEiBGydu1aad++PX33M6gg+c033+g5bVejH29OfSgvX75c32FAzWlTH96PPPKIvpM5fVcz//3vf3Xf3X333bJlyxYZOXKkvPjii7JixQp93PTPEmNu3QFU/wv96NGjLvMPcGsxMTFy6NAhPbr2z3/+U98bUM3VQM3k5ubK2LFj9Ry2Jk2aWN2cOqdfv37Or9XcKxWQoqKiZPXq1XryL2r2x6AaIfrb3/6mn6sRIvW7UM0XSk5OFtMxQlQPhIeH68erV1So545j+Mno0aNlw4YN8vnnn+uJwg6qnyoqKqSwsNClPH34P+ov8Hbt2knXrl31CIea5L9w4UL6robUaZ38/Hzp0qWL+Pr66k0FSjWBVX2t/gqnH2tOjQbdc8898t133/EzWENq5Zga1a0uLi7Oeeox3PDPEgJRPRAdHa1/WNPS0pz7iouL9QoBNccDPy0lVWFIneZJT0/XfVad+pBXKy+q96Falq9+UdCHN/5rs7y8nL6roT59+ujTjmqUzbGpv9bVXBjH1/RjzV24cEH+85//6A95fgZrRk0TuPpyI//+97/1SJti/GeJ1bO6UfOVKQcPHtSb+t82f/58/fX333+vj8+ZM8ceFBRk//TTT+2HDx+2DxgwwB4dHW0vLS21uuleYeTIkfbAwED7jh077Hl5ec7t0qVLzjIjRoyw22w2e3p6un3//v32Hj166A12+0svvaRX5GVnZ+ufL/Xcx8fHvnXrVn2cvvtlqq8yU+jHG/vLX/6i//2qn8Gvv/7anpCQYA8JCdErRhX67tb27t1r9/X1tb/66qv2b7/91v7hhx/amzZtav/ggw+cZUz+LCEQ1RGff/65DkJXb8nJyc7lki+//LI9LCxML5Hs06ePPSsry+pme43r9Z3ali1b5iyj/sH/+c9/1svJ1S+J3/72tzo0wW4fOnSoPSoqyt6oUSN769at9c+XIwwp9J17AhH9eGNPP/20PSIiQv8M3nHHHfr5d9995zxO39XM+vXr7R06dNCfE7GxsfbFixe7HK8y+LPER/3H6lEqAAAAKzGHCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAAAx3f8D/bIXaWDCCD0AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Build train/test\n",
        "x_train, plain_train, train_masks, y_train = prepare_indexed_data(clean_train_dataset, word2id, hyperparameters.max_word_len)\n",
        "x_test,  plain_test, test_masks,  y_test  = prepare_indexed_data(clean_test_dataset,  word2id, hyperparameters.max_word_len)\n",
        "x_valid,  plain_valid, valid_masks,  y_valid  = prepare_indexed_data(clean_validation_dataset,  word2id, hyperparameters.max_word_len)\n",
        "\n",
        "print(\"Shape of x_train:\", x_train.shape)\n",
        "print(\"Shape of train_masks:\", train_masks.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "\n",
        "# Sanity Check \n",
        "i = 0\n",
        "print(\"\\n--- Example ---\")\n",
        "print(\"First example real length (mask sum):\", int(train_masks[i].sum()))\n",
        "print(\"First example PAD count:\", int((train_masks[i]==0).sum()))\n",
        "\n",
        "# Check we didn't cut off too much\n",
        "sns.histplot(np.sum(train_masks.numpy(), axis=1), bins = 16)\n",
        "plt.axvline(hyperparameters.max_word_len, color='red')\n",
        "plt.show()\n",
        "\n",
        "# Prepare the datasets for model training\n",
        "train_ds = TensorDataset(x_train, train_masks, y_train)\n",
        "test_ds = TensorDataset(x_test, test_masks,y_test)\n",
        "valid_ds = TensorDataset(x_valid, valid_masks,y_valid)\n",
        "\n",
        "# Dataloaders (shuffle for train)\n",
        "train_loader = DataLoader(train_ds, batch_size=hyperparameters.batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_ds, batch_size=hyperparameters.batch_size, shuffle=False, num_workers=0)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=hyperparameters.batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the first batch from the train_loader Get the first batch from the train_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Make an embedding model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding matrix created with shape: (20264, 200)\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "fast_text_model = FastText(clean_t_dataset, # Both premise and Hypothesis \n",
        "                           vector_size=hyperparameters.embedding_dim,\n",
        "                           window=hyperparameters.emb_window_size,\n",
        "                           sg=1,\n",
        "                           epochs=hyperparameters.emb_epoch\n",
        "                           )\n",
        "\n",
        "fast_text_model.wv.most_similar('saturn', topn=10)\n",
        "\n",
        "def build_embedding_matrix(word2id, pretrained_vectors, embedding_size):\n",
        "    vocab_size = len(word2id)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_size), dtype=np.float32)\n",
        "\n",
        "    # Fill the matrix with the pre-trained vectors\n",
        "    for word, idx in word2id.items():\n",
        "        if word == '[PAD]':# If [PAD] ignore \n",
        "            continue\n",
        "        try:\n",
        "            vec = pretrained_vectors[word] #Checks the size of the vector of each word\n",
        "            if vec.shape[0] == embedding_size:\n",
        "                embedding_matrix[idx] = vec.astype(np.float32)\n",
        "            else:\n",
        "                # fallback if dims don’t match\n",
        "                embedding_matrix[idx] = np.random.normal(0.0, 0.02, size=(embedding_size,)).astype(np.float32)\n",
        "        except KeyError:\n",
        "            # special tokens or OOV start them with random values \n",
        "            embedding_matrix[idx] = np.random.normal(0.0, 0.02, size=(embedding_size,)).astype(np.float32)\n",
        "\n",
        "    print(\"Embedding matrix created with shape:\", embedding_matrix.shape)\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "embedding_matrix = build_embedding_matrix(word2id, fast_text_model.wv, hyperparameters.embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " idx  token                 first 5 dims\n",
            "--------------------------------------------------------------------------------\n",
            "3073  cac                   [ 0.06354549 -0.14943452 -0.04193051  0.21102275 -0.03419415]\n",
            "13448  paramecia             [-0.04356378  0.05170649 -0.00366549  0.07747105 -0.20206356]\n",
            "19449  vampires              [-0.06245305 -0.00057457 -0.08822831  0.09055686  0.01412382]\n",
            "12288  multitrophic          [-0.219932    0.13852207 -0.02607702  0.12100194 -0.15039049]\n",
            "11611  megohmeter            [-0.02440806 -0.05539543 -0.12427013  0.11560817 -0.12200683]\n"
          ]
        }
      ],
      "source": [
        "# Look at embedding matrix with random words\n",
        "candidates = [i for i in range(embedding_matrix.shape[0]) \n",
        "              if id2word.get(i) not in (None, \"[PAD]\")]\n",
        "\n",
        "rng = np.random\n",
        "chosen = rng.choice(candidates, size=min(5, len(candidates)), replace=False)\n",
        "\n",
        "print(f\"{'idx':>4}  {'token':<20}  first 5 dims\")\n",
        "print(\"-\" * 80)\n",
        "for i in chosen:\n",
        "    token = id2word[i]\n",
        "    print(f\"{i:>4}  {token:<20}  {embedding_matrix[i][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note we have collected following parameters from the pre-processing:\n",
        "\n",
        "- VOCAB_SIZE\n",
        "- MAX_SEQ_LEN\n",
        "- BATCH_SIZE\n",
        "- embedding_dim\n",
        "  We have collected the objects:\n",
        "- embedding_matrix\n",
        "- fast_text_model\n",
        "- clean_train_dataset, clean_test_dataset, clean_validation_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**We build a Configuration class to have all the hyperparameters together**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional Encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model A: RNN with Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSTM With Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=2, dropout=0.3, num_layers = 2):\n",
        "        super(LSTMAttentionModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "        context = self.dropout(context)\n",
        "        return self.fc(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gru with Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRUAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, unk_prob=0.1, num_layers = 2):\n",
        "        super(GRUAttentionModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
        "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 2)\n",
        "        self.unk_id = UNK_ID\n",
        "        self.unk_prob = unk_prob\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Only replace tokens during training\n",
        "        if self.training and self.unk_prob > 0:\n",
        "            mask = (x != PAD_ID) & (x != SEP_ID) & (x != CLS_ID) & (x != UNK_ID)\n",
        "            replace_mask = (torch.rand_like(x.float()) < self.unk_prob) & mask\n",
        "            x = torch.where(replace_mask, torch.full_like(x, self.unk_id), x)\n",
        "        # Rest of the forward pass...\n",
        "        embedded = self.embedding(x)\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "        gru_out = self.norm(gru_out)\n",
        "        attn_weights = torch.softmax(self.attention(gru_out), dim=1)\n",
        "        context = torch.sum(attn_weights * gru_out, dim=1)\n",
        "        context = self.dropout(context)\n",
        "        return self.fc(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model B: Transformer with Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional Encoding for the transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,d_model,max_len = 512, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0,max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:,0::2] = torch.sin(position * div_term)\n",
        "        pe[:,1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x + self.pe[:, :x.size(1), :] \n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_classes, n_heads, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        vocab_size, embed_dim = embedding_matrix.shape\n",
        "        self.d_model = embed_dim\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False, padding_idx=word2id['[PAD]'])\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=hyperparameters.max_word_len, dropout=hyperparameters.dropout)\n",
        "\n",
        "        # Transformer Encoder Layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=hyperparameters.n_heads,\n",
        "            dropout=hyperparameters.dropout,\n",
        "            dim_feedforward=256, #might add it to the hyperparams \n",
        "            batch_first=True \n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=hyperparameters.n_layers)\n",
        "\n",
        "        # Final Classification Head\n",
        "        self.classifier = nn.Linear(embed_dim,num_classes)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        padding_mask = (src_mask == 0)\n",
        "\n",
        "        # Apply embedding and positional encoding\n",
        "        embedded = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        pos_encoded = self.pos_encoder(embedded)\n",
        "\n",
        "        # Pass trough the transformer encoder\n",
        "        encoded = self.transformer_encoder(pos_encoded,src_key_padding_mask = padding_mask)\n",
        "\n",
        "        # Use the output of the [CLS] token for classification\n",
        "        cls_output = encoded[:,0,:]\n",
        "\n",
        "        # Get final logits from the classifier\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model C: Wide and Deep RNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model seeks to build on Model A by splitting into three components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRUThreeStreamNLI(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, wide_layers=2, prem_layers=2, hypo_layers=2, num_fc_layers=2, embedding_dropout=0.2, dropout=0.3, unk_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embed_dropout = nn.Dropout(embedding_dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Encoders\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True,\n",
        "                          bidirectional=True, num_layers=wide_layers, dropout=dropout if wide_layers > 1 else 0.0)\n",
        "        self.prem_encoder = nn.GRU(\n",
        "            embedding_dim, hidden_dim, num_layers=prem_layers,\n",
        "            batch_first=True, bidirectional=True, dropout=dropout if prem_layers > 1 else 0.0\n",
        "        )\n",
        "        self.hypo_encoder = nn.GRU(\n",
        "            embedding_dim, hidden_dim, num_layers=hypo_layers,\n",
        "            batch_first=True, bidirectional=True, dropout=dropout if hypo_layers > 1 else 0.0\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
        "\n",
        "\n",
        "        # Self-attention for premise and hypothesis\n",
        "        self.self_attn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        # Cross-attention for the wide stream\n",
        "        self.cross_attn_query = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.cross_attn_key = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.cross_attn_value = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "        # Fusion and classification layers\n",
        "        fusion_dim = hidden_dim * (4 + 2 + 2)  # 4H from cross + 2H + 2H = 8H total\n",
        "        fc_layers = []\n",
        "        in_dim = fusion_dim\n",
        "        out_dim = hidden_dim * 2\n",
        "\n",
        "        for i in range(num_fc_layers):\n",
        "            fc_layers.extend([\n",
        "                nn.Linear(in_dim, out_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            # for next layer\n",
        "            in_dim = out_dim\n",
        "            # Optionally shrink layer width gradually:\n",
        "            out_dim = max(hidden_dim, out_dim // 2)\n",
        "\n",
        "        self.fc_layers = nn.Sequential(*fc_layers)\n",
        "        self.classifier = nn.Linear(in_dim, 2)\n",
        "        self.unk_id = UNK_ID\n",
        "        self.unk_prob = unk_prob\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Token corruption (word dropout)\n",
        "        if self.training and self.unk_prob > 0:\n",
        "            mask = (x != PAD_ID) & (x != SEP_ID) & (x != CLS_ID) & (x != UNK_ID)\n",
        "            replace_mask = (torch.rand_like(x.float()) < self.unk_prob) & mask\n",
        "            x = torch.where(replace_mask, torch.full_like(x, self.unk_id), x)\n",
        "\n",
        "        # Split into premise/hypothesis \n",
        "        sep_positions = (x == SEP_ID).int().argmax(dim=1)\n",
        "        batch_idx = torch.arange(x.size(0), device=x.device)\n",
        "        token_range = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
        "        premise_mask = token_range < sep_positions.unsqueeze(1)\n",
        "        hypothesis_mask = token_range > sep_positions.unsqueeze(1)\n",
        "        premise = torch.where(premise_mask, x, PAD_ID)\n",
        "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
        "\n",
        "        #  Embeddings\n",
        "        embedded = self.embed_dropout(self.embedding(x).transpose(1,2)).transpose(1,2)\n",
        "        emb_prem = self.embed_dropout(self.embedding(premise).transpose(1,2)).transpose(1,2)\n",
        "        emb_hypo = self.embed_dropout(self.embedding(hypothesis).transpose(1,2)).transpose(1,2)\n",
        "\n",
        "        # Encode all three streams\n",
        "        full_out, _ = self.gru(embedded)\n",
        "        prem_out, _ = self.gru(emb_prem)\n",
        "        hypo_out, _ = self.gru(emb_hypo)\n",
        "\n",
        "        full_out = self.norm(full_out)\n",
        "        prem_out, _ = self.prem_encoder(emb_prem)\n",
        "        hypo_out, _ = self.hypo_encoder(emb_hypo)\n",
        "\n",
        "        # # Wide stream (cross-attention over full sequence)\n",
        "        # Q = self.cross_attn_query(full_out)\n",
        "        # K = self.cross_attn_key(full_out)\n",
        "        # V = self.cross_attn_value(full_out)\n",
        "        # attn_scores = torch.bmm(Q, K.transpose(1, 2)) / (Q.size(-1) ** 0.5)\n",
        "        # attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        # cross_context = torch.bmm(attn_weights, V).mean(dim=1)  # (B, hidden_dim)\n",
        "\n",
        "        # --- Bidirectional cross-attention between premise and hypothesis ---\n",
        "        # Project to query/key/value spaces\n",
        "        Q_p = self.cross_attn_query(prem_out)   # (B, Lp, H)\n",
        "        K_h = self.cross_attn_key(hypo_out)\n",
        "        V_h = self.cross_attn_value(hypo_out)\n",
        "\n",
        "        Q_h = self.cross_attn_query(hypo_out)   # (B, Lh, H)\n",
        "        K_p = self.cross_attn_key(prem_out)\n",
        "        V_p = self.cross_attn_value(prem_out)\n",
        "\n",
        "        # Premise → Hypothesis attention (H attends to P)\n",
        "        scores_h2p = torch.bmm(Q_h, K_p.transpose(1, 2)) / (Q_h.size(-1) ** 0.5)\n",
        "        weights_h2p = F.softmax(scores_h2p, dim=-1)\n",
        "        context_h2p = torch.bmm(weights_h2p, V_p)  # (B, Lh, H)\n",
        "\n",
        "        # Hypothesis → Premise attention (P attends to H)\n",
        "        scores_p2h = torch.bmm(Q_p, K_h.transpose(1, 2)) / (Q_p.size(-1) ** 0.5)\n",
        "        weights_p2h = F.softmax(scores_p2h, dim=-1)\n",
        "        context_p2h = torch.bmm(weights_p2h, V_h)  # (B, Lp, H)\n",
        "\n",
        "        # Pool both directions\n",
        "        context_h2p = context_h2p.mean(dim=1)  # (B, H)\n",
        "        context_p2h = context_p2h.mean(dim=1)  # (B, H)\n",
        "\n",
        "        # Fuse bidirectional cross contexts\n",
        "        cross_context = torch.cat([\n",
        "            context_p2h,                   # P→H\n",
        "            context_h2p,                   # H→P\n",
        "            torch.abs(context_p2h - context_h2p),  # difference\n",
        "            context_p2h * context_h2p              # element-wise product\n",
        "        ], dim=1)  # (B, 4H)\n",
        "\n",
        "        # Deep streams (self-attention pooling)\n",
        "        def self_attention_pooling(out):\n",
        "            weights = F.softmax(self.self_attn(out), dim=1)  # (B, L, 1)\n",
        "            return torch.sum(weights * out, dim=1)           # (B, 2H)\n",
        "\n",
        "        prem_context = self_attention_pooling(prem_out)\n",
        "        hypo_context = self_attention_pooling(hypo_out)\n",
        "\n",
        "        # Fusion\n",
        "        fused = torch.cat([cross_context, prem_context, hypo_context], dim=1)\n",
        "        fused = self.fc_layers(fused)\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "contain_UNK = np.zeros(len(plain_valid), dtype=bool)\n",
        "for i, sent in enumerate(plain_valid):\n",
        "    if UNK_ID in sent:\n",
        "        contain_UNK[i] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = len(word2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "def instantiate_models(gru = False, lstm = False, transformer = False, monstruosity = False):\n",
        "    \n",
        "    #Build Gru Attention\n",
        "    if gru:\n",
        "        GRU_model = GRUAttentionModel(\n",
        "            vocab_size=vocab_size,\n",
        "            embedding_dim=hyperparameters.embedding_dim,\n",
        "            hidden_dim=hyperparameters.hidden_dim,\n",
        "            dropout=hyperparameters.dropout,\n",
        "            unk_prob=0.7,\n",
        "            num_layers=hyperparameters.n_layers\n",
        "        ).to(device)\n",
        "    \n",
        "    # Build LSTM with Attention\n",
        "    if lstm:\n",
        "        LSTM_model = LSTMAttentionModel(\n",
        "            vocab_size=vocab_size,\n",
        "            embedding_dim=hyperparameters.embedding_dim,\n",
        "            hidden_dim=hyperparameters.hidden_dim,\n",
        "            dropout=hyperparameters.dropout,\n",
        "            num_layers=hyperparameters.n_layers\n",
        "        ).to(device)\n",
        "\n",
        "    # Build Transformer \n",
        "    if transformer:\n",
        "        TRANSFORMER_model = TransformerClassifier(\n",
        "            embedding_matrix=embedding_matrix,\n",
        "            num_classes=2,\n",
        "            n_heads=hyperparameters.n_heads,\n",
        "            n_layers=hyperparameters.n_layers,\n",
        "            dropout=hyperparameters.dropout\n",
        "        )\n",
        "\n",
        "    if monstruosity:\n",
        "        GRU_model2 = GRUThreeStreamNLI(\n",
        "            vocab_size=vocab_size,\n",
        "            embedding_dim=hyperparameters.embedding_dim,\n",
        "            hidden_dim=hyperparameters.hidden_dim, \n",
        "            wide_layers=2, \n",
        "            prem_layers=4, \n",
        "            hypo_layers=2, \n",
        "            num_fc_layers=2,\n",
        "            embedding_dropout=0.2,\n",
        "            dropout=0.5, \n",
        "            unk_prob=0.1\n",
        "        ).to(device)\n",
        "\n",
        "    \n",
        "    return GRU_model, LSTM_model, TRANSFORMER_model, GRU_model2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "gru_model, lstm_model, transformer_model, monstruosity = instantiate_models(gru=True, lstm=True, transformer=True, monstruosity=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Global Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for xb, mb, yb in loader: # x_train, train_mask, y_train\n",
        "        xb,mb,yb = xb.to(device), mb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb, mb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)# Gradient clipping, prevents gradients to become to large \n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * yb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        total_correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return total_loss/total, total_correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    y_train_np = y_train.cpu().numpy()\n",
        "    weights = compute_class_weight('balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float).to(device))\n",
        "\n",
        "    for xb, mb, yb in loader:\n",
        "        xb, mb, yb = xb.to(device), mb.to(device), yb.to(device).long()\n",
        "        out = model(xb, mb)\n",
        "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "        loss = criterion(logits, yb)\n",
        "        batch_size = yb.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == yb).sum().item()\n",
        "        total += batch_size\n",
        "\n",
        "    return total_loss / total, total_correct / total\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, optimizer, n_epochs=10, patience=3,criterion=nn.CrossEntropyLoss()):\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer,criterion)\n",
        "        val_loss, val_acc = evaluate_model(model, valid_loader)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs} - \"\n",
        "              f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f} - \"\n",
        "              f\"Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            epochs_no_improve = 0\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def train_models(gru = False, lstm = False, transformer = False, monstruosity = False ):\n",
        "    # Train GRU \n",
        "    if gru:\n",
        "        gru_model.to(device)\n",
        "        optimizer = torch.optim.AdamW(gru_model.parameters(), lr=hyperparameters.lr, weight_decay=hyperparameters.wd)\n",
        "        trained_GRU_model, history = train_model(gru_model, train_loader, valid_loader, optimizer, n_epochs=hyperparameters.train_num_epoch, patience=5)\n",
        "    \n",
        "    # Train LSTM \n",
        "    if lstm:\n",
        "        lstm_model.to(device)\n",
        "        optimizer = torch.optim.AdamW(gru_model.parameters(), lr=hyperparameters.lr, weight_decay=hyperparameters.wd)\n",
        "        trained_LSTM_model, history = train_model(lstm_model, train_loader, valid_loader, optimizer, n_epochs=hyperparameters.train_num_epoch, patience=5)\n",
        "\n",
        "    # Train Transformer \n",
        "    if transformer:\n",
        "        transformer_model.to(device)\n",
        "        optimizer = torch.optim.AdamW(transformer_model.parameters(), lr = hyperparameters.lr, weight_decay=hyperparameters.wd)\n",
        "        trained_Transformer_model, history = train_model(transformer_model, train_loader=train_loader, valid_loader=valid_loader,optimizer=optimizer, n_epochs=hyperparameters.train_num_epoch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7 - Train loss: 0.6638, Train acc: 0.6192 - Val loss: 0.7655, Val acc: 0.5177\n",
            "Epoch 2/7 - Train loss: 0.6420, Train acc: 0.6412 - Val loss: 0.7811, Val acc: 0.5338\n",
            "Epoch 3/7 - Train loss: 0.6270, Train acc: 0.6553 - Val loss: 0.8240, Val acc: 0.5561\n",
            "Epoch 4/7 - Train loss: 0.6209, Train acc: 0.6603 - Val loss: 0.7682, Val acc: 0.5737\n",
            "Epoch 5/7 - Train loss: 0.6078, Train acc: 0.6717 - Val loss: 0.7795, Val acc: 0.5952\n",
            "Epoch 6/7 - Train loss: 0.5988, Train acc: 0.6766 - Val loss: 0.8412, Val acc: 0.5806\n",
            "Epoch 7/7 - Train loss: 0.5780, Train acc: 0.6974 - Val loss: 0.7592, Val acc: 0.6060\n"
          ]
        }
      ],
      "source": [
        "# You can train one like : train_models(gru=True)\n",
        "train_models(transformer=True)\n",
        "# Or many:\n",
        "#train_models(gru=False, lstm=True,transformer=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "def model_validation(model):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in test_loader:\n",
        "            input_ids      = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels         = labels.to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(\"Accuracy:\", round(accuracy_score(all_labels, all_preds), 2))\n",
        "    print(f\"F1 Score: {f1_score(all_labels, all_preds):.2f}\")\n",
        "    print(\"Precision:\", round(precision_score(all_labels, all_preds), 2))\n",
        "    print(\"Recall:\", round(recall_score(all_labels, all_preds), 2))\n",
        "    print(\"-\" * 40)\n",
        "    print(classification_report(all_labels, all_preds, target_names=[\"neutral\", \"entails\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.66\n",
            "F1 Score: 0.36\n",
            "Precision: 0.71\n",
            "Recall: 0.24\n",
            "----------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral       0.65      0.93      0.77      1275\n",
            "     entails       0.71      0.24      0.36       842\n",
            "\n",
            "    accuracy                           0.66      2117\n",
            "   macro avg       0.68      0.59      0.57      2117\n",
            "weighted avg       0.67      0.66      0.61      2117\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_validation(transformer_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
