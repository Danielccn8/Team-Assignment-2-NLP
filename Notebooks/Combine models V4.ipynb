{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 CITS4012 Project\n",
        "\n",
        "_Make sure you change the file name with your group id._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import copy\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "if not nltk.download('punkt'):\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "if not nltk.download('stopwords'):\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Stemmer and Lemmatizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "if not nltk.download('wordnet'):\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('omw-1.4')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sys.path.append(os.path.abspath('..')) \n",
        "import math\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "sys.path.append(os.path.abspath('../')) # Points to the current folder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "# Helper functions for training\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **We build a Configuration class to have all the hyperparameters together**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class construct for hyperparameters \n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.min_word_len = 3\n",
        "        self.max_word_len = 64\n",
        "        self.batch_size = 64\n",
        "        self.embedding_dim = 200\n",
        "        self.hidden_dim = 64\n",
        "        self.max_combined_len = 130 #to join premise + hypothesis + cls + sep \n",
        "        self.patience = 5\n",
        "        #Embedding\n",
        "        self.embedding_method = \"\"\n",
        "        self.emb_lr = 0.0005\n",
        "        self.emb_epoch = 3\n",
        "        self.emb_window_size = 5\n",
        "        # Models\n",
        "        self.n_heads = 2\n",
        "        self.n_layers = 1\n",
        "        self.dropout = 0.3\n",
        "        self.num_classes = 2\n",
        "        self.lr = 0.00005\n",
        "        self.wd = 0.0005\n",
        "        self.train_num_epoch = 7\n",
        "        self.label_smoothing = 0.0\n",
        "\n",
        "    def display(self): # Display all the config parameters \n",
        "        print(\"=\" * 60)\n",
        "        print(\"CONFIGURATION PARAMETERS\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"\\nEmbedding Parameters:\")\n",
        "        print(f\"  Method: {self.embedding_method}\")\n",
        "        print(f\"  Max length: {self.max_word_len}\")\n",
        "        print(f\"  Learning rate: {self.emb_lr}\")\n",
        "        print(f\"  Embedding size: {self.embedding_dim}\")\n",
        "        print(f\"  Epochs: {self.emb_epoch}\")\n",
        "        print(f\"  Window size: {self.emb_window_size}\")\n",
        "        \n",
        "        print(\"\\nTransformer Parameters:\")\n",
        "        print(f\"  Batch size: {self.batch_size}\")\n",
        "        print(f\"  Number of heads: {self.n_heads}\")\n",
        "        print(f\"  Number of layers: {self.n_layers}\")\n",
        "        print(f\"  Dropout: {self.dropout}\")\n",
        "        print(f\"  Number of classes: {self.num_classes}\")\n",
        "        print(f\"  Learning rate: {self.lr}\")\n",
        "        print(f\"  Weight decay: {self.wd}\")\n",
        "        print(f\"  Training epochs: {self.train_num_epoch}\")\n",
        "\n",
        "        print(f\"  Label smoothing: {self.label_smoothing}\")\n",
        "        print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparameters = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.Dataset Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# These are common English contractions.\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "# Helper function for lemmatization with POS tagging\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if no match\n",
        "    \n",
        "# Allowed POS tags for filtering (example: nouns, verbs, adjectives, adverbs)\n",
        "allowed_pos_tags = {'NN', 'NNS', 'NNP', 'NNPS',   # Nouns\n",
        "                    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Verbs\n",
        "                    'JJ', 'JJR', 'JJS',           # Adjectives\n",
        "                    'RB', 'RBR', 'RBS'}           # Adverbs\n",
        "\n",
        "# Filter the desired POS tags\n",
        "def filter_tokens_by_pos(tokens):\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    filtered = [word for word, tag in tagged_tokens if tag in allowed_pos_tags]\n",
        "    return filtered\n",
        "\n",
        "def clean_dataset(dataset, min_word_len, max_word_len, method=['stem, lemmatize'], pos_filter=False, stop_w=False, clean_nums = False):\n",
        "    cleaned_premises = []\n",
        "    cleaned_hypotheses = []\n",
        "    cleaned_labels = []\n",
        "\n",
        "    for index, row in dataset.iterrows():\n",
        "        premise = row['premise']\n",
        "        hypothesis = row['hypothesis']\n",
        "        label = row['label']\n",
        "\n",
        "        # Lowercase\n",
        "        premise = premise.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "        # Expand contractions\n",
        "        for contraction, full_form in contraction_dict.items():\n",
        "            premise = premise.replace(contraction, full_form)\n",
        "            hypothesis = hypothesis.replace(contraction, full_form)\n",
        "\n",
        "        # Remove punctuation/special chars\n",
        "        premise = re.sub(r'[^a-zA-Z0-9\\s.-]', ' ', premise)\n",
        "        hypothesis = re.sub(r'[^a-zA-Z0-9\\s.-]', ' ', hypothesis)\n",
        "\n",
        "        # Replace underscores/hyphens with spaces, then normalize whitespace\n",
        "        premise = re.sub(r'[-–—_]+', ' ', premise)\n",
        "        hypothesis = re.sub(r'[-–—_]+', ' ', hypothesis)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        premise = re.sub(r'\\s+', ' ', premise).strip()\n",
        "        hypothesis = re.sub(r'\\s+', ' ', hypothesis).strip()\n",
        "\n",
        "        # Tokenization\n",
        "        premise_tokens = word_tokenize(premise)\n",
        "        hypothesis_tokens = word_tokenize(hypothesis)\n",
        "\n",
        "        # # Replace numbers with '<NUM>'\n",
        "        if clean_nums is True:\n",
        "            premise_tokens = ['[NUM]' if any(char.isdigit() for char in word) else word for word in premise_tokens]\n",
        "            hypothesis_tokens = ['[NUM]' if any(char.isdigit() for char in word) else word for word in hypothesis_tokens]\n",
        "\n",
        "\n",
        "        # Stemming/Lemmatization\n",
        "        if 'stem' in method:\n",
        "            stemmer = PorterStemmer()\n",
        "            premise_tokens = [stemmer.stem(word) for word in premise_tokens]\n",
        "            hypothesis_tokens = [stemmer.stem(word) for word in hypothesis_tokens]\n",
        "        elif 'lemmatize' in method:\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            premise_pos_tags = pos_tag(premise_tokens)\n",
        "            hypothesis_pos_tags = pos_tag(hypothesis_tokens)\n",
        "            premise_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in premise_pos_tags]\n",
        "            hypothesis_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in hypothesis_pos_tags]\n",
        "\n",
        "        # POS Filtering\n",
        "        if pos_filter:\n",
        "            premise_tokens = filter_tokens_by_pos(premise_tokens)\n",
        "            hypothesis_tokens = filter_tokens_by_pos(hypothesis_tokens)\n",
        "\n",
        "        # Remove stop words\n",
        "        if stop_w:\n",
        "            premise_tokens = [word for word in premise_tokens if word not in stop_words]\n",
        "            hypothesis_tokens = [word for word in hypothesis_tokens if word not in stop_words]\n",
        "\n",
        "        # Now check token length AFTER cleaning\n",
        "        if (min_word_len <= len(premise_tokens) <= max_word_len and\n",
        "            min_word_len <= len(hypothesis_tokens) <= max_word_len):\n",
        "            cleaned_premises.append(premise_tokens)\n",
        "            cleaned_hypotheses.append(hypothesis_tokens)\n",
        "            cleaned_labels.append(label)\n",
        "        # else: skip row\n",
        "\n",
        "    # Build DataFrame from all cleaned token lists\n",
        "    new_dataset = pd.DataFrame({\n",
        "        'premise': cleaned_premises,\n",
        "        'hypothesis': cleaned_hypotheses,\n",
        "        'label': cleaned_labels\n",
        "    })\n",
        "\n",
        "    return new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Make the Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw data\n",
        "train_df = pd.read_json('../Dataset/train.json')\n",
        "test_df = pd.read_json('../Dataset/test.json')\n",
        "validation_df = pd.read_json('../Dataset/validation.json')\n",
        "\n",
        "# Clean datasets. MAX_WORD_LENGTH set to 64 to remove very long texts\n",
        "clean_train_dataset = clean_dataset(train_df, hyperparameters.min_word_len, hyperparameters.max_word_len, stop_w=True, method=[], pos_filter=False)\n",
        "clean_test_dataset = clean_dataset(test_df, hyperparameters.min_word_len, hyperparameters.max_word_len,stop_w=True, method=[], pos_filter= False )\n",
        "clean_validation_dataset = clean_dataset(validation_df, hyperparameters.min_word_len, hyperparameters.max_word_len,stop_w=True, method=[], pos_filter= False )\n",
        "\n",
        "# Combine clean premises and hypotheses\n",
        "clean_t_dataset = clean_train_dataset['premise'] + clean_train_dataset['hypothesis']\n",
        "clean_t_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sum(train_df['label']=='entails') / (sum(train_df['label']!='entails') + sum(train_df['label']=='entails'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sum(validation_df['label']=='entails') / (sum(validation_df['label']!='entails') + sum(validation_df['label']=='entails'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sum(test_df['label']=='entails') / (sum(test_df['label']!='entails') + sum(test_df['label']=='entails'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a unique word list from the cleaned dataset\n",
        "unique_words = set()\n",
        "for sentence in clean_t_dataset: # Both Premise and Hypothesis \n",
        "    for word in sentence:\n",
        "        unique_words.add(word)\n",
        "        \n",
        "unique_words_list = sorted(list(unique_words))\n",
        "\n",
        "# Make dictionary of words and indices\n",
        "word2id = {w:i for i,w in enumerate(unique_words_list)}\n",
        "id2word = {i:w for i,w in enumerate(unique_words_list)}\n",
        "\n",
        "\n",
        "# Add special tokens to use later\n",
        "SPECIAL_TOKENS = ['[PAD]','[UNK]','[CLS]','[SEP]'] \n",
        "for tok in SPECIAL_TOKENS:\n",
        "    if tok not in word2id:\n",
        "        idx = len(word2id)\n",
        "        word2id[tok] = idx\n",
        "        id2word[idx] = tok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEP_ID = word2id['[SEP]']\n",
        "UNK_ID = word2id['[UNK]']\n",
        "PAD_ID = word2id['[PAD]']\n",
        "CLS_ID = word2id['[CLS]']\n",
        "\n",
        "\n",
        "def prepare_indexed_data(df, word2id, max_len):\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    labels = []\n",
        "\n",
        "    label_map = {'neutral': 0, 'entails': 1}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        premise_toks = row['premise']\n",
        "        hypothesis_toks = row['hypothesis']\n",
        "        label = row['label']\n",
        "\n",
        "        # Add special tokens \n",
        "        tokens = [CLS_ID] \\\n",
        "                + [word2id.get(w,UNK_ID) for w in premise_toks] \\\n",
        "                + [SEP_ID] \\\n",
        "                + [word2id.get(w,UNK_ID) for w in hypothesis_toks] \n",
        "        # Truncate\n",
        "        tokens = tokens[:max_len] #to combine len of hypothesis and premise, plus cls and sep  \n",
        "\n",
        "        # Attention mask \n",
        "        attn = [1] * len(tokens)\n",
        "\n",
        "        # Pad\n",
        "        pad_len = max_len - len(tokens) # To fill the [PAD]\n",
        "        if pad_len > 0:\n",
        "            tokens += [PAD_ID] * pad_len\n",
        "            attn += [0] * pad_len # FLag positions as padding \n",
        "\n",
        "        input_ids.append(tokens)\n",
        "        attention_mask.append(attn)\n",
        "        labels.append(label_map[label])\n",
        "\n",
        "    return (torch.LongTensor(input_ids),\n",
        "            np.array(input_ids),\n",
        "            torch.LongTensor(attention_mask),\n",
        "            torch.LongTensor(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build train/test\n",
        "x_train, plain_train, train_masks, y_train = prepare_indexed_data(clean_train_dataset, word2id, hyperparameters.max_combined_len)\n",
        "x_test,  plain_test, test_masks,  y_test  = prepare_indexed_data(clean_test_dataset,  word2id, hyperparameters.max_combined_len)\n",
        "x_valid,  plain_valid, valid_masks,  y_valid  = prepare_indexed_data(clean_validation_dataset,  word2id, hyperparameters.max_combined_len)\n",
        "\n",
        "print(\"Shape of x_train:\", x_train.shape)\n",
        "print(\"Shape of train_masks:\", train_masks.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "\n",
        "# Sanity Check \n",
        "i = 0\n",
        "print(\"\\n--- Example ---\")\n",
        "print(\"First example real length (mask sum):\", int(train_masks[i].sum()))\n",
        "print(\"First example PAD count:\", int((train_masks[i]==0).sum()))\n",
        "\n",
        "# Check we didn't cut off too much\n",
        "sns.histplot(np.sum(train_masks.numpy(), axis=1), bins = 16)\n",
        "plt.axvline(hyperparameters.max_word_len, color='red')\n",
        "plt.show()\n",
        "\n",
        "# Prepare the datasets for model training\n",
        "train_ds = TensorDataset(x_train, train_masks, y_train)\n",
        "test_ds = TensorDataset(x_test, test_masks,y_test)\n",
        "valid_ds = TensorDataset(x_valid, valid_masks,y_valid)\n",
        "\n",
        "# Dataloaders (shuffle for train)\n",
        "train_loader = DataLoader(train_ds, batch_size=hyperparameters.batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_ds, batch_size=hyperparameters.batch_size, shuffle=False, num_workers=0)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=hyperparameters.batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the first batch from the train_loader Get the first batch from the train_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Make an embedding model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "fast_text_model = FastText(clean_t_dataset, # Both premise and Hypothesis \n",
        "                           vector_size=hyperparameters.embedding_dim,\n",
        "                           window=hyperparameters.emb_window_size,\n",
        "                           sg=1,\n",
        "                           epochs=hyperparameters.emb_epoch\n",
        "                           )\n",
        "\n",
        "fast_text_model.wv.most_similar('saturn', topn=10)\n",
        "\n",
        "def build_embedding_matrix(word2id, pretrained_vectors, embedding_size):\n",
        "    vocab_size = len(word2id)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_size), dtype=np.float32)\n",
        "\n",
        "    # Fill the matrix with the pre-trained vectors\n",
        "    for word, idx in word2id.items():\n",
        "        if word == '[PAD]':# If [PAD] ignore \n",
        "            continue\n",
        "        try:\n",
        "            vec = pretrained_vectors[word] #Checks the size of the vector of each word\n",
        "            if vec.shape[0] == embedding_size:\n",
        "                embedding_matrix[idx] = vec.astype(np.float32)\n",
        "            else:\n",
        "                # fallback if dims don’t match\n",
        "                embedding_matrix[idx] = np.random.normal(0.0, 0.02, size=(embedding_size,)).astype(np.float32)\n",
        "        except KeyError:\n",
        "            # special tokens or OOV start them with random values \n",
        "            embedding_matrix[idx] = np.random.normal(0.0, 0.02, size=(embedding_size,)).astype(np.float32)\n",
        "\n",
        "    print(\"Embedding matrix created with shape:\", embedding_matrix.shape)\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "embedding_matrix = build_embedding_matrix(word2id, fast_text_model.wv, hyperparameters.embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look at embedding matrix with random words\n",
        "candidates = [i for i in range(embedding_matrix.shape[0]) \n",
        "              if id2word.get(i) not in (None, \"[PAD]\")]\n",
        "\n",
        "rng = np.random\n",
        "chosen = rng.choice(candidates, size=min(5, len(candidates)), replace=False)\n",
        "\n",
        "print(f\"{'idx':>4}  {'token':<20}  first 5 dims\")\n",
        "print(\"-\" * 80)\n",
        "for i in chosen:\n",
        "    token = id2word[i]\n",
        "    print(f\"{i:>4}  {token:<20}  {embedding_matrix[i][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note we have collected following parameters from the pre-processing:\n",
        "\n",
        "- VOCAB_SIZE\n",
        "- MAX_SEQ_LEN\n",
        "- BATCH_SIZE\n",
        "- embedding_dim\n",
        "  We have collected the objects:\n",
        "- embedding_matrix\n",
        "- fast_text_model\n",
        "- clean_train_dataset, clean_test_dataset, clean_validation_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**We build a Configuration class to have all the hyperparameters together**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional Encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model A: RNN with Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSTM With Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=2, dropout=0.3, num_layers = 2):\n",
        "        super(LSTMAttentionModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "    def forward(self, x, mask=None, return_weights = False):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "        context = self.dropout(context)\n",
        "\n",
        "        logits = self.fc(context)\n",
        "        if return_weights:\n",
        "            return logits, attn_weights\n",
        "        else:\n",
        "            return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gru with Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRUAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, unk_prob=0.1, num_layers = 2):\n",
        "        super(GRUAttentionModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
        "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 2)\n",
        "        self.unk_id = UNK_ID\n",
        "        self.unk_prob = unk_prob\n",
        "\n",
        "    def forward(self, x, mask=None, return_weights = False):\n",
        "        # Only replace tokens during training\n",
        "        if self.training and self.unk_prob > 0:\n",
        "            mask = (x != PAD_ID) & (x != SEP_ID) & (x != CLS_ID) & (x != UNK_ID)\n",
        "            replace_mask = (torch.rand_like(x.float()) < self.unk_prob) & mask\n",
        "            x = torch.where(replace_mask, torch.full_like(x, self.unk_id), x)\n",
        "        # Rest of the forward pass...\n",
        "        embedded = self.embedding(x)\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "        gru_out = self.norm(gru_out)\n",
        "        attn_weights = torch.softmax(self.attention(gru_out), dim=1)\n",
        "        context = torch.sum(attn_weights * gru_out, dim=1)\n",
        "        context = self.dropout(context)\n",
        "\n",
        "        logits = self.fc(context)\n",
        "        # Possibiity to return the attention weights \n",
        "        if return_weights:\n",
        "            return logits, attn_weights\n",
        "        else:\n",
        "            return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model B: Transformer with Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional Encoding for the transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,d_model,max_len = 512, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0,max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:,0::2] = torch.sin(position * div_term)\n",
        "        pe[:,1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x + self.pe[:, :x.size(1), :] \n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_classes, n_heads, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        vocab_size, embed_dim = embedding_matrix.shape\n",
        "        self.d_model = embed_dim\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False, padding_idx=word2id['[PAD]'])\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=hyperparameters.max_combined_len, dropout=hyperparameters.dropout)\n",
        "\n",
        "        # Transformer Encoder Layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=n_heads,\n",
        "            dropout=hyperparameters.dropout,\n",
        "            dim_feedforward=(embed_dim*4),\n",
        "            batch_first=True \n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=n_layers)\n",
        "\n",
        "        # Final Classification Head\n",
        "        #self.classifier = nn.Linear(embed_dim,num_classes)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim, embed_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        padding_mask = (src_mask == 0)\n",
        "\n",
        "        # Apply embedding and positional encoding\n",
        "        embedded = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        pos_encoded = self.pos_encoder(embedded)\n",
        "\n",
        "        # Pass trough the transformer encoder\n",
        "        encoded = self.transformer_encoder(pos_encoded,src_key_padding_mask = padding_mask)\n",
        "\n",
        "        # Use the output of the [CLS] token for classification\n",
        "        cls_output = encoded[:,0,:]\n",
        "\n",
        "        # Get final logits from the classifier\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "    \n",
        "    # Get attention from the first layer\n",
        "    def get_attention_weights(self, src, src_mask):\n",
        "        # Get embeddings and positional encoding\n",
        "        padding_mask = (src_mask == 0)\n",
        "        embedded = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        pos_encoded = self.pos_encoder(embedded)\n",
        "\n",
        "        # Access the FIRST encoder layer \n",
        "        first_encoder_layer = self.transformer_encoder.layers[0]\n",
        "\n",
        "        # Call its self-attention module with need_weights=True\n",
        "        _, attn_weights = first_encoder_layer.self_attn(\n",
        "            pos_encoded, pos_encoded, pos_encoded,\n",
        "            key_padding_mask=padding_mask,\n",
        "            need_weights=True,\n",
        "            average_attn_weights=False\n",
        "        )\n",
        "\n",
        "        return attn_weights.cpu().detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model C: Wide and Deep RNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model seeks to build on Model A by splitting into three components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRUThreeStreamNLI(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, wide_layers=2, prem_layers=2, hypo_layers=2, num_fc_layers=2, embedding_dropout=0.2, dropout=0.3, unk_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embed_dropout = nn.Dropout(embedding_dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Encoders\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True,\n",
        "                          bidirectional=True, num_layers=wide_layers, dropout=dropout if wide_layers > 1 else 0.0)\n",
        "        self.prem_encoder = nn.GRU(\n",
        "            embedding_dim, hidden_dim, num_layers=prem_layers,\n",
        "            batch_first=True, bidirectional=True, dropout=dropout if prem_layers > 1 else 0.0\n",
        "        )\n",
        "        self.hypo_encoder = nn.GRU(\n",
        "            embedding_dim, hidden_dim, num_layers=hypo_layers,\n",
        "            batch_first=True, bidirectional=True, dropout=dropout if hypo_layers > 1 else 0.0\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
        "\n",
        "\n",
        "        # Self-attention for premise and hypothesis\n",
        "        self.self_attn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        # Cross-attention for the wide stream\n",
        "        self.cross_attn_query = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.cross_attn_key = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.cross_attn_value = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "        # Fusion and classification layers\n",
        "        fusion_dim = hidden_dim * (4 + 2 + 2)  # 4H from cross + 2H + 2H = 8H total\n",
        "        fc_layers = []\n",
        "        in_dim = fusion_dim\n",
        "        out_dim = hidden_dim * 2\n",
        "\n",
        "        for i in range(num_fc_layers):\n",
        "            fc_layers.extend([\n",
        "                nn.Linear(in_dim, out_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            # for next layer\n",
        "            in_dim = out_dim\n",
        "            # Optionally shrink layer width gradually:\n",
        "            out_dim = max(hidden_dim, out_dim // 2)\n",
        "\n",
        "        self.fc_layers = nn.Sequential(*fc_layers)\n",
        "        self.classifier = nn.Linear(in_dim, 2)\n",
        "        self.unk_id = UNK_ID\n",
        "        self.unk_prob = unk_prob\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Token corruption (word dropout)\n",
        "        if self.training and self.unk_prob > 0:\n",
        "            mask = (x != PAD_ID) & (x != SEP_ID) & (x != CLS_ID) & (x != UNK_ID)\n",
        "            replace_mask = (torch.rand_like(x.float()) < self.unk_prob) & mask\n",
        "            x = torch.where(replace_mask, torch.full_like(x, self.unk_id), x)\n",
        "\n",
        "        # Split into premise/hypothesis \n",
        "        sep_positions = (x == SEP_ID).int().argmax(dim=1)\n",
        "        batch_idx = torch.arange(x.size(0), device=x.device)\n",
        "        token_range = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
        "        premise_mask = token_range < sep_positions.unsqueeze(1)\n",
        "        hypothesis_mask = token_range > sep_positions.unsqueeze(1)\n",
        "        premise = torch.where(premise_mask, x, PAD_ID)\n",
        "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
        "\n",
        "        #  Embeddings\n",
        "        embedded = self.embed_dropout(self.embedding(x).transpose(1,2)).transpose(1,2)\n",
        "        emb_prem = self.embed_dropout(self.embedding(premise).transpose(1,2)).transpose(1,2)\n",
        "        emb_hypo = self.embed_dropout(self.embedding(hypothesis).transpose(1,2)).transpose(1,2)\n",
        "\n",
        "        # Encode all three streams\n",
        "        full_out, _ = self.gru(embedded)\n",
        "        prem_out, _ = self.gru(emb_prem)\n",
        "        hypo_out, _ = self.gru(emb_hypo)\n",
        "\n",
        "        full_out = self.norm(full_out)\n",
        "        prem_out, _ = self.prem_encoder(emb_prem)\n",
        "        hypo_out, _ = self.hypo_encoder(emb_hypo)\n",
        "\n",
        "        # # Wide stream (cross-attention over full sequence)\n",
        "        # Q = self.cross_attn_query(full_out)\n",
        "        # K = self.cross_attn_key(full_out)\n",
        "        # V = self.cross_attn_value(full_out)\n",
        "        # attn_scores = torch.bmm(Q, K.transpose(1, 2)) / (Q.size(-1) ** 0.5)\n",
        "        # attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        # cross_context = torch.bmm(attn_weights, V).mean(dim=1)  # (B, hidden_dim)\n",
        "\n",
        "        # --- Bidirectional cross-attention between premise and hypothesis ---\n",
        "        # Project to query/key/value spaces\n",
        "        Q_p = self.cross_attn_query(prem_out)   # (B, Lp, H)\n",
        "        K_h = self.cross_attn_key(hypo_out)\n",
        "        V_h = self.cross_attn_value(hypo_out)\n",
        "\n",
        "        Q_h = self.cross_attn_query(hypo_out)   # (B, Lh, H)\n",
        "        K_p = self.cross_attn_key(prem_out)\n",
        "        V_p = self.cross_attn_value(prem_out)\n",
        "\n",
        "        # Premise → Hypothesis attention (H attends to P)\n",
        "        scores_h2p = torch.bmm(Q_h, K_p.transpose(1, 2)) / (Q_h.size(-1) ** 0.5)\n",
        "        weights_h2p = F.softmax(scores_h2p, dim=-1)\n",
        "        context_h2p = torch.bmm(weights_h2p, V_p)  # (B, Lh, H)\n",
        "\n",
        "        # Hypothesis → Premise attention (P attends to H)\n",
        "        scores_p2h = torch.bmm(Q_p, K_h.transpose(1, 2)) / (Q_p.size(-1) ** 0.5)\n",
        "        weights_p2h = F.softmax(scores_p2h, dim=-1)\n",
        "        context_p2h = torch.bmm(weights_p2h, V_h)  # (B, Lp, H)\n",
        "\n",
        "        # Pool both directions\n",
        "        context_h2p = context_h2p.mean(dim=1)  # (B, H)\n",
        "        context_p2h = context_p2h.mean(dim=1)  # (B, H)\n",
        "\n",
        "        # Fuse bidirectional cross contexts\n",
        "        cross_context = torch.cat([\n",
        "            context_p2h,                   # P→H\n",
        "            context_h2p,                   # H→P\n",
        "            torch.abs(context_p2h - context_h2p),  # difference\n",
        "            context_p2h * context_h2p              # element-wise product\n",
        "        ], dim=1)  # (B, 4H)\n",
        "\n",
        "        # Deep streams (self-attention pooling)\n",
        "        def self_attention_pooling(out):\n",
        "            weights = F.softmax(self.self_attn(out), dim=1)  # (B, L, 1)\n",
        "            return torch.sum(weights * out, dim=1)           # (B, 2H)\n",
        "\n",
        "        prem_context = self_attention_pooling(prem_out)\n",
        "        hypo_context = self_attention_pooling(hypo_out)\n",
        "\n",
        "        # Fusion\n",
        "        fused = torch.cat([cross_context, prem_context, hypo_context], dim=1)\n",
        "        fused = self.fc_layers(fused)\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "contain_UNK = np.zeros(len(plain_valid), dtype=bool)\n",
        "for i, sent in enumerate(plain_valid):\n",
        "    if UNK_ID in sent:\n",
        "        contain_UNK[i] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = len(word2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def instantiate_models(gru=False, lstm=False, transformer=False, monstruosity=False):\n",
        "    \n",
        "    models = {} \n",
        "\n",
        "    # Build Gru Attention\n",
        "    if gru:\n",
        "        models['gru'] = GRUAttentionModel(\n",
        "            vocab_size=vocab_size,\n",
        "            embedding_dim=hyperparameters.embedding_dim,\n",
        "            hidden_dim=hyperparameters.hidden_dim,\n",
        "            dropout=hyperparameters.dropout,\n",
        "            unk_prob=0.7,\n",
        "            num_layers=hyperparameters.n_layers\n",
        "        ).to(device)\n",
        "    \n",
        "    # Build LSTM with Attention\n",
        "    if lstm:\n",
        "        models['lstm'] = LSTMAttentionModel(\n",
        "            vocab_size=vocab_size,\n",
        "            embedding_dim=hyperparameters.embedding_dim,\n",
        "            hidden_dim=hyperparameters.hidden_dim,\n",
        "            dropout=hyperparameters.dropout,\n",
        "            num_layers=hyperparameters.n_layers\n",
        "        ).to(device)\n",
        "\n",
        "    # Build Transformer \n",
        "    if transformer:\n",
        "        models['transformer'] = TransformerClassifier(\n",
        "            embedding_matrix=embedding_matrix,\n",
        "            num_classes=2,\n",
        "            n_heads=hyperparameters.n_heads,\n",
        "            n_layers=hyperparameters.n_layers,\n",
        "            dropout=hyperparameters.dropout\n",
        "        ).to(device) \n",
        "\n",
        "    if monstruosity:\n",
        "        models['monstruosity'] = GRUThreeStreamNLI(\n",
        "            vocab_size=vocab_size,\n",
        "            embedding_dim=hyperparameters.embedding_dim,\n",
        "            hidden_dim=hyperparameters.hidden_dim, \n",
        "            wide_layers=2, \n",
        "            prem_layers=4, \n",
        "            hypo_layers=2, \n",
        "            num_fc_layers=2,\n",
        "            embedding_dropout=0.2,\n",
        "            dropout=0.5, \n",
        "            unk_prob=0.1\n",
        "        ).to(device)\n",
        "\n",
        "    return models "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = instantiate_models(gru=True, lstm=True, transformer=True, monstruosity=True)\n",
        "#transformer_model = instantiate_models(transformer=True)\n",
        "#lstm_model = instantiate_models(lstm=True)\n",
        "#monstruosity_model = instantiate_models(monstruosity=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Global Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for xb, mb, yb in loader: # x_train, train_mask, y_train\n",
        "        xb,mb,yb = xb.to(device), mb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb, mb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)# Gradient clipping, prevents gradients to become to large \n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * yb.size(0)\n",
        "        preds = logits.argmax(1)\n",
        "        total_correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return total_loss/total, total_correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for xb, mb, yb in loader:\n",
        "        xb, mb, yb = xb.to(device), mb.to(device), yb.to(device).long()\n",
        "        out = model(xb, mb)\n",
        "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "        loss = criterion(logits, yb)\n",
        "        batch_size = yb.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == yb).sum().item()\n",
        "        total += batch_size\n",
        "\n",
        "    return total_loss / total, total_correct / total\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, optimizer, n_epochs=10, patience=3,criterion=nn.CrossEntropyLoss()):\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer,criterion)\n",
        "        val_loss, val_acc = evaluate_model(model, valid_loader, criterion)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs} - \"\n",
        "              f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f} - \"\n",
        "              f\"Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            epochs_no_improve = 0\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def train_models(models_to_train):\n",
        "    trained_models = {}\n",
        "    histories = {}\n",
        "\n",
        "    # Calculate class weights\n",
        "    y_train_np = y_train.cpu().numpy()\n",
        "    weights = compute_class_weight('balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float).to(device))\n",
        "\n",
        "    # Iterate through the models that were passed in\n",
        "    for name, model in models_to_train.items():\n",
        "        print(f\"--- Training {name.upper()} Model ---\")\n",
        "        model.to(device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparameters.lr, weight_decay=hyperparameters.wd)\n",
        "        \n",
        "        trained_model, history = train_model(\n",
        "            model, \n",
        "            train_loader, \n",
        "            valid_loader, \n",
        "            optimizer, \n",
        "            n_epochs=hyperparameters.train_num_epoch, \n",
        "            patience=5, \n",
        "            criterion=criterion\n",
        "        )\n",
        "        \n",
        "        trained_models[name] = trained_model\n",
        "        histories[name] = history\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return trained_models, histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# You can train one like : train_models(gru=True)\n",
        "\n",
        "trained_models, histories = train_models(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print model acuracy in samples with and without UNK tokens \n",
        "def analyze_unk_performance(model, data_loader, plain_data):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Get all predictions and labels\n",
        "    with torch.no_grad():\n",
        "        for inputs, masks, labels in data_loader:\n",
        "            outputs = model(inputs.to(device), masks.to(device))\n",
        "            preds = torch.max(outputs, dim=1)[1]\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    correct = (predictions == all_labels)\n",
        "\n",
        "    # Check which original sentences contain the UNK_ID\n",
        "    contain_UNK = np.array([UNK_ID in sent for sent in plain_data])\n",
        "\n",
        "    # Calculate accuracy for sentences with UNK tokens\n",
        "    unk_correct = correct[contain_UNK]\n",
        "    unk_accuracy = unk_correct.mean() if len(unk_correct) > 0 else 0\n",
        "\n",
        "    # Calculate accuracy for sentences without UNK tokens\n",
        "    no_unk_correct = correct[~contain_UNK]\n",
        "    no_unk_accuracy = no_unk_correct.mean() if len(no_unk_correct) > 0 else 0\n",
        "\n",
        "    print(\"--- UNK Token Analysis ---\")\n",
        "    print(f\"Accuracy on samples WITH [UNK] tokens:   {unk_accuracy:.4f} ({len(unk_correct)} samples)\")\n",
        "    print(f\"Accuracy on samples WITHOUT [UNK] tokens: {no_unk_accuracy:.4f} ({len(no_unk_correct)} samples)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "def model_validation(model):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in test_loader:\n",
        "            input_ids      = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels         = labels.to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(\"Accuracy:\", round(accuracy_score(all_labels, all_preds), 2))\n",
        "    print(f\"F1 Score: {f1_score(all_labels, all_preds):.2f}\")\n",
        "    print(\"Precision:\", round(precision_score(all_labels, all_preds), 2))\n",
        "    print(\"Recall:\", round(recall_score(all_labels, all_preds), 2))\n",
        "    print(\"-\" * 40)\n",
        "    print(classification_report(all_labels, all_preds, target_names=[\"neutral\", \"entails\"]))\n",
        "\n",
        "    print(\"\\n\")\n",
        "    analyze_unk_performance(model, test_loader, plain_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_all_models(trained_models):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL MODEL EVALUATION ON TEST SET\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    for model_name, model in trained_models.items():\n",
        "        print(f\"--- Evaluating {model_name.upper()} Model ---\")\n",
        "        model_validation(model)\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the models that were trained\n",
        "evaluate_all_models(trained_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots and visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the trainning loss vs the validation loss\n",
        "def plot_training_histories(histories):\n",
        "    if not histories:\n",
        "        print(\"No histories to plot.\")\n",
        "        return\n",
        "\n",
        "    for model_name, history in histories.items():\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(history['train_loss'], label='Training Loss', color='blue', marker='o')\n",
        "        plt.plot(history['val_loss'], label='Validation Loss', color='orange', marker='o')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(f'Training & Validation Loss for {model_name.upper()} Model')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_training_histories(histories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot the Attention of the models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rnn_attention(tokens, attention_weights):\n",
        "    # 1. Remove padding from tokens for a cleaner plot\n",
        "    try:\n",
        "        real_len = tokens.index('[PAD]')\n",
        "        tokens_no_padding = tokens[:real_len]\n",
        "        weights_no_padding = attention_weights.squeeze().cpu().numpy()[:real_len]\n",
        "    except ValueError:\n",
        "        tokens_no_padding = tokens\n",
        "        weights_no_padding = attention_weights.squeeze().cpu().numpy()\n",
        "\n",
        "    # 2. Create the bar plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x=tokens_no_padding, y=weights_no_padding, palette='viridis')\n",
        "    plt.title('RNN Attention Weights')\n",
        "    plt.xlabel('Tokens')\n",
        "    plt.ylabel('Attention Score')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Specific Plot for transformer\n",
        "def plot_attention_head(attention_weights, tokens, head_index):\n",
        "    \"\"\"Plots a readable heatmap for a single attention head, ignoring padding.\"\"\"\n",
        "    \n",
        "    # 1. Find the actual length of the sentence by finding the first [PAD] token\n",
        "    try:\n",
        "        real_len = tokens.index('[PAD]')\n",
        "    except ValueError:\n",
        "        real_len = len(tokens)\n",
        "\n",
        "    # 2. Slice the tokens and weights to remove padding\n",
        "    tokens_no_padding = tokens[:real_len]\n",
        "    head_weights = attention_weights[0, head_index][:real_len, :real_len]\n",
        "    \n",
        "    # 3. Create the plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        head_weights, \n",
        "        xticklabels=tokens_no_padding, \n",
        "        yticklabels=tokens_no_padding, \n",
        "        cmap='viridis', \n",
        "        ax=ax\n",
        "    )\n",
        "    \n",
        "    ax.set_title(f'Attention Head #{head_index}')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Master function to visualize attention\n",
        "def visualize_attention(model_name, model, sample_token_ids, sample_mask):\n",
        "\n",
        "    print(f\"--- Visualizing Attention for {model_name.upper()} Model ---\")\n",
        "    \n",
        "    # Convert token IDs back to words for plotting\n",
        "    plot_tokens = [id2word[int(token_id)] for token_id in sample_token_ids]\n",
        "    \n",
        "    model.eval() # Set model to evaluation mode\n",
        "    \n",
        "    # Add a batch dimension for model input\n",
        "    sample_token_ids = sample_token_ids.unsqueeze(0).to(device)\n",
        "    sample_mask = sample_mask.unsqueeze(0).to(device)\n",
        "\n",
        "    if model_name in ['gru', 'lstm']:\n",
        "        # For RNNs, call forward with return_weights=True\n",
        "        _, attention_weights = model(sample_token_ids, sample_mask, return_weights=True)\n",
        "        plot_rnn_attention(plot_tokens, attention_weights.detach())\n",
        "        \n",
        "    elif model_name == 'transformer':\n",
        "        # For Transformer, call the special get_attention_weights method\n",
        "        # First, ensure the method exists\n",
        "        if not hasattr(model, 'get_attention_weights'):\n",
        "            print(\"Error: The Transformer model needs a 'get_attention_weights' method.\")\n",
        "            return\n",
        "            \n",
        "        attention_weights = model.get_attention_weights(sample_token_ids, sample_mask)\n",
        "        \n",
        "        # Plot the attention for each head\n",
        "        num_heads = attention_weights.shape[1]\n",
        "        for i in range(num_heads):\n",
        "            plot_attention_head(attention_weights.detach().cpu().numpy(), plot_tokens, head_index=i)\n",
        "    else:\n",
        "        print(f\"Attention visualization for model type '{model_name}' is not supported yet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a sample to visualize (e.g., the 5th sentence in the training set)\n",
        "sample_idx = 5\n",
        "sample_tokens = x_train[sample_idx]\n",
        "sample_mask = train_masks[sample_idx]\n",
        "\n",
        "# Visualize attention for the trained GRU model\n",
        "if 'gru' in trained_models:\n",
        "    visualize_attention('gru', trained_models['gru'], sample_tokens, sample_mask)\n",
        "\n",
        "if 'lstm' in trained_models:\n",
        "    visualize_attention('lstm', trained_models['lstm'], sample_tokens, sample_mask)\n",
        "\n",
        "# Visualize attention for the trained Transformer model\n",
        "if 'transformer' in trained_models:\n",
        "    visualize_attention('transformer', trained_models['transformer'], sample_tokens, sample_mask)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
