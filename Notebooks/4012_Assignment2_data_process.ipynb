{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b192e0-210a-49f4-8626-8246026e2e23",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba3422-24b7-46c4-86c4-31ce441812a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Do initial data cleaning and summary statistics (e.g. mean, max length etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232a741b-3c79-4d97-8494-d04cc22d4c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN DATASET ANALYSIS\n",
      "==================================================\n",
      "Total entries: 23088\n",
      "Label distribution: {'neutral': 14618, 'entails': 8470}\n",
      "\n",
      "Premise length - Avg: 18.1, Max: 10587, 95th %ile: 33.0\n",
      "Hypothesis length - Avg: 11.7, Max: 36, 95th %ile: 20.0\n",
      "Total length (P+H) - Avg: 29.8, Max: 10605, 95th %ile: 48.0\n",
      "\n",
      "Complexity score - Avg: 0.3, Max: 9\n",
      "Entries with complex logic (>2): 49/23088\n",
      "\n",
      "Longest premises (>30 words):\n",
      "  5: Pluto is about 39 times more distant from the Sun than is the Earth, and it takes about 250 Earth ye...\n",
      "  19: Even if you don't live near a stream or river (though most people do), there are many little things ...\n",
      "  31: Slugs, believe it or not have a very important purpose. They are decomposers, which means they eats ...\n",
      "\n",
      "Most complex examples (score >= 3):\n",
      "  186: Score 3\n",
      "     Premise: If a child is having difficulty, she explains, \"then everyone, parents, child an...\n",
      "     Hypothesis: Children resemble their parents because they have similar dna.\n",
      "  270: Score 3\n",
      "     Premise: When the National Academy of Sciences published a guide on teaching evolution la...\n",
      "     Hypothesis: Hail during a storm describes water in a solid state.\n",
      "  537: Score 3\n",
      "     Premise: Regardless of changing weather... all squirrels instinctively prepare for winter...\n",
      "     Hypothesis: One way to change water from a liquid to a solid is to decrease the temperature.\n",
      "\n",
      " TEST DATASET ANALYSIS\n",
      "==================================================\n",
      "Total entries: 2126\n",
      "Label distribution: {'neutral': 1284, 'entails': 842}\n",
      "\n",
      "Premise length - Avg: 16.8, Max: 46, 95th %ile: 31.0\n",
      "Hypothesis length - Avg: 12.5, Max: 28, 95th %ile: 21.0\n",
      "Total length (P+H) - Avg: 29.3, Max: 64, 95th %ile: 48.0\n",
      "\n",
      "Complexity score - Avg: 0.3, Max: 3\n",
      "Entries with complex logic (>2): 4/2126\n",
      "\n",
      "Longest premises (>30 words):\n",
      "  14: It is the diametrically opposite process of nuclear fission, in which an atom of the heavy isotope U...\n",
      "  55: There are spark plugs in the engine that ignite the gasoline, causing an explosion that pushes  the ...\n",
      "  58: The significance of these changes lies in the demonstration that they were associated with the relea...\n",
      "\n",
      "Most complex examples (score >= 3):\n",
      "  0: Score 3\n",
      "     Premise: Based on the list provided of the uses of substances 1-7, estimate the pH of eac...\n",
      "     Hypothesis: If a substance has a ph value greater than 7,that indicates that it is base.\n",
      "  604: Score 3\n",
      "     Premise: The pH estimate of the ER lumen is based on an indirect assay which indicates th...\n",
      "     Hypothesis: If a substance has a ph value greater than 7,that indicates that it is base.\n",
      "  760: Score 3\n",
      "     Premise: Obviously, we do not apply Mendelian inheritance crosses to humans, although Ped...\n",
      "     Hypothesis: Pedigree is utilized to analyze simple mendelian inheritance.\n",
      "\n",
      " VALIDATION DATASET ANALYSIS\n",
      "==================================================\n",
      "Total entries: 1304\n",
      "Label distribution: {'neutral': 647, 'entails': 657}\n",
      "\n",
      "Premise length - Avg: 17.3, Max: 52, 95th %ile: 34.0\n",
      "Hypothesis length - Avg: 12.4, Max: 30, 95th %ile: 21.0\n",
      "Total length (P+H) - Avg: 29.7, Max: 72, 95th %ile: 50.0\n",
      "\n",
      "Complexity score - Avg: 0.2, Max: 3\n",
      "Entries with complex logic (>2): 1/1304\n",
      "\n",
      "Longest premises (>30 words):\n",
      "  28: And in December 2010, Governor David Paterson issued an Executive Order calling for a temporary time...\n",
      "  32: Meanwhile, Bittman discovers that extracting natural gas through fracking delays our transition to r...\n",
      "  33: Its uses include organic and inorganic chemical processing, textile and pulp bleaching, metal treati...\n",
      "\n",
      "Most complex examples (score >= 3):\n",
      "  1210: Score 3\n",
      "     Premise: It seems clear that most research suggests that drinking milk is more likely to ...\n",
      "     Hypothesis: Smoking tobaccowould  most likely have a negative impact on the ability of an individual to succeed at physical activities because it it decreases stamina and cardiovascular efficiency.\n",
      "\n",
      " RECOMMENDATION ANALYSIS\n",
      "==================================================\n",
      "Overall average sequence length (P+H): 29.8 words\n",
      "Overall average complexity score: 0.3\n",
      "Sequences > 50 words: 3.4%\n",
      "Sequences with complex logic (>2): 0.2%\n",
      "\n",
      " ARCHITECTURE RECOMMENDATION:\n",
      "RECOMMEND: LSTM (default for NLI tasks)\n",
      "   - NLI typically involves moderate complexity\n",
      "   - LSTM provides better performance margin\n",
      "   - More impressive for academic purposes\n",
      "\n",
      " Additional factors:\n",
      "- Academic context: LSTM shows deeper understanding\n",
      "- Attention requirement: LSTM+attention is a strong combination\n",
      "- You can implement GRU later as comparative model\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = {\n",
    "    \"train\": \"train.json\",\n",
    "    \"test\": \"test.json\",\n",
    "    \"validation\": \"validation.json\"\n",
    "\n",
    "}\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def analyze_sequences(premises, hypotheses, labels, dataset_name):\n",
    "    premise_lengths = [len(str(p).split()) for p in premises]\n",
    "    hypothesis_lengths = [len(str(h).split()) for h in hypotheses]\n",
    "    total_lengths = [p_len + h_len for p_len, h_len in zip(premise_lengths, hypothesis_lengths)]\n",
    "    \n",
    "    complex_words = ['although', 'however', 'therefore', 'because', 'if', 'then', \n",
    "                    'unless', 'while', 'despite', 'according to', 'based on',\n",
    "                    'estimate', 'calculate', 'determine', 'analyze', 'conclude']\n",
    "    \n",
    "    complexity_scores = []\n",
    "    for p, h in zip(premises, hypotheses):\n",
    "        text = f\"{p} {h}\".lower()\n",
    "        score = sum(1 for word in complex_words if word in text)\n",
    "        complexity_scores.append(score)\n",
    "    \n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    print(f\"\\n {dataset_name.upper()} DATASET ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"Total entries: {len(premises)}\")\n",
    "    print(f\"Label distribution: {dict(label_counts)}\")\n",
    "    \n",
    "    print(f\"\\nPremise length - Avg: {np.mean(premise_lengths):.1f}, Max: {max(premise_lengths)}, 95th %ile: {np.percentile(premise_lengths, 95):.1f}\")\n",
    "    print(f\"Hypothesis length - Avg: {np.mean(hypothesis_lengths):.1f}, Max: {max(hypothesis_lengths)}, 95th %ile: {np.percentile(hypothesis_lengths, 95):.1f}\")\n",
    "\n",
    "    #Add total length string of Premise and Hypothesis to get TOTAL string length\n",
    "    print(f\"Total length (P+H) - Avg: {np.mean(total_lengths):.1f}, Max: {max(total_lengths)}, 95th %ile: {np.percentile(total_lengths, 95):.1f}\")\n",
    "    \n",
    "    print(f\"\\nComplexity score - Avg: {np.mean(complexity_scores):.1f}, Max: {max(complexity_scores)}\")\n",
    "    print(f\"Entries with complex logic (>2): {sum(1 for score in complexity_scores if score > 2)}/{len(complexity_scores)}\")\n",
    "    \n",
    "    print(f\"\\nLongest premises (>30 words):\")\n",
    "    long_indices = [i for i, length in enumerate(premise_lengths) if length > 30]\n",
    "    for i in long_indices[:3]: \n",
    "        print(f\"  {i}: {premises[i][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nMost complex examples (score >= 3):\")\n",
    "    complex_indices = [i for i, score in enumerate(complexity_scores) if score >= 3]\n",
    "    for i in complex_indices[:3]:  \n",
    "        print(f\"  {i}: Score {complexity_scores[i]}\")\n",
    "        print(f\"     Premise: {premises[i][:80]}...\")\n",
    "        print(f\"     Hypothesis: {hypotheses[i]}\")\n",
    "    \n",
    "    return {\n",
    "        'premise_lengths': premise_lengths,\n",
    "        'hypothesis_lengths': hypothesis_lengths,\n",
    "        'total_lengths': total_lengths,\n",
    "        'complexity_scores': complexity_scores,\n",
    "        'label_counts': label_counts\n",
    "    }\n",
    "\n",
    "all_analyses = {}\n",
    "\n",
    "for name, file_path in files.items():\n",
    "    try:\n",
    "        data = load_json(file_path)\n",
    "        \n",
    "        premises_dict = data['premise']\n",
    "        hypotheses_dict = data['hypothesis'] \n",
    "        labels_dict = data['label']\n",
    "        \n",
    "        premises = [premises_dict[key] for key in sorted(premises_dict.keys(), key=int)]\n",
    "        hypotheses = [hypotheses_dict[key] for key in sorted(hypotheses_dict.keys(), key=int)]\n",
    "        labels = [labels_dict[key] for key in sorted(labels_dict.keys(), key=int)]\n",
    "        \n",
    "        analysis = analyze_sequences(premises, hypotheses, labels, name)\n",
    "        all_analyses[name] = analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error analyzing {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n RECOMMENDATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_total_lengths = []\n",
    "all_complexity_scores = []\n",
    "\n",
    "for name, analysis in all_analyses.items():\n",
    "    all_total_lengths.extend(analysis['total_lengths'])\n",
    "    all_complexity_scores.extend(analysis['complexity_scores'])\n",
    "\n",
    "avg_total_length = np.mean(all_total_lengths)\n",
    "avg_complexity = np.mean(all_complexity_scores)\n",
    "pct_long_sequences = sum(1 for length in all_total_lengths if length > 50) / len(all_total_lengths) * 100\n",
    "pct_complex_sequences = sum(1 for score in all_complexity_scores if score > 2) / len(all_complexity_scores) * 100\n",
    "\n",
    "print(f\"Overall average sequence length (P+H): {avg_total_length:.1f} words\")\n",
    "print(f\"Overall average complexity score: {avg_complexity:.1f}\")\n",
    "print(f\"Sequences > 50 words: {pct_long_sequences:.1f}%\")\n",
    "print(f\"Sequences with complex logic (>2): {pct_complex_sequences:.1f}%\")\n",
    "\n",
    "print(f\"\\n ARCHITECTURE RECOMMENDATION:\")\n",
    "\n",
    "if avg_total_length > 40 or pct_long_sequences > 20 or avg_complexity > 2.5:\n",
    "    print(\"RECOMMEND: LSTM\")\n",
    "    print(\"   - Longer sequences benefit from LSTM's cell state\")\n",
    "    print(\"   - Complex reasoning patterns need fine-grained gating\")\n",
    "    print(\"   - Better for preserving long-range dependencies\")\n",
    "elif avg_total_length < 20 and avg_complexity < 1.5:\n",
    "    print(\"RECOMMEND: GRU\") \n",
    "    print(\"   - Shorter sequences work well with GRU\")\n",
    "    print(\"   - Simpler patterns don't need LSTM's complexity\")\n",
    "    print(\"   - Faster training and simpler implementation\")\n",
    "else:\n",
    "    print(\"RECOMMEND: LSTM (default for NLI tasks)\")\n",
    "    print(\"   - NLI typically involves moderate complexity\")\n",
    "    print(\"   - LSTM provides better performance margin\")\n",
    "    print(\"   - More impressive for academic purposes\")\n",
    "\n",
    "print(f\"\\n Additional factors:\")\n",
    "print(\"- Academic context: LSTM shows deeper understanding\")\n",
    "print(\"- Attention requirement: LSTM+attention is a strong combination\")\n",
    "print(\"- You can implement GRU later as comparative model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbda43-2fde-4a9f-a778-00049a2f6de6",
   "metadata": {},
   "source": [
    "Found outlier of max length of 10587. This is alarming as this can skew the data, given that max length of premise of test and validation is 45 -55 we can restrict the training data to have this range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14996f1b-6fab-446d-bcad-bb36933d5f58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### First how many does it catch if premise  x <= 80 words. Arbitrarily chose 80 as starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45ddd07-67c9-4a60-bb46-e3ba14fb30b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTLIER DETECTION ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Analyzing: train.json\n",
      "Total entries: 23088\n",
      "Outliers found: 9\n",
      "Outlier percentage: 0.04%\n",
      "\n",
      "Top 5 outliers (showing worst offenders):\n",
      "Index 1104: 10605 words total\n",
      "  Premise: 10587 words\n",
      "  Hypothesis: 18 words\n",
      "  Label: neutral\n",
      "  Premise preview: The History of Polar Front and Air Mass Concept in the United States -\tThe best explanation for how ...\n",
      "\n",
      "Index 1129: 6258 words total\n",
      "  Premise: 6247 words\n",
      "  Hypothesis: 11 words\n",
      "  Label: entails\n",
      "  Premise preview: Structure of Plant Cell Walls XLIII.\tA cell wall is not present in animal cells.\tneutral\tA cell wall...\n",
      "\n",
      "Index 1484: 2413 words total\n",
      "  Premise: 2388 words\n",
      "  Hypothesis: 25 words\n",
      "  Label: neutral\n",
      "  Premise preview: The fat in whole milk helps one-year olds get all the fat they need to be healthy and grow well, pro...\n",
      "\n",
      "Index 995: 1284 words total\n",
      "  Premise: 1270 words\n",
      "  Hypothesis: 14 words\n",
      "  Label: neutral\n",
      "  Premise preview: We have all these flowering apple trees, and they're all in bloom.\tA sign that an apple tree is goin...\n",
      "\n",
      "Index 270: 1026 words total\n",
      "  Premise: 1016 words\n",
      "  Hypothesis: 10 words\n",
      "  Label: entails\n",
      "  Premise preview: When the National Academy of Sciences published a guide on teaching evolution last month, it was no ...\n",
      "\n",
      "\n",
      "Analyzing: test.json\n",
      "Total entries: 2126\n",
      "Outliers found: 0\n",
      "Outlier percentage: 0.00%\n",
      "✓ No outliers found!\n",
      "\n",
      "Analyzing: validation.json\n",
      "Total entries: 1304\n",
      "Outliers found: 0\n",
      "Outlier percentage: 0.00%\n",
      "✓ No outliers found!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "MAX_TOTAL_LENGTH = 100    \n",
    "MAX_PREMISE_LENGTH = 80    \n",
    "MAX_HYPOTHESIS_LENGTH = 40  \n",
    "\n",
    "def find_outliers(data, thresholds):\n",
    "    outliers = []\n",
    "    \n",
    "    premises_dict = data['premise']\n",
    "    hypotheses_dict = data['hypothesis']\n",
    "    labels_dict = data['label']\n",
    "    \n",
    "    premises = [premises_dict[key] for key in sorted(premises_dict.keys(), key=int)]\n",
    "    hypotheses = [hypotheses_dict[key] for key in sorted(hypotheses_dict.keys(), key=int)]\n",
    "    labels = [labels_dict[key] for key in sorted(labels_dict.keys(), key=int)]\n",
    "    \n",
    "    for i, (premise, hypothesis, label) in enumerate(zip(premises, hypotheses, labels)):\n",
    "        premise_words = len(str(premise).split())\n",
    "        hypothesis_words = len(str(hypothesis).split())\n",
    "        total_words = premise_words + hypothesis_words\n",
    "        \n",
    "        if (total_words > thresholds['max_total'] or \n",
    "            premise_words > thresholds['max_premise'] or \n",
    "            hypothesis_words > thresholds['max_hypothesis']):\n",
    "            \n",
    "            outliers.append({\n",
    "                'index': i,\n",
    "                'premise': premise,\n",
    "                'hypothesis': hypothesis, \n",
    "                'label': label,\n",
    "                'premise_length': premise_words,\n",
    "                'hypothesis_length': hypothesis_words,\n",
    "                'total_length': total_words\n",
    "            })\n",
    "    \n",
    "    return outliers, premises, hypotheses, labels\n",
    "\n",
    "files_to_check = [\"train.json\", \"test.json\", \"validation.json\"]\n",
    "thresholds = {\n",
    "    'max_total': MAX_TOTAL_LENGTH,\n",
    "    'max_premise': MAX_PREMISE_LENGTH,\n",
    "    'max_hypothesis': MAX_HYPOTHESIS_LENGTH\n",
    "}\n",
    "\n",
    "print(\"OUTLIER DETECTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for file_path in files_to_check:\n",
    "    try:\n",
    "        print(f\"\\nAnalyzing: {file_path}\")\n",
    "        data = json.load(open(file_path, \"r\", encoding=\"utf-8\"))\n",
    "        \n",
    "        outliers, premises, hypotheses, labels = find_outliers(data, thresholds)\n",
    "        \n",
    "        print(f\"Total entries: {len(premises)}\")\n",
    "        print(f\"Outliers found: {len(outliers)}\")\n",
    "        print(f\"Outlier percentage: {len(outliers)/len(premises)*100:.2f}%\")\n",
    "        \n",
    "        if outliers:\n",
    "            print(f\"\\nTop 5 outliers (showing worst offenders):\")\n",
    "            # Sort by total length descending\n",
    "            sorted_outliers = sorted(outliers, key=lambda x: x['total_length'], reverse=True)\n",
    "            for outlier in sorted_outliers[:5]:\n",
    "                print(f\"Index {outlier['index']}: {outlier['total_length']} words total\")\n",
    "                print(f\"  Premise: {outlier['premise_length']} words\")\n",
    "                print(f\"  Hypothesis: {outlier['hypothesis_length']} words\")\n",
    "                print(f\"  Label: {outlier['label']}\")\n",
    "                print(f\"  Premise preview: {outlier['premise'][:100]}...\")\n",
    "                print()\n",
    "                \n",
    "        else:\n",
    "            print(\"✓ No outliers found!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212927ff-f6ef-46e6-ac0b-7e3989d17861",
   "metadata": {},
   "source": [
    "Analysis\n",
    "\n",
    "9 outliers were found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba5d02-9d97-472e-924c-0b32e40f492b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### How about if restrict the threshold further -- premise x <= 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3be20f-707f-464a-9ba3-a3a118cc94ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTLIER DETECTION ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Analyzing: train.json\n",
      "Total entries: 23088\n",
      "Outliers found: 10\n",
      "Outlier percentage: 0.04%\n",
      "\n",
      "Top 5 outliers (showing worst offenders):\n",
      "Index 1104: 10605 words total\n",
      "  Premise: 10587 words\n",
      "  Hypothesis: 18 words\n",
      "  Label: neutral\n",
      "  Premise preview: The History of Polar Front and Air Mass Concept in the United States -\tThe best explanation for how ...\n",
      "\n",
      "Index 1129: 6258 words total\n",
      "  Premise: 6247 words\n",
      "  Hypothesis: 11 words\n",
      "  Label: entails\n",
      "  Premise preview: Structure of Plant Cell Walls XLIII.\tA cell wall is not present in animal cells.\tneutral\tA cell wall...\n",
      "\n",
      "Index 1484: 2413 words total\n",
      "  Premise: 2388 words\n",
      "  Hypothesis: 25 words\n",
      "  Label: neutral\n",
      "  Premise preview: The fat in whole milk helps one-year olds get all the fat they need to be healthy and grow well, pro...\n",
      "\n",
      "Index 995: 1284 words total\n",
      "  Premise: 1270 words\n",
      "  Hypothesis: 14 words\n",
      "  Label: neutral\n",
      "  Premise preview: We have all these flowering apple trees, and they're all in bloom.\tA sign that an apple tree is goin...\n",
      "\n",
      "Index 270: 1026 words total\n",
      "  Premise: 1016 words\n",
      "  Hypothesis: 10 words\n",
      "  Label: entails\n",
      "  Premise preview: When the National Academy of Sciences published a guide on teaching evolution last month, it was no ...\n",
      "\n",
      "\n",
      "Analyzing: test.json\n",
      "Total entries: 2126\n",
      "Outliers found: 0\n",
      "Outlier percentage: 0.00%\n",
      "✓ No outliers found!\n",
      "\n",
      "Analyzing: validation.json\n",
      "Total entries: 1304\n",
      "Outliers found: 0\n",
      "Outlier percentage: 0.00%\n",
      "✓ No outliers found!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "MAX_TOTAL_LENGTH = 100    \n",
    "MAX_PREMISE_LENGTH = 55 \n",
    "MAX_HYPOTHESIS_LENGTH = 40  \n",
    "\n",
    "def find_outliers(data, thresholds):\n",
    "    outliers = []\n",
    "    \n",
    "    premises_dict = data['premise']\n",
    "    hypotheses_dict = data['hypothesis']\n",
    "    labels_dict = data['label']\n",
    "    \n",
    "    premises = [premises_dict[key] for key in sorted(premises_dict.keys(), key=int)]\n",
    "    hypotheses = [hypotheses_dict[key] for key in sorted(hypotheses_dict.keys(), key=int)]\n",
    "    labels = [labels_dict[key] for key in sorted(labels_dict.keys(), key=int)]\n",
    "    \n",
    "    for i, (premise, hypothesis, label) in enumerate(zip(premises, hypotheses, labels)):\n",
    "        premise_words = len(str(premise).split())\n",
    "        hypothesis_words = len(str(hypothesis).split())\n",
    "        total_words = premise_words + hypothesis_words\n",
    "        \n",
    "        if (total_words > thresholds['max_total'] or \n",
    "            premise_words > thresholds['max_premise'] or \n",
    "            hypothesis_words > thresholds['max_hypothesis']):\n",
    "            \n",
    "            outliers.append({\n",
    "                'index': i,\n",
    "                'premise': premise,\n",
    "                'hypothesis': hypothesis, \n",
    "                'label': label,\n",
    "                'premise_length': premise_words,\n",
    "                'hypothesis_length': hypothesis_words,\n",
    "                'total_length': total_words\n",
    "            })\n",
    "    \n",
    "    return outliers, premises, hypotheses, labels\n",
    "\n",
    "files_to_check = [\"train.json\", \"test.json\", \"validation.json\"]\n",
    "thresholds = {\n",
    "    'max_total': MAX_TOTAL_LENGTH,\n",
    "    'max_premise': MAX_PREMISE_LENGTH,\n",
    "    'max_hypothesis': MAX_HYPOTHESIS_LENGTH\n",
    "}\n",
    "\n",
    "print(\"OUTLIER DETECTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for file_path in files_to_check:\n",
    "    try:\n",
    "        print(f\"\\nAnalyzing: {file_path}\")\n",
    "        data = json.load(open(file_path, \"r\", encoding=\"utf-8\"))\n",
    "        \n",
    "        outliers, premises, hypotheses, labels = find_outliers(data, thresholds)\n",
    "        \n",
    "        print(f\"Total entries: {len(premises)}\")\n",
    "        print(f\"Outliers found: {len(outliers)}\")\n",
    "        print(f\"Outlier percentage: {len(outliers)/len(premises)*100:.2f}%\")\n",
    "        \n",
    "        if outliers:\n",
    "            print(f\"\\nTop 5 outliers (showing worst offenders):\")\n",
    "            # Sort by total length descending\n",
    "            sorted_outliers = sorted(outliers, key=lambda x: x['total_length'], reverse=True)\n",
    "            for outlier in sorted_outliers[:5]:\n",
    "                print(f\"Index {outlier['index']}: {outlier['total_length']} words total\")\n",
    "                print(f\"  Premise: {outlier['premise_length']} words\")\n",
    "                print(f\"  Hypothesis: {outlier['hypothesis_length']} words\")\n",
    "                print(f\"  Label: {outlier['label']}\")\n",
    "                print(f\"  Premise preview: {outlier['premise'][:100]}...\")\n",
    "                print()\n",
    "                \n",
    "        else:\n",
    "            print(\"✓ No outliers found!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf8eee7-a66c-472e-a12e-f5a2ed2b98f8",
   "metadata": {},
   "source": [
    "Now there are 10 outliers found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae3d3a-1f33-4e21-a5a2-fd739189b1f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65008c1d-3da2-4d26-ae22-2a4452773529",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### For 9 outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356e6572-d5e5-4b5c-9364-f83e8752922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " COMPLETE OUTLIER HANDLING PIPELINE\n",
      "============================================================\n",
      "\n",
      "Processing: train.json\n",
      "Total entries: 23088\n",
      "Outliers found: 9\n",
      "✓ Outliers exported to: train_outliers.txt\n",
      "✓ Clean dataset created: train_clean.json\n",
      "  Removed 9 outliers, 23079 entries remaining\n",
      "\n",
      "Processing: validation.json\n",
      "Total entries: 1304\n",
      "Outliers found: 0\n",
      "✓ No outliers found - using original data\n",
      "\n",
      "Processing: test.json\n",
      "Total entries: 2126\n",
      "Outliers found: 0\n",
      "✓ No outliers found - using original data\n",
      "\n",
      " OUTLIER PROCESSING COMPLETE\n",
      "============================================================\n",
      "Now continuing with clean data...\n",
      "train.json: 23079 clean entries\n",
      "validation.json: 1304 clean entries\n",
      "test.json: 2126 clean entries\n",
      "\n",
      " READY FOR NEXT STEPS:\n",
      "- Text preprocessing (tokenization, cleaning)\n",
      "- Label encoding (entails/neutral to numerical)\n",
      "- LSTM model implementation\n",
      "- Training with clean data\n",
      "\n",
      " Sample from clean training data:\n",
      "First premise: Pluto rotates once on its axis every 6.39 Earth days;...\n",
      "First hypothesis: Earth rotates on its axis once times in one day.\n",
      "First label: neutral\n"
     ]
    }
   ],
   "source": [
    "# RUN CODE TO EXPORT OUTLIERS TO A TXT FILE -> train_outliers.txt\n",
    "\n",
    "# Configuration\n",
    "MAX_TOTAL_LENGTH = 100\n",
    "MAX_PREMISE_LENGTH = 80  \n",
    "MAX_HYPOTHESIS_LENGTH = 40\n",
    "\n",
    "def find_outliers(data, thresholds):\n",
    "    \"\"\"Find entries that exceed length thresholds.\"\"\"\n",
    "    outliers = []\n",
    "    \n",
    "    premises_dict = data['premise']\n",
    "    hypotheses_dict = data['hypothesis']\n",
    "    labels_dict = data['label']\n",
    "    \n",
    "    # Convert to lists and sort by numeric keys\n",
    "    premises = [premises_dict[key] for key in sorted(premises_dict.keys(), key=int)]\n",
    "    hypotheses = [hypotheses_dict[key] for key in sorted(hypotheses_dict.keys(), key=int)]\n",
    "    labels = [labels_dict[key] for key in sorted(labels_dict.keys(), key=int)]\n",
    "    \n",
    "    for i, (premise, hypothesis, label) in enumerate(zip(premises, hypotheses, labels)):\n",
    "        premise_words = len(str(premise).split())\n",
    "        hypothesis_words = len(str(hypothesis).split())\n",
    "        total_words = premise_words + hypothesis_words\n",
    "        \n",
    "        if (total_words > thresholds['max_total'] or \n",
    "            premise_words > thresholds['max_premise'] or \n",
    "            hypothesis_words > thresholds['max_hypothesis']):\n",
    "            \n",
    "            outliers.append({\n",
    "                'index': i,\n",
    "                'premise': premise,\n",
    "                'hypothesis': hypothesis, \n",
    "                'label': label,\n",
    "                'premise_length': premise_words,\n",
    "                'hypothesis_length': hypothesis_words,\n",
    "                'total_length': total_words\n",
    "            })\n",
    "    \n",
    "    return outliers, premises, hypotheses, labels\n",
    "\n",
    "def export_outliers(outliers, output_path):\n",
    "    \"\"\"Export outliers to a TXT file.\"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"OUTLIERS DETECTED\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Total outliers found: {len(outliers)}\\n\\n\")\n",
    "        \n",
    "        for outlier in outliers:\n",
    "            f.write(f\"INDEX: {outlier['index']}\\n\")\n",
    "            f.write(f\"TOTAL WORDS: {outlier['total_length']}\\n\")\n",
    "            f.write(f\"PREMISE WORDS: {outlier['premise_length']}\\n\")\n",
    "            f.write(f\"HYPOTHESIS WORDS: {outlier['hypothesis_length']}\\n\")\n",
    "            f.write(f\"LABEL: {outlier['label']}\\n\")\n",
    "            f.write(f\"PREMISE: {outlier['premise']}\\n\")\n",
    "            f.write(f\"HYPOTHESIS: {outlier['hypothesis']}\\n\")\n",
    "            f.write(\"-\" * 60 + \"\\n\\n\")\n",
    "\n",
    "def create_clean_dataset(premises, hypotheses, labels, outlier_indices, output_path):\n",
    "    \"\"\"Create a clean dataset without outliers.\"\"\"\n",
    "    clean_premises = [p for i, p in enumerate(premises) if i not in outlier_indices]\n",
    "    clean_hypotheses = [h for i, h in enumerate(hypotheses) if i not in outlier_indices]\n",
    "    clean_labels = [l for i, l in enumerate(labels) if i not in outlier_indices]\n",
    "    \n",
    "    clean_data = {\n",
    "        'premise': dict(enumerate(clean_premises)),\n",
    "        'hypothesis': dict(enumerate(clean_hypotheses)),\n",
    "        'label': dict(enumerate(clean_labels))\n",
    "    }\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(clean_data, f, indent=2)\n",
    "    \n",
    "    return clean_premises, clean_hypotheses, clean_labels\n",
    "\n",
    "# Main processing pipeline\n",
    "print(\" COMPLETE OUTLIER HANDLING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "files_to_process = [\"train.json\", \"validation.json\", \"test.json\"]\n",
    "thresholds = {\n",
    "    'max_total': MAX_TOTAL_LENGTH,\n",
    "    'max_premise': MAX_PREMISE_LENGTH,\n",
    "    'max_hypothesis': MAX_HYPOTHESIS_LENGTH\n",
    "}\n",
    "\n",
    "all_clean_data = {}\n",
    "\n",
    "for file_path in files_to_process:\n",
    "    print(f\"\\nProcessing: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load and analyze\n",
    "        data = json.load(open(file_path, \"r\", encoding=\"utf-8\"))\n",
    "        outliers, premises, hypotheses, labels = find_outliers(data, thresholds)\n",
    "        \n",
    "        print(f\"Total entries: {len(premises)}\")\n",
    "        print(f\"Outliers found: {len(outliers)}\")\n",
    "        \n",
    "        if outliers:\n",
    "            # Export outliers\n",
    "            outlier_file = file_path.replace(\".json\", \"_outliers.txt\")\n",
    "            export_outliers(outliers, outlier_file)\n",
    "            print(f\"✓ Outliers exported to: {outlier_file}\")\n",
    "            \n",
    "            # Create clean version\n",
    "            outlier_indices = [outlier['index'] for outlier in outliers]\n",
    "            clean_file = file_path.replace(\".json\", \"_clean.json\")\n",
    "            clean_premises, clean_hypotheses, clean_labels = create_clean_dataset(\n",
    "                premises, hypotheses, labels, outlier_indices, clean_file\n",
    "            )\n",
    "            \n",
    "            print(f\"✓ Clean dataset created: {clean_file}\")\n",
    "            print(f\"  Removed {len(outliers)} outliers, {len(clean_premises)} entries remaining\")\n",
    "            \n",
    "            # Store clean data for later use\n",
    "            all_clean_data[file_path] = {\n",
    "                'premises': clean_premises,\n",
    "                'hypotheses': clean_hypotheses,\n",
    "                'labels': clean_labels\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            print(\"✓ No outliers found - using original data\")\n",
    "            # Store original data\n",
    "            all_clean_data[file_path] = {\n",
    "                'premises': premises,\n",
    "                'hypotheses': hypotheses,\n",
    "                'labels': labels\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Continue with processing clean data\n",
    "print(f\"\\n OUTLIER PROCESSING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Now continuing with clean data...\")\n",
    "\n",
    "# Show summary\n",
    "for file_path, data in all_clean_data.items():\n",
    "    print(f\"{file_path}: {len(data['premises'])} clean entries\")\n",
    "\n",
    "# ==================== CONTINUE WITH YOUR NEXT STEPS HERE ====================\n",
    "# Now you can proceed with text preprocessing, model training, etc.\n",
    "# Example: access clean train data with all_clean_data['train.json']['premises']\n",
    "\n",
    "print(f\"\\n READY FOR NEXT STEPS:\")\n",
    "print(\"- Text preprocessing (tokenization, cleaning)\")\n",
    "print(\"- Label encoding (entails/neutral to numerical)\")\n",
    "print(\"- LSTM model implementation\")\n",
    "print(\"- Training with clean data\")\n",
    "\n",
    "# Quick verification\n",
    "if 'train.json' in all_clean_data:\n",
    "    train_data = all_clean_data['train.json']\n",
    "    print(f\"\\n Sample from clean training data:\")\n",
    "    print(f\"First premise: {train_data['premises'][0][:100]}...\")\n",
    "    print(f\"First hypothesis: {train_data['hypotheses'][0]}\")\n",
    "    print(f\"First label: {train_data['labels'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f24df4-12ad-4d43-bb96-ae30df37e5a8",
   "metadata": {},
   "source": [
    "### For 10 outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4691e1c9-4504-4369-bab6-f45faf4686d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " COMPLETE OUTLIER HANDLING PIPELINE\n",
      "============================================================\n",
      "\n",
      "Processing: train.json\n",
      "Total entries: 23088\n",
      "Outliers found: 10\n",
      "✓ Outliers exported to: train_outliers.txt\n",
      "✓ Clean dataset created: train_clean.json\n",
      "  Removed 10 outliers, 23078 entries remaining\n",
      "\n",
      "Processing: validation.json\n",
      "Total entries: 1304\n",
      "Outliers found: 0\n",
      "✓ No outliers found - using original data\n",
      "\n",
      "Processing: test.json\n",
      "Total entries: 2126\n",
      "Outliers found: 0\n",
      "✓ No outliers found - using original data\n",
      "\n",
      " OUTLIER PROCESSING COMPLETE\n",
      "============================================================\n",
      "Now continuing with clean data...\n",
      "train.json: 23078 clean entries\n",
      "validation.json: 1304 clean entries\n",
      "test.json: 2126 clean entries\n",
      "\n",
      " READY FOR NEXT STEPS:\n",
      "- Text preprocessing (tokenization, cleaning)\n",
      "- Label encoding (entails/neutral to numerical)\n",
      "- LSTM model implementation\n",
      "- Training with clean data\n",
      "\n",
      " Sample from clean training data:\n",
      "First premise: Pluto rotates once on its axis every 6.39 Earth days;...\n",
      "First hypothesis: Earth rotates on its axis once times in one day.\n",
      "First label: neutral\n"
     ]
    }
   ],
   "source": [
    "MAX_TOTAL_LENGTH = 100\n",
    "MAX_PREMISE_LENGTH = 55\n",
    "MAX_HYPOTHESIS_LENGTH = 40\n",
    "\n",
    "def find_outliers(data, thresholds):\n",
    "    outliers = []\n",
    "    \n",
    "    premises_dict = data['premise']\n",
    "    hypotheses_dict = data['hypothesis']\n",
    "    labels_dict = data['label']\n",
    "    \n",
    "    premises = [premises_dict[key] for key in sorted(premises_dict.keys(), key=int)]\n",
    "    hypotheses = [hypotheses_dict[key] for key in sorted(hypotheses_dict.keys(), key=int)]\n",
    "    labels = [labels_dict[key] for key in sorted(labels_dict.keys(), key=int)]\n",
    "    \n",
    "    for i, (premise, hypothesis, label) in enumerate(zip(premises, hypotheses, labels)):\n",
    "        premise_words = len(str(premise).split())\n",
    "        hypothesis_words = len(str(hypothesis).split())\n",
    "        total_words = premise_words + hypothesis_words\n",
    "        \n",
    "        if (total_words > thresholds['max_total'] or \n",
    "            premise_words > thresholds['max_premise'] or \n",
    "            hypothesis_words > thresholds['max_hypothesis']):\n",
    "            \n",
    "            outliers.append({\n",
    "                'index': i,\n",
    "                'premise': premise,\n",
    "                'hypothesis': hypothesis, \n",
    "                'label': label,\n",
    "                'premise_length': premise_words,\n",
    "                'hypothesis_length': hypothesis_words,\n",
    "                'total_length': total_words\n",
    "            })\n",
    "    \n",
    "    return outliers, premises, hypotheses, labels\n",
    "\n",
    "def export_outliers(outliers, output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"OUTLIERS DETECTED\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Total outliers found: {len(outliers)}\\n\\n\")\n",
    "        \n",
    "        for outlier in outliers:\n",
    "            f.write(f\"INDEX: {outlier['index']}\\n\")\n",
    "            f.write(f\"TOTAL WORDS: {outlier['total_length']}\\n\")\n",
    "            f.write(f\"PREMISE WORDS: {outlier['premise_length']}\\n\")\n",
    "            f.write(f\"HYPOTHESIS WORDS: {outlier['hypothesis_length']}\\n\")\n",
    "            f.write(f\"LABEL: {outlier['label']}\\n\")\n",
    "            f.write(f\"PREMISE: {outlier['premise']}\\n\")\n",
    "            f.write(f\"HYPOTHESIS: {outlier['hypothesis']}\\n\")\n",
    "            f.write(\"-\" * 60 + \"\\n\\n\")\n",
    "\n",
    "def create_clean_dataset(premises, hypotheses, labels, outlier_indices, output_path):\n",
    "    clean_premises = [p for i, p in enumerate(premises) if i not in outlier_indices]\n",
    "    clean_hypotheses = [h for i, h in enumerate(hypotheses) if i not in outlier_indices]\n",
    "    clean_labels = [l for i, l in enumerate(labels) if i not in outlier_indices]\n",
    "    \n",
    "    clean_data = {\n",
    "        'premise': dict(enumerate(clean_premises)),\n",
    "        'hypothesis': dict(enumerate(clean_hypotheses)),\n",
    "        'label': dict(enumerate(clean_labels))\n",
    "    }\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(clean_data, f, indent=2)\n",
    "    \n",
    "    return clean_premises, clean_hypotheses, clean_labels\n",
    "\n",
    "print(\" COMPLETE OUTLIER HANDLING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "files_to_process = [\"train.json\", \"validation.json\", \"test.json\"]\n",
    "thresholds = {\n",
    "    'max_total': MAX_TOTAL_LENGTH,\n",
    "    'max_premise': MAX_PREMISE_LENGTH,\n",
    "    'max_hypothesis': MAX_HYPOTHESIS_LENGTH\n",
    "}\n",
    "\n",
    "all_clean_data = {}\n",
    "\n",
    "for file_path in files_to_process:\n",
    "    print(f\"\\nProcessing: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        data = json.load(open(file_path, \"r\", encoding=\"utf-8\"))\n",
    "        outliers, premises, hypotheses, labels = find_outliers(data, thresholds)\n",
    "        \n",
    "        print(f\"Total entries: {len(premises)}\")\n",
    "        print(f\"Outliers found: {len(outliers)}\")\n",
    "        \n",
    "        if outliers:\n",
    "            outlier_file = file_path.replace(\".json\", \"_outliers.txt\")\n",
    "            export_outliers(outliers, outlier_file)\n",
    "            print(f\"✓ Outliers exported to: {outlier_file}\")\n",
    "            \n",
    "            outlier_indices = [outlier['index'] for outlier in outliers]\n",
    "            clean_file = file_path.replace(\".json\", \"_clean.json\")\n",
    "            clean_premises, clean_hypotheses, clean_labels = create_clean_dataset(\n",
    "                premises, hypotheses, labels, outlier_indices, clean_file\n",
    "            )\n",
    "            \n",
    "            print(f\"✓ Clean dataset created: {clean_file}\")\n",
    "            print(f\"  Removed {len(outliers)} outliers, {len(clean_premises)} entries remaining\")\n",
    "            \n",
    "            all_clean_data[file_path] = {\n",
    "                'premises': clean_premises,\n",
    "                'hypotheses': clean_hypotheses,\n",
    "                'labels': clean_labels\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            print(\"✓ No outliers found - using original data\")\n",
    "            all_clean_data[file_path] = {\n",
    "                'premises': premises,\n",
    "                'hypotheses': hypotheses,\n",
    "                'labels': labels\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n OUTLIER PROCESSING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Now continuing with clean data...\")\n",
    "\n",
    "for file_path, data in all_clean_data.items():\n",
    "    print(f\"{file_path}: {len(data['premises'])} clean entries\")\n",
    "\n",
    "print(f\"\\n READY FOR NEXT STEPS:\")\n",
    "print(\"- Text preprocessing (tokenization, cleaning)\")\n",
    "print(\"- Label encoding (entails/neutral to numerical)\")\n",
    "print(\"- LSTM model implementation\")\n",
    "print(\"- Training with clean data\")\n",
    "\n",
    "if 'train.json' in all_clean_data:\n",
    "    train_data = all_clean_data['train.json']\n",
    "    print(f\"\\n Sample from clean training data:\")\n",
    "    print(f\"First premise: {train_data['premises'][0][:100]}...\")\n",
    "    print(f\"First hypothesis: {train_data['hypotheses'][0]}\")\n",
    "    print(f\"First label: {train_data['labels'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006b2c6-4ed6-4114-a50c-37f92cd69432",
   "metadata": {},
   "source": [
    "#### AFTER inspecting outlier txt file, manually input index to remove outliers from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14bb5034-01d4-4e50-8e7f-9925bf147c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING AND CLEANING TRAIN DATA\n",
      "==================================================\n",
      "Original train entries: 23088\n",
      "Outliers removed: 10\n",
      "Clean train entries: 23078\n",
      "✓ train_cleaned.json saved\n",
      "✓ train_cleaned.txt saved\n",
      "\n",
      " OUTLIER REMOVAL COMPLETE!\n",
      "==================================================\n",
      "Files created:\n",
      "  - train_cleaned.json (for model training)\n",
      "  - train_cleaned.txt (for human inspection)\n"
     ]
    }
   ],
   "source": [
    "#INSPECTING the outlier txt file, FIND A WAY TO REMOVE IT\n",
    "import json\n",
    "\n",
    "# Known outlier indices from train.json\n",
    "# TRAIN_OUTLIER_INDICES = [270, 537, 606, 608, 760 , 995, 1104, 1129,1484 ] # Add all 9 outlier indices here\n",
    "TRAIN_OUTLIER_INDICES = [270, 537, 606, 608, 760 , 995, 1104, 1129,1484, 19888] # Add all 10 outlier indices here\n",
    "\n",
    "print(\"LOADING AND CLEANING TRAIN DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Load the original train.json\n",
    "    with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    # Extract and sort the data\n",
    "    premises_dict = train_data['premise']\n",
    "    hypotheses_dict = train_data['hypothesis']\n",
    "    labels_dict = train_data['label']\n",
    "    \n",
    "    train_premises = [premises_dict[key] for key in sorted(premises_dict.keys(), key=int)]\n",
    "    train_hypotheses = [hypotheses_dict[key] for key in sorted(hypotheses_dict.keys(), key=int)]\n",
    "    train_labels = [labels_dict[key] for key in sorted(labels_dict.keys(), key=int)]\n",
    "    \n",
    "    print(f\"Original train entries: {len(train_premises)}\")\n",
    "    \n",
    "    # Remove outliers\n",
    "    clean_train_premises = [p for i, p in enumerate(train_premises) if i not in TRAIN_OUTLIER_INDICES]\n",
    "    clean_train_hypotheses = [h for i, h in enumerate(train_hypotheses) if i not in TRAIN_OUTLIER_INDICES]\n",
    "    clean_train_labels = [l for i, l in enumerate(train_labels) if i not in TRAIN_OUTLIER_INDICES]\n",
    "    \n",
    "    print(f\"Outliers removed: {len(TRAIN_OUTLIER_INDICES)}\")\n",
    "    print(f\"Clean train entries: {len(clean_train_premises)}\")\n",
    "    \n",
    "    # Save cleaned data to JSON\n",
    "    cleaned_train_data = {\n",
    "        'premise': dict(enumerate(clean_train_premises)),\n",
    "        'hypothesis': dict(enumerate(clean_train_hypotheses)),\n",
    "        'label': dict(enumerate(clean_train_labels))\n",
    "    }\n",
    "    \n",
    "    with open(\"train_cleaned.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned_train_data, f, indent=2)\n",
    "        print(\"✓ train_cleaned.json saved\")\n",
    "    \n",
    "    # Save cleaned data to TXT\n",
    "    with open(\"train_cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"TRAIN DATASET - CLEANED (OUTLIERS REMOVED)\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        f.write(f\"Outliers removed: {len(TRAIN_OUTLIER_INDICES)}\\n\")\n",
    "        f.write(f\"Total entries: {len(clean_train_premises)}\\n\\n\")\n",
    "        \n",
    "        for i, (premise, hypothesis, label) in enumerate(zip(clean_train_premises, clean_train_hypotheses, clean_train_labels)):\n",
    "            f.write(f\"ENTRY {i}:\\n\")\n",
    "            f.write(f\"PREMISE: {premise}\\n\")\n",
    "            f.write(f\"HYPOTHESIS: {hypothesis}\\n\")\n",
    "            f.write(f\"LABEL: {label}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        print(\"✓ train_cleaned.txt saved\")\n",
    "    \n",
    "    print(\"\\n OUTLIER REMOVAL COMPLETE!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Files created:\")\n",
    "    print(\"  - train_cleaned.json (for model training)\")\n",
    "    print(\"  - train_cleaned.txt (for human inspection)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure train.json is in the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb272b14-f93c-457a-b816-589bfe751f7b",
   "metadata": {},
   "source": [
    "#### **Verify if the outliers were successfully removed in the cleaned training JSON file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7edb816-09f5-4a1f-a81d-98fdb24a46c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 VERIFYING OUTLIER REMOVAL SUCCESS\n",
      "============================================================\n",
      "\n",
      "1. CHECKING ORIGINAL train.json:\n",
      "   Original entries: 23088\n",
      "   Original outliers: 10\n",
      "   Original outliers found:\n",
      "     Index 270: 1026 words (P:1016, H:10)\n",
      "     Index 537: 791 words (P:775, H:16)\n",
      "     Index 606: 663 words (P:646, H:17)\n",
      "     Index 608: 358 words (P:344, H:14)\n",
      "     Index 760: 615 words (P:595, H:20)\n",
      "     Index 995: 1284 words (P:1270, H:14)\n",
      "     Index 1104: 10605 words (P:10587, H:18)\n",
      "     Index 1129: 6258 words (P:6247, H:11)\n",
      "     Index 1484: 2413 words (P:2388, H:25)\n",
      "     Index 19888: 68 words (P:56, H:12)\n",
      "\n",
      "2. CHECKING CLEANED train_cleaned.json:\n",
      "   Cleaned entries: 23078\n",
      "   Cleaned outliers: 0\n",
      "   ✅ SUCCESS: No outliers found in cleaned data!\n",
      "\n",
      "3. SUMMARY COMPARISON:\n",
      "========================================\n",
      "   Original entries: 23088\n",
      "   Cleaned entries: 23078\n",
      "   Outliers removed: 10\n",
      "   Longest sequence in cleaned data: 81 words\n",
      "   ✅ SUCCESS: All sequences are within reasonable length!\n",
      "\n",
      "============================================================\n",
      "VERIFICATION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "MAX_TOTAL_LENGTH = 100\n",
    "MAX_PREMISE_LENGTH = 55 \n",
    "MAX_HYPOTHESIS_LENGTH = 40\n",
    "\n",
    "def check_outliers(premises, hypotheses, labels, dataset_name):\n",
    "    outliers = []\n",
    "    \n",
    "    for i, (premise, hypothesis, label) in enumerate(zip(premises, hypotheses, labels)):\n",
    "        premise_words = len(str(premise).split())\n",
    "        hypothesis_words = len(str(hypothesis).split())\n",
    "        total_words = premise_words + hypothesis_words\n",
    "        \n",
    "        if (total_words > MAX_TOTAL_LENGTH or \n",
    "            premise_words > MAX_PREMISE_LENGTH or \n",
    "            hypothesis_words > MAX_HYPOTHESIS_LENGTH):\n",
    "            \n",
    "            outliers.append({\n",
    "                'index': i,\n",
    "                'premise_length': premise_words,\n",
    "                'hypothesis_length': hypothesis_words,\n",
    "                'total_length': total_words,\n",
    "                'label': label,\n",
    "                'premise': premise,\n",
    "                'hypothesis': hypothesis\n",
    "            })\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "print(\"🔍 VERIFYING OUTLIER REMOVAL SUCCESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. CHECKING ORIGINAL train.json:\")\n",
    "try:\n",
    "    with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        original_data = json.load(f)\n",
    "    \n",
    "    original_premises = [original_data['premise'][key] for key in sorted(original_data['premise'].keys(), key=int)]\n",
    "    original_hypotheses = [original_data['hypothesis'][key] for key in sorted(original_data['hypothesis'].keys(), key=int)]\n",
    "    original_labels = [original_data['label'][key] for key in sorted(original_data['label'].keys(), key=int)]\n",
    "    \n",
    "    original_outliers = check_outliers(original_premises, original_hypotheses, original_labels, \"original\")\n",
    "    print(f\"   Original entries: {len(original_premises)}\")\n",
    "    print(f\"   Original outliers: {len(original_outliers)}\")\n",
    "    \n",
    "    if original_outliers:\n",
    "        print(\"   Original outliers found:\")\n",
    "        for outlier in original_outliers:\n",
    "            print(f\"     Index {outlier['index']}: {outlier['total_length']} words (P:{outlier['premise_length']}, H:{outlier['hypothesis_length']})\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"   Error loading original: {e}\")\n",
    "\n",
    "print(\"\\n2. CHECKING CLEANED train_cleaned.json:\")\n",
    "try:\n",
    "    with open(\"train_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        cleaned_data = json.load(f)\n",
    "    \n",
    "    cleaned_premises = list(cleaned_data['premise'].values())\n",
    "    cleaned_hypotheses = list(cleaned_data['hypothesis'].values())\n",
    "    cleaned_labels = list(cleaned_data['label'].values())\n",
    "    \n",
    "    cleaned_outliers = check_outliers(cleaned_premises, cleaned_hypotheses, cleaned_labels, \"cleaned\")\n",
    "    print(f\"   Cleaned entries: {len(cleaned_premises)}\")\n",
    "    print(f\"   Cleaned outliers: {len(cleaned_outliers)}\")\n",
    "    \n",
    "    if cleaned_outliers:\n",
    "        print(\"   ❌ STILL HAVE OUTLIERS:\")\n",
    "        for outlier in cleaned_outliers:\n",
    "            print(f\"     Index {outlier['index']}: {outlier['total_length']} words (P:{outlier['premise_length']}, H:{outlier['hypothesis_length']})\")\n",
    "            print(f\"       Premise: {outlier['premise'][:100]}...\")\n",
    "            print(f\"       Hypothesis: {outlier['hypothesis']}\")\n",
    "            print(f\"       Label: {outlier['label']}\")\n",
    "            print(\"       \" + \"-\" * 50)\n",
    "    else:\n",
    "        print(\"   ✅ SUCCESS: No outliers found in cleaned data!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   Error loading cleaned: {e}\")\n",
    "\n",
    "print(\"\\n3. SUMMARY COMPARISON:\")\n",
    "print(\"=\" * 40)\n",
    "try:\n",
    "    original_count = len(original_premises)\n",
    "    cleaned_count = len(cleaned_premises)\n",
    "    outliers_removed = original_count - cleaned_count\n",
    "    \n",
    "    print(f\"   Original entries: {original_count}\")\n",
    "    print(f\"   Cleaned entries: {cleaned_count}\")\n",
    "    print(f\"   Outliers removed: {outliers_removed}\")\n",
    "\n",
    "    longest_cleaned = max(len(str(p).split()) + len(str(h).split()) \n",
    "                         for p, h in zip(cleaned_premises, cleaned_hypotheses))\n",
    "    print(f\"   Longest sequence in cleaned data: {longest_cleaned} words\")\n",
    "    \n",
    "    if longest_cleaned <= MAX_TOTAL_LENGTH:\n",
    "        print(\"   ✅ SUCCESS: All sequences are within reasonable length!\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  WARNING: Still have long sequences (> {MAX_TOTAL_LENGTH} words)\")\n",
    "        \n",
    "except NameError:\n",
    "    print(\"   Could not compare - data not loaded properly\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe89282-55ba-4a20-8603-aef348230295",
   "metadata": {},
   "source": [
    "# FURTHER DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e371c4-8a07-4f88-979c-6173fad72d31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### RegEx Text Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b387233a-a040-46ef-af9e-f81aea0c5370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMAL DATA CLEANING PIPELINE\n",
      "============================================================\n",
      "Cleaning actions: Remove URLs, HTML tags, and excessive repetitive symbols\n",
      "Retaining: Case, punctuation, numbers, single/double symbols, -- dashes\n",
      "============================================================\n",
      "\n",
      "Processing TRAINING: train_cleaned.json -> train_final_cleaned.json\n",
      "Successfully cleaned and saved train_final_cleaned.json\n",
      "TRAINING Statistics:\n",
      "   Total entries: 23078\n",
      "   Cleaning changes: 927\n",
      "\n",
      "Processing TEST: test.json -> test_final_cleaned.json\n",
      "Successfully cleaned and saved test_final_cleaned.json\n",
      "TEST Statistics:\n",
      "   Total entries: 2126\n",
      "   Cleaning changes: 84\n",
      "\n",
      "Processing VALIDATION: validation.json -> validation_final_cleaned.json\n",
      "Successfully cleaned and saved validation_final_cleaned.json\n",
      "VALIDATION Statistics:\n",
      "   Total entries: 1304\n",
      "   Cleaning changes: 80\n",
      "\n",
      "============================================================\n",
      "CLEANING COMPLETE - READY FOR TOKENIZATION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Minimal text cleaning: remove URLs, HTML tags, and extra whitespace only\n",
    "    Keep most symbols and punctuation, only remove excessive repetitive symbols\n",
    "    \"\"\"\n",
    "    original_text = str(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', original_text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove excessive repetitive symbols (3+ of the same symbol in a row)\n",
    "    # But keep double dashes (--) as requested\n",
    "    text = re.sub(r'\\.{3,}', ' ', text)    # Remove multiple dots (....)\n",
    "    text = re.sub(r'\\*{3,}', ' ', text)    # Remove multiple asterisks (***)\n",
    "    text = re.sub(r'={3,}', ' ', text)     # Remove multiple equals (===)\n",
    "    text = re.sub(r'\\!{3,}', ' ', text)    # Remove multiple exclamations (!!!)\n",
    "    text = re.sub(r'\\?{2,}', ' ', text)    # Remove multiple question marks (??)\n",
    "    \n",
    "    # Remove extra whitespace (multiple spaces/tabs/newlines to single space)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return original_text, text\n",
    "\n",
    "def process_dataset(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process a complete dataset file with minimal cleaning\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    cleaning_report = []\n",
    "    \n",
    "    # Clean premises\n",
    "    cleaned_premises = {}\n",
    "    for key, premise in data['premise'].items():\n",
    "        original, cleaned = clean_text(premise)\n",
    "        cleaned_premises[key] = cleaned\n",
    "        if original != cleaned:\n",
    "            cleaning_report.append({\n",
    "                'type': 'premise',\n",
    "                'key': key,\n",
    "                'original': original,\n",
    "                'cleaned': cleaned\n",
    "            })\n",
    "    \n",
    "    # Clean hypotheses\n",
    "    cleaned_hypotheses = {}\n",
    "    for key, hypothesis in data['hypothesis'].items():\n",
    "        original, cleaned = clean_text(hypothesis)\n",
    "        cleaned_hypotheses[key] = cleaned\n",
    "        if original != cleaned:\n",
    "            cleaning_report.append({\n",
    "                'type': 'hypothesis', \n",
    "                'key': key,\n",
    "                'original': original,\n",
    "                'cleaned': cleaned\n",
    "            })\n",
    "    \n",
    "    # Create cleaned dataset\n",
    "    cleaned_data = {\n",
    "        'premise': cleaned_premises,\n",
    "        'hypothesis': cleaned_hypotheses,\n",
    "        'label': data['label']\n",
    "    }\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return cleaned_data, cleaning_report\n",
    "\n",
    "# Files to process\n",
    "datasets = [\n",
    "    (\"train_cleaned.json\", \"train_final_cleaned.json\", \"TRAINING\"),\n",
    "    (\"test.json\", \"test_final_cleaned.json\", \"TEST\"), \n",
    "    (\"validation.json\", \"validation_final_cleaned.json\", \"VALIDATION\")\n",
    "]\n",
    "\n",
    "print(\"MINIMAL DATA CLEANING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Cleaning actions: Remove URLs, HTML tags, and excessive repetitive symbols\")\n",
    "print(\"Retaining: Case, punctuation, numbers, single/double symbols, -- dashes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cleaning_reports = {}\n",
    "\n",
    "# Process all datasets\n",
    "for input_file, output_file, dataset_name in datasets:\n",
    "    try:\n",
    "        print(f\"\\nProcessing {dataset_name}: {input_file} -> {output_file}\")\n",
    "        \n",
    "        cleaned_data, report = process_dataset(input_file, output_file)\n",
    "        cleaning_reports[dataset_name] = report\n",
    "        \n",
    "        print(f\"Successfully cleaned and saved {output_file}\")\n",
    "        \n",
    "        # Show statistics\n",
    "        total_entries = len(cleaned_data['premise'])\n",
    "        changes = len(report)\n",
    "        \n",
    "        print(f\"{dataset_name} Statistics:\")\n",
    "        print(f\"   Total entries: {total_entries}\")\n",
    "        print(f\"   Cleaning changes: {changes}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {input_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLEANING COMPLETE - READY FOR TOKENIZATION\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e4732f-eb5e-4235-82dd-c77b6c8e91dd",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d11cbec-726c-4c39-9eb8-820644dbcd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 BUILDING VOCABULARY AND TOKENIZING DATA...\n",
      "============================================================\n",
      "Vocabulary size: 20000\n",
      "Train sequences: 23078\n",
      "Validation sequences: 1304\n",
      "Test sequences: 2126\n",
      "Sequence length: 50\n",
      "✅ TOKENIZATION COMPLETE - READY FOR MODEL TRAINING\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def build_vocab(texts, max_vocab_size=20000):\n",
    "    \"\"\"Build vocabulary from all texts\"\"\"\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    # Create vocabulary with special tokens\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    \n",
    "    # Add most common words\n",
    "    for word, _ in counter.most_common(max_vocab_size - 2):  # Reserve 2 for special tokens\n",
    "        vocab[word] = len(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def text_to_sequence(text, vocab, max_length=None):\n",
    "    \"\"\"Convert text to sequence of integers\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    sequence = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "    \n",
    "    if max_length:\n",
    "        if len(sequence) > max_length:\n",
    "            sequence = sequence[:max_length]  # Truncate\n",
    "        else:\n",
    "            sequence = sequence + [vocab['<PAD>']] * (max_length - len(sequence))  # Pad\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "def load_and_tokenize_data(file_path, vocab, max_length=50):\n",
    "    \"\"\"Load data and tokenize premises and hypotheses\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    premises = list(data['premise'].values())\n",
    "    hypotheses = list(data['hypothesis'].values())\n",
    "    labels = list(data['label'].values())\n",
    "    \n",
    "    # Tokenize and convert to sequences\n",
    "    premises_seq = [text_to_sequence(premise, vocab, max_length) for premise in premises]\n",
    "    hypotheses_seq = [text_to_sequence(hypothesis, vocab, max_length) for hypothesis in hypotheses]\n",
    "    \n",
    "    return premises_seq, hypotheses_seq, labels\n",
    "\n",
    "print(\"🔤 BUILDING VOCABULARY AND TOKENIZING DATA...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load all text data to build comprehensive vocabulary\n",
    "all_texts = []\n",
    "for file_path in [\"train_final_cleaned.json\", \"test_final_cleaned.json\", \"validation_final_cleaned.json\"]:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        all_texts.extend(list(data['premise'].values()))\n",
    "        all_texts.extend(list(data['hypothesis'].values()))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab(all_texts, max_vocab_size=20000)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Tokenize all datasets\n",
    "max_seq_length = 50  # Adjust based on your data analysis\n",
    "\n",
    "train_premises_seq, train_hypotheses_seq, train_labels = load_and_tokenize_data(\n",
    "    \"train_final_cleaned.json\", vocab, max_seq_length\n",
    ")\n",
    "val_premises_seq, val_hypotheses_seq, val_labels = load_and_tokenize_data(\n",
    "    \"validation_final_cleaned.json\", vocab, max_seq_length\n",
    ")\n",
    "test_premises_seq, test_hypotheses_seq, test_labels = load_and_tokenize_data(\n",
    "    \"test_final_cleaned.json\", vocab, max_seq_length\n",
    ")\n",
    "\n",
    "print(f\"Train sequences: {len(train_premises_seq)}\")\n",
    "print(f\"Validation sequences: {len(val_premises_seq)}\")\n",
    "print(f\"Test sequences: {len(test_premises_seq)}\")\n",
    "print(f\"Sequence length: {max_seq_length}\")\n",
    "\n",
    "# Convert labels to numerical format if needed (assuming they're already numerical)\n",
    "# If labels are strings like \"entails\"/\"neutral\", you'd need to encode them here\n",
    "\n",
    "print(\"✅ TOKENIZATION COMPLETE - READY FOR MODEL TRAINING\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f11021-e234-44a2-b01a-12f8e9a2d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 TOKENIZATION PREVIEW\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "Original Premise: Pluto rotates once on its axis every 6.39 Earth days;\n",
      "Tokenized Premise: ['Pluto', 'rotates', 'once', 'on', 'its', 'axis', 'every', '6.39', 'Earth', 'days', ';']\n",
      "Token IDs: [1202, 1227, 453, 25, 56, 387, 406, 14694, 111, 893, 44]\n",
      "Original Hypothesis: Earth rotates on its axis once times in one day.\n",
      "Tokenized Hypothesis: ['Earth', 'rotates', 'on', 'its', 'axis', 'once', 'times', 'in', 'one', 'day', '.']\n",
      "Token IDs: [111, 1227, 25, 56, 387, 453, 276, 9, 35, 302, 2]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "Original Premise: ---Glenn ========================================================= Once per day, the earth rotates about its axis.\n",
      "Tokenized Premise: ['--', '-Glenn', 'Once', 'per', 'day', ',', 'the', 'earth', 'rotates', 'about', 'its', 'axis', '.']\n",
      "Token IDs: [230, 14695, 2895, 313, 302, 5, 3, 62, 1227, 165, 56, 387, 2]\n",
      "Original Hypothesis: Earth rotates on its axis once times in one day.\n",
      "Tokenized Hypothesis: ['Earth', 'rotates', 'on', 'its', 'axis', 'once', 'times', 'in', 'one', 'day', '.']\n",
      "Token IDs: [111, 1227, 25, 56, 387, 453, 276, 9, 35, 302, 2]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample 3:\n",
      "Original Premise: geysers - periodic gush of hot water at the surface of the Earth.\n",
      "Tokenized Premise: ['geysers', '-', 'periodic', 'gush', 'of', 'hot', 'water', 'at', 'the', 'surface', 'of', 'the', 'Earth', '.']\n",
      "Token IDs: [4546, 509, 800, 14696, 4, 428, 22, 40, 3, 83, 4, 3, 111, 2]\n",
      "Original Hypothesis: The surface of the sun is much hotter than almost anything on earth.\n",
      "Tokenized Hypothesis: ['The', 'surface', 'of', 'the', 'sun', 'is', 'much', 'hotter', 'than', 'almost', 'anything', 'on', 'earth', '.']\n",
      "Token IDs: [11, 83, 4, 3, 58, 6, 351, 2322, 76, 726, 1085, 25, 62, 2]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def preview_tokenization(premises, hypotheses, vocab, num_samples=3):\n",
    "    \"\"\"Preview tokenization results for a few samples\"\"\"\n",
    "    print(\"\\n\" + \"🔍 TOKENIZATION PREVIEW\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create reverse vocabulary for decoding\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    for i in range(min(num_samples, len(premises))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Original Premise: {premises[i]}\")\n",
    "        print(f\"Tokenized Premise: {[reverse_vocab.get(idx, '<UNK>') for idx in train_premises_seq[i] if idx != vocab['<PAD>']]}\")\n",
    "        print(f\"Token IDs: {[idx for idx in train_premises_seq[i] if idx != vocab['<PAD>']]}\")\n",
    "        \n",
    "        print(f\"Original Hypothesis: {hypotheses[i]}\")\n",
    "        print(f\"Tokenized Hypothesis: {[reverse_vocab.get(idx, '<UNK>') for idx in train_hypotheses_seq[i] if idx != vocab['<PAD>']]}\")\n",
    "        print(f\"Token IDs: {[idx for idx in train_hypotheses_seq[i] if idx != vocab['<PAD>']]}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Add this right after your tokenization code\n",
    "preview_tokenization(train_premises[:5], train_hypotheses[:5], vocab, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2fd18-c247-455a-ba63-d197abd0138c",
   "metadata": {},
   "source": [
    "### LSTM (WITH ATTENTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbb35290-c85d-4a00-ba25-5f3ff703abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== FIXED DATASET CLASS WITH PADDING ====================\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, premises_seq, hypotheses_seq, labels):\n",
    "        self.premises_seq = premises_seq\n",
    "        self.hypotheses_seq = hypotheses_seq\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.premises_seq[idx], dtype=torch.long),\n",
    "            torch.tensor(self.hypotheses_seq[idx], dtype=torch.long),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to pad sequences in the same batch.\"\"\"\n",
    "    premises, hypotheses, labels = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    premises_padded = pad_sequence(premises, batch_first=True, padding_value=0)\n",
    "    hypotheses_padded = pad_sequence(hypotheses, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return premises_padded, hypotheses_padded, labels\n",
    "\n",
    "# Create datasets with the custom collate function\n",
    "train_dataset = NLIDataset(train_premises_seq, train_hypotheses_seq, train_labels_enc)\n",
    "val_dataset = NLIDataset(val_premises_seq, val_hypotheses_seq, val_labels_enc)\n",
    "test_dataset = NLIDataset(test_premises_seq, test_hypotheses_seq, test_labels_enc)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#MODEL 1: LSTM with Attention\n",
    "class LSTMAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, num_classes=2, num_layers=2, dropout=0.3):\n",
    "        super(LSTMAttentionModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm_premise = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                                   batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_hypothesis = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
    "                                      batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 6, hidden_dim),  # Increased because of bidirectional\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "        # Embedding\n",
    "        prem_embed = self.embedding(premise)\n",
    "        hyp_embed = self.embedding(hypothesis)\n",
    "        \n",
    "        # LSTM encoding\n",
    "        prem_lstm, _ = self.lstm_premise(prem_embed)\n",
    "        hyp_lstm, _ = self.lstm_hypothesis(hyp_embed)\n",
    "        \n",
    "        # Attention between premise and hypothesis\n",
    "        attn_output, attn_weights = self.attention(prem_lstm, hyp_lstm, hyp_lstm)\n",
    "        \n",
    "        # Pooling\n",
    "        prem_pooled = torch.mean(prem_lstm, dim=1)\n",
    "        hyp_pooled = torch.mean(hyp_lstm, dim=1)\n",
    "        attn_pooled = torch.mean(attn_output, dim=1)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat([prem_pooled, hyp_pooled, attn_pooled], dim=1)\n",
    "        \n",
    "        return self.classifier(combined), attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf07150-a297-40a4-a765-f538b71a7fba",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a9949-8543-45f1-82bf-9a4f3e8e33cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FIXED MODELS - TRAINING STARTING...\n",
      "\n",
      " TRAINING LSTM WITH ATTENTION\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, num_classes=2, num_layers=2, dropout=0.3):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru_premise = nn.GRU(embedding_dim, hidden_dim, num_layers, \n",
    "                                 batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.gru_hypothesis = nn.GRU(embedding_dim, hidden_dim, num_layers,\n",
    "                                    batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim),  # bidirectional * 2 for both premise and hypothesis\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "        # Embedding\n",
    "        prem_embed = self.embedding(premise)\n",
    "        hyp_embed = self.embedding(hypothesis)\n",
    "        \n",
    "        # GRU encoding\n",
    "        prem_gru, _ = self.gru_premise(prem_embed)\n",
    "        hyp_gru, _ = self.gru_hypothesis(hyp_embed)\n",
    "        \n",
    "        # Pooling\n",
    "        prem_pooled = torch.mean(prem_gru, dim=1)\n",
    "        hyp_pooled = torch.mean(hyp_gru, dim=1)\n",
    "        \n",
    "        # Concatenate and classify\n",
    "        combined = torch.cat([prem_pooled, hyp_pooled], dim=1)\n",
    "        return self.classifier(combined), None  # Return None for attention weights for compatibility\n",
    "\n",
    "# ==================== FIXED TRAINING FUNCTION ====================\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001, model_name=\"model\"):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    \n",
    "    print(f\"\\n TRAINING {model_name.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for premise, hypothesis, labels in train_loader:\n",
    "            premise, hypothesis, labels = premise.to(device), hypothesis.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(premise, hypothesis)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for premise, hypothesis, labels in val_loader:\n",
    "                premise, hypothesis, labels = premise.to(device), hypothesis.to(device), labels.to(device)\n",
    "                \n",
    "                outputs, _ = model(premise, hypothesis)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        val_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f} | \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f} | \"\n",
    "              f\"Val Acc: {accuracy:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "# ==================== INSTANTIATE AND TRAIN MODELS ====================\n",
    "print(\" FIXED MODELS - TRAINING STARTING...\")\n",
    "\n",
    "# Model 1: LSTM with Attention\n",
    "lstm_model = LSTMAttentionModel(len(vocab), embedding_dim=100, hidden_dim=128)\n",
    "lstm_train_loss, lstm_val_loss, lstm_val_acc = train_model(\n",
    "    lstm_model, train_loader, val_loader, num_epochs=5, model_name=\"LSTM with Attention\"\n",
    ")\n",
    "\n",
    "# Model 2: GRU Model\n",
    "gru_model = GRUModel(len(vocab), embedding_dim=100, hidden_dim=128)\n",
    "gru_train_loss, gru_val_loss, gru_val_acc = train_model(\n",
    "    gru_model, train_loader, val_loader, num_epochs=5, model_name=\"GRU\"\n",
    ")\n",
    "\n",
    "print(\"\\n MODELS TRAINED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab9fcd-d8b2-4b60-8ff9-3349e7ae3408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==================== COMPREHENSIVE MODEL EVALUATION ====================\n",
    "def evaluate_model(model, test_loader, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive evaluation of model performance.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    test_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for premise, hypothesis, labels in test_loader:\n",
    "            premise, hypothesis, labels = premise.to(device), hypothesis.to(device), labels.to(device)\n",
    "            \n",
    "            outputs, _ = model(premise, hypothesis)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, \n",
    "                                       target_names=['entails', 'neutral'])\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # ROC AUC (requires probability scores)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(all_labels, [p[1] for p in all_probs])  # Use probability for class 1\n",
    "    except:\n",
    "        roc_auc = \"N/A\"\n",
    "    \n",
    "    print(f\"\\n {model_name.upper()} EVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader):.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': test_loss/len(test_loader),\n",
    "        'roc_auc': roc_auc,\n",
    "        'predictions': all_preds,\n",
    "        'probabilities': all_probs,\n",
    "        'true_labels': all_labels\n",
    "    }\n",
    "\n",
    "# ==================== COMPARATIVE ANALYSIS ====================\n",
    "def compare_models(results_dict):\n",
    "    \"\"\"Compare performance of multiple models.\"\"\"\n",
    "    print(\"\\n MODEL COMPARISON ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    models = list(results_dict.keys())\n",
    "    metrics = ['accuracy', 'loss', 'roc_auc']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\n{metric.upper():<12} {' | '.join(f'{m}: {results_dict[m][metric]:.4f}' for m in models)}\")\n",
    "    \n",
    "    # Find best model\n",
    "    best_acc_model = max(models, key=lambda m: results_dict[m]['accuracy'])\n",
    "    best_loss_model = min(models, key=lambda m: results_dict[m]['loss'])\n",
    "    \n",
    "    print(f\"\\n Best Accuracy: {best_acc_model} ({results_dict[best_acc_model]['accuracy']:.4f})\")\n",
    "    print(f\" Best Loss: {best_loss_model} ({results_dict[best_loss_model]['loss']:.4f})\")\n",
    "\n",
    "# ==================== VISUALIZATION ====================\n",
    "def plot_training_history(lstm_train_loss, lstm_val_loss, lstm_val_acc,\n",
    "                         gru_train_loss, gru_val_loss, gru_val_acc):\n",
    "    \"\"\"Plot training history for both models.\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training Loss\n",
    "    ax1.plot(lstm_train_loss, label='LSTM Train', marker='o')\n",
    "    ax1.plot(gru_train_loss, label='GRU Train', marker='s')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Validation Loss\n",
    "    ax2.plot(lstm_val_loss, label='LSTM Val', marker='o')\n",
    "    ax2.plot(gru_val_loss, label='GRU Val', marker='s')\n",
    "    ax2.set_title('Validation Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Validation Accuracy\n",
    "    ax3.plot(lstm_val_acc, label='LSTM', marker='o')\n",
    "    ax3.plot(gru_val_acc, label='GRU', marker='s')\n",
    "    ax3.set_title('Validation Accuracy')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # Final comparison bar chart\n",
    "    models = ['LSTM', 'GRU']\n",
    "    final_acc = [lstm_val_acc[-1], gru_val_acc[-1]]\n",
    "    final_loss = [lstm_val_loss[-1], gru_val_loss[-1]]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax4.bar(x - width/2, final_acc, width, label='Accuracy', alpha=0.8)\n",
    "    ax4.bar(x + width/2, final_loss, width, label='Loss', alpha=0.8)\n",
    "    ax4.set_title('Final Epoch Comparison')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(models)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==================== ERROR ANALYSIS ====================\n",
    "def error_analysis(model, test_loader, model_name, num_samples=5):\n",
    "    \"\"\"Analyze specific errors made by the model.\"\"\"\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for premise, hypothesis, labels in test_loader:\n",
    "            premise, hypothesis, labels = premise.to(device), hypothesis.to(device), labels.to(device)\n",
    "            \n",
    "            outputs, _ = model(premise, hypothesis)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Find incorrect predictions\n",
    "            incorrect_mask = (preds != labels)\n",
    "            if incorrect_mask.any():\n",
    "                incorrect_indices = torch.where(incorrect_mask)[0]\n",
    "                for idx in incorrect_indices:\n",
    "                    errors.append({\n",
    "                        'premise': premise[idx].cpu().numpy(),\n",
    "                        'hypothesis': hypothesis[idx].cpu().numpy(),\n",
    "                        'true_label': labels[idx].item(),\n",
    "                        'predicted_label': preds[idx].item(),\n",
    "                        'probabilities': torch.softmax(outputs[idx], dim=0).cpu().numpy()\n",
    "                    })\n",
    "                    if len(errors) >= num_samples:\n",
    "                        break\n",
    "            if len(errors) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n {model_name.upper()} ERROR ANALYSIS (First {num_samples} errors)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, error in enumerate(errors):\n",
    "        print(f\"\\nError {i+1}:\")\n",
    "        print(f\"True: {error['true_label']} ({'entails' if error['true_label'] == 0 else 'neutral'})\")\n",
    "        print(f\"Pred: {error['predicted_label']} ({'entails' if error['predicted_label'] == 0 else 'neutral'})\")\n",
    "        print(f\"Confidence: {max(error['probabilities']):.3f}\")\n",
    "        # You can add actual text here if you have the reverse mapping\n",
    "\n",
    "# ==================== RUN COMPREHENSIVE EVALUATION ====================\n",
    "print(\" RUNNING COMPREHENSIVE EVALUATION...\")\n",
    "\n",
    "# Evaluate both models on test set\n",
    "lstm_results = evaluate_model(lstm_model, test_loader, \"LSTM with Attention\")\n",
    "gru_results = evaluate_model(gru_model, test_loader, \"GRU\")\n",
    "\n",
    "# Compare models\n",
    "compare_models({\n",
    "    'LSTM': lstm_results,\n",
    "    'GRU': gru_results\n",
    "})\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(lstm_train_loss, lstm_val_loss, lstm_val_acc,\n",
    "                     gru_train_loss, gru_val_loss, gru_val_acc)\n",
    "\n",
    "# Error analysis\n",
    "error_analysis(lstm_model, test_loader, \"LSTM\")\n",
    "error_analysis(gru_model, test_loader, \"GRU\")\n",
    "\n",
    "print(\"\\n COMPREHENSIVE EVALUATION COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e2000-25d3-46e5-91bf-b715f6f7cbce",
   "metadata": {},
   "source": [
    "### LSTM (WITHOUT ABLATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e25d3-3d66-471f-844e-5caacac8439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attention Mechanism Ablation\n",
    "\n",
    "# Create LSTM model WITHOUT attention\n",
    "class LSTMModelNoAttention(nn.Module):\n",
    "    # Same as LSTM but remove the attention parts\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, num_classes=2):\n",
    "        super(LSTMModelNoAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm_premise = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.lstm_hypothesis = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(hidden_dim * 4, num_classes)  # No attention features\n",
    "    \n",
    "    def forward(self, premise, hypothesis):\n",
    "        prem_embed = self.embedding(premise)\n",
    "        hyp_embed = self.embedding(hypothesis)\n",
    "        prem_lstm, _ = self.lstm_premise(prem_embed)\n",
    "        hyp_lstm, _ = self.lstm_hypothesis(hyp_embed)\n",
    "        prem_pooled = torch.mean(prem_lstm, dim=1)\n",
    "        hyp_pooled = torch.mean(hyp_lstm, dim=1)\n",
    "        combined = torch.cat([prem_pooled, hyp_pooled], dim=1)\n",
    "        return self.classifier(combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
