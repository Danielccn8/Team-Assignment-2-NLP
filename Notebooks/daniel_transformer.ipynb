{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38257243",
   "metadata": {},
   "source": [
    "# Daniel's Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7496f70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.9\n",
      "Number of GPUs: 1\n",
      "GPU name: NVIDIA GeForce MX450\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423eb4d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Daniel's Branch Created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242854e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        # The two lines below can sometimes slow down training but ensure full reproducibility\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call the function at the beginning of your script\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec746543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\dccn9\\onedrive - uwa\\data science\\semester july-november 2025\\cits4012 - natural language processing\\team assignment 2 nlp\\.venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\dccn9\\onedrive - uwa\\data science\\semester july-november 2025\\cits4012 - natural language processing\\team assignment 2 nlp\\.venv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\dccn9\\onedrive - uwa\\data science\\semester july-november 2025\\cits4012 - natural language processing\\team assignment 2 nlp\\.venv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dccn9\\onedrive - uwa\\data science\\semester july-november 2025\\cits4012 - natural language processing\\team assignment 2 nlp\\.venv\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dccn9\\onedrive - uwa\\data science\\semester july-november 2025\\cits4012 - natural language processing\\team assignment 2 nlp\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24fff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d41667",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('../Dataset/train.json')\n",
    "test_df = pd.read_json('../Dataset/test.json')\n",
    "validation_df = pd.read_json('../Dataset/validation.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea26d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6793498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pluto rotates once on its axis every 6.39 Eart...</td>\n",
       "      <td>Earth rotates on its axis once times in one day.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---Glenn =====================================...</td>\n",
       "      <td>Earth rotates on its axis once times in one day.</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geysers - periodic gush of hot water at the su...</td>\n",
       "      <td>The surface of the sun is much hotter than alm...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facts: Liquid water droplets can be changed in...</td>\n",
       "      <td>Evaporation is responsible for changing liquid...</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By comparison, the earth rotates on its axis o...</td>\n",
       "      <td>Earth rotates on its axis once times in one day.</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  Pluto rotates once on its axis every 6.39 Eart...   \n",
       "1  ---Glenn =====================================...   \n",
       "2  geysers - periodic gush of hot water at the su...   \n",
       "3  Facts: Liquid water droplets can be changed in...   \n",
       "4  By comparison, the earth rotates on its axis o...   \n",
       "\n",
       "                                          hypothesis    label  \n",
       "0   Earth rotates on its axis once times in one day.  neutral  \n",
       "1   Earth rotates on its axis once times in one day.  entails  \n",
       "2  The surface of the sun is much hotter than alm...  neutral  \n",
       "3  Evaporation is responsible for changing liquid...  entails  \n",
       "4   Earth rotates on its axis once times in one day.  entails  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b8d239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23083</th>\n",
       "      <td>which is not only the motion of our bodies, bu...</td>\n",
       "      <td>Work is done only if a force is exerted in the...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23084</th>\n",
       "      <td>The Red Star, that celestial curse whose eccen...</td>\n",
       "      <td>Red-shift refers to a shift toward red in the ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23085</th>\n",
       "      <td>The lines in the spectrum of a luminous body s...</td>\n",
       "      <td>Red-shift refers to a shift toward red in the ...</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23086</th>\n",
       "      <td>The radial velocity of a star away from or tow...</td>\n",
       "      <td>Red-shift refers to a shift toward red in the ...</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23087</th>\n",
       "      <td>This expansion causes the light from distant s...</td>\n",
       "      <td>Red-shift refers to a shift toward red in the ...</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "23083  which is not only the motion of our bodies, bu...   \n",
       "23084  The Red Star, that celestial curse whose eccen...   \n",
       "23085  The lines in the spectrum of a luminous body s...   \n",
       "23086  The radial velocity of a star away from or tow...   \n",
       "23087  This expansion causes the light from distant s...   \n",
       "\n",
       "                                              hypothesis    label  \n",
       "23083  Work is done only if a force is exerted in the...  neutral  \n",
       "23084  Red-shift refers to a shift toward red in the ...  neutral  \n",
       "23085  Red-shift refers to a shift toward red in the ...  entails  \n",
       "23086  Red-shift refers to a shift toward red in the ...  entails  \n",
       "23087  Red-shift refers to a shift toward red in the ...  entails  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a38ce21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on the list provided of the uses of subs...</td>\n",
       "      <td>If a substance has a ph value greater than 7,t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If one or two            base pairs are change...</td>\n",
       "      <td>Invertebrates (and higher animals) can also be...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>At high temperatures, the solid dye converts i...</td>\n",
       "      <td>Gases and liquids become solids at low tempera...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chapter 11 Gas and Kinetic Theory .</td>\n",
       "      <td>The behavior of ideal gases is explained by ki...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Both the continental crust and the oceanic cru...</td>\n",
       "      <td>Gabbro is a dark dense rock that can be found ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  Based on the list provided of the uses of subs...   \n",
       "1  If one or two            base pairs are change...   \n",
       "2  At high temperatures, the solid dye converts i...   \n",
       "3                Chapter 11 Gas and Kinetic Theory .   \n",
       "4  Both the continental crust and the oceanic cru...   \n",
       "\n",
       "                                          hypothesis    label  \n",
       "0  If a substance has a ph value greater than 7,t...  neutral  \n",
       "1  Invertebrates (and higher animals) can also be...  neutral  \n",
       "2  Gases and liquids become solids at low tempera...  neutral  \n",
       "3  The behavior of ideal gases is explained by ki...  neutral  \n",
       "4  Gabbro is a dark dense rock that can be found ...  neutral  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e37ae",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917129ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DCCN9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DCCN9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "if not nltk.download('punkt'):\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "if not nltk.download('stopwords'):\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91b88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are just common English contractions. We used it in Lab 5 before!\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
    "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94ade6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DCCN9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stemmer and Lemmatizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "if not nltk.download('wordnet'):\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b71d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for lemmatization with POS tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3309d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowed POS tags for filtering (example: nouns, verbs, adjectives, adverbs)\n",
    "allowed_pos_tags = {'NN', 'NNS', 'NNP', 'NNPS',   # Nouns\n",
    "                    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Verbs\n",
    "                    'JJ', 'JJR', 'JJS',           # Adjectives\n",
    "                    'RB', 'RBR', 'RBS'}           # Adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec143704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the desired POS tags\n",
    "def filter_tokens_by_pos(tokens):\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    filtered = [word for word, tag in tagged_tokens if tag in allowed_pos_tags]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ae31e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "MIN_WORD_LENGTH = 3\n",
    "MAX_WORD_LENGTH = 124 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97386b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset, MIN_WORD_LENGTH=3, MAX_WORD_LENGTH=50, method=None, pos_filter=False, stop_w=False):\n",
    "    cleaned_premises = []\n",
    "    cleaned_hypotheses = []\n",
    "    cleaned_labels = []\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        premise = row['premise']\n",
    "        hypothesis = row['hypothesis']\n",
    "        label = row['label']\n",
    "\n",
    "        # Lowercase\n",
    "        premise = premise.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "        # Expand contractions\n",
    "        for contraction, full_form in contraction_dict.items():\n",
    "            premise = premise.replace(contraction, full_form)\n",
    "            hypothesis = hypothesis.replace(contraction, full_form)\n",
    "\n",
    "        # Remove punctuation/special chars\n",
    "        premise = re.sub(r'[^\\w\\s]', '', premise)\n",
    "        hypothesis = re.sub(r'[^\\w\\s]', '', hypothesis)\n",
    "\n",
    "        # Normalize whitespace\n",
    "        premise = re.sub(r'\\s+', ' ', premise).strip()\n",
    "        hypothesis = re.sub(r'\\s+', ' ', hypothesis).strip()\n",
    "\n",
    "        # Tokenization\n",
    "        premise_tokens = word_tokenize(premise)\n",
    "        hypothesis_tokens = word_tokenize(hypothesis)\n",
    "\n",
    "        # Stemming/Lemmatization\n",
    "        if method == 'stem':\n",
    "            stemmer = PorterStemmer()\n",
    "            premise_tokens = [stemmer.stem(word) for word in premise_tokens]\n",
    "            hypothesis_tokens = [stemmer.stem(word) for word in hypothesis_tokens]\n",
    "        elif method == 'lemmatize':\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            premise_pos_tags = pos_tag(premise_tokens)\n",
    "            hypothesis_pos_tags = pos_tag(hypothesis_tokens)\n",
    "            premise_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in premise_pos_tags]\n",
    "            hypothesis_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in hypothesis_pos_tags]\n",
    "\n",
    "        # POS Filtering\n",
    "        if pos_filter:\n",
    "            premise_tokens = filter_tokens_by_pos(premise_tokens)\n",
    "            hypothesis_tokens = filter_tokens_by_pos(hypothesis_tokens)\n",
    "\n",
    "        # Remove stop words\n",
    "        if stop_w:\n",
    "            premise_tokens = [word for word in premise_tokens if word not in stop_words]\n",
    "            hypothesis_tokens = [word for word in hypothesis_tokens if word not in stop_words]\n",
    "\n",
    "        # Now check token length AFTER cleaning\n",
    "        if (MIN_WORD_LENGTH <= len(premise_tokens) <= MAX_WORD_LENGTH and\n",
    "            MIN_WORD_LENGTH <= len(hypothesis_tokens) <= MAX_WORD_LENGTH):\n",
    "            cleaned_premises.append(premise_tokens)\n",
    "            cleaned_hypotheses.append(hypothesis_tokens)\n",
    "            cleaned_labels.append(label)\n",
    "        # else: skip row\n",
    "\n",
    "    # Build DataFrame from all cleaned token lists\n",
    "    new_dataset = pd.DataFrame({\n",
    "        'premise': cleaned_premises,\n",
    "        'hypothesis': cleaned_hypotheses,\n",
    "        'label': cleaned_labels\n",
    "    })\n",
    "\n",
    "    return new_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "810a3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_dataset = clean_dataset(train_df, stop_w=False, method=None, pos_filter=False,MAX_WORD_LENGTH=60)\n",
    "clean_test_dataset = clean_dataset(test_df,stop_w=False, method=None, pos_filter= False )\n",
    "clean_validation_dataset = clean_dataset(validation_df,stop_w=False, method=None, pos_filter= False )\n",
    "\n",
    "clean_t_dataset = clean_train_dataset['premise'] + clean_train_dataset['hypothesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3779c637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pluto, rotates, once, on, its, axis, every, 6...</td>\n",
       "      <td>[earth, rotates, on, its, axis, once, times, i...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[glenn, once, per, day, the, earth, rotates, a...</td>\n",
       "      <td>[earth, rotates, on, its, axis, once, times, i...</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[geysers, periodic, gush, of, hot, water, at, ...</td>\n",
       "      <td>[the, surface, of, the, sun, is, much, hotter,...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[facts, liquid, water, droplets, can, be, chan...</td>\n",
       "      <td>[evaporation, is, responsible, for, changing, ...</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[by, comparison, the, earth, rotates, on, its,...</td>\n",
       "      <td>[earth, rotates, on, its, axis, once, times, i...</td>\n",
       "      <td>entails</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  [pluto, rotates, once, on, its, axis, every, 6...   \n",
       "1  [glenn, once, per, day, the, earth, rotates, a...   \n",
       "2  [geysers, periodic, gush, of, hot, water, at, ...   \n",
       "3  [facts, liquid, water, droplets, can, be, chan...   \n",
       "4  [by, comparison, the, earth, rotates, on, its,...   \n",
       "\n",
       "                                          hypothesis    label  \n",
       "0  [earth, rotates, on, its, axis, once, times, i...  neutral  \n",
       "1  [earth, rotates, on, its, axis, once, times, i...  entails  \n",
       "2  [the, surface, of, the, sun, is, much, hotter,...  neutral  \n",
       "3  [evaporation, is, responsible, for, changing, ...  entails  \n",
       "4  [earth, rotates, on, its, axis, once, times, i...  entails  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f86f3c4",
   "metadata": {},
   "source": [
    "## Build dictionary of words and word2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e95f693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [pluto, rotates, once, on, its, axis, every, 6...\n",
      "1        [glenn, once, per, day, the, earth, rotates, a...\n",
      "2        [geysers, periodic, gush, of, hot, water, at, ...\n",
      "3        [facts, liquid, water, droplets, can, be, chan...\n",
      "4        [by, comparison, the, earth, rotates, on, its,...\n",
      "                               ...                        \n",
      "22910    [which, is, not, only, the, motion, of, our, b...\n",
      "22911    [the, red, star, that, celestial, curse, whose...\n",
      "22912    [the, lines, in, the, spectrum, of, a, luminou...\n",
      "22913    [the, radial, velocity, of, a, star, away, fro...\n",
      "22914    [this, expansion, causes, the, light, from, di...\n",
      "Name: premise, Length: 22915, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(clean_train_dataset['premise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c0d42bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique words: 22140\n"
     ]
    }
   ],
   "source": [
    "# Create a unique word list from the cleaned dataset\n",
    "unique_words = set()\n",
    "for sentence in clean_t_dataset: # Both Premise and Hypothesis \n",
    "    for word in sentence:\n",
    "        unique_words.add(word)\n",
    "\n",
    "print(f\"Count of unique words: {len(unique_words)}\")\n",
    "#No lemmatization or stemming count = 22555\n",
    "#Lemmatization count = 18986\n",
    "#Stemming count = 15954\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73c6c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_list = sorted(list(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "759cdfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of words with each method.\n",
    "\n",
    "word2id = {w:i for i,w in enumerate(unique_words_list)}\n",
    "id2word = {i:w for i,w in enumerate(unique_words_list)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2358751",
   "metadata": {},
   "source": [
    "# Build Embedding Model using Gensim and FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9c274b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1a241c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 1024\n",
    "learn_rate = 0.001\n",
    "embedding_size = 200\n",
    "no_of_epochs = 8\n",
    "window_size = 3\n",
    "vocab_size = len(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f5cb6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_model = FastText(clean_t_dataset, # Both premise and Hypothesis \n",
    "                           vector_size=embedding_size,\n",
    "                           window=window_size,\n",
    "                           sg=1,\n",
    "                           epochs=no_of_epochs\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f01dcf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('saturns', 0.9542921781539917),\n",
       " ('1980', 0.8083610534667969),\n",
       " ('neptune', 0.8019237518310547),\n",
       " ('1979', 0.7835524678230286),\n",
       " ('1981', 0.7748010158538818),\n",
       " ('voyager', 0.7630559206008911),\n",
       " ('1980s', 0.7596765160560608),\n",
       " ('180', 0.7465062737464905),\n",
       " ('spacecraft', 0.744134783744812),\n",
       " ('1989', 0.7426692247390747)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text_model.wv.most_similar('saturn', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a81f2",
   "metadata": {},
   "source": [
    "# Transformer Model Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c199dc1",
   "metadata": {},
   "source": [
    "## First we need to prepare the data for to input into the transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ea8b5",
   "metadata": {},
   "source": [
    "Lets add some special tokens to my vocabularies to use as separation tokens [SEP] [PAD]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85a51cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['[SEP]', '[PAD]']\n",
    "for tok in SPECIAL_TOKENS:\n",
    "    if tok not in word2id:\n",
    "        idx = len(word2id)\n",
    "        word2id[tok] = idx\n",
    "        id2word[idx] = tok\n",
    "\n",
    "sep_index = word2id['[SEP]']\n",
    "pad_index = word2id['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57ad55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Transformer\n",
    "#max_sequence_len = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d45d43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a preparation data function for the transformer\n",
    "def prepare_transformer_data(df, word2id, max_sequence_len):\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    # Assuming pad_index is defined globally or passed in\n",
    "    pad_index = word2id['[PAD]']\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        premise = row['premise']\n",
    "        hypothesis = row['hypothesis']\n",
    "        label = row['label']\n",
    "\n",
    "        tokens = premise + ['[SEP]'] + hypothesis\n",
    "        \n",
    "        # Truncate tokens if they are too long FIRST\n",
    "        tokens = tokens[:max_sequence_len]\n",
    "        \n",
    "        indices = [word2id.get(w, pad_index) for w in tokens]\n",
    "        \n",
    "        # Create attention mask based on the TRUE length of tokens\n",
    "        attn = [1] * len(indices)\n",
    "\n",
    "        # Now, pad both indices and the attention mask\n",
    "        pad_len = max_sequence_len - len(indices)\n",
    "        if pad_len > 0:\n",
    "            indices += [pad_index] * pad_len\n",
    "            attn += [0] * pad_len # Correctly add 'pad_len' zeros\n",
    "\n",
    "        input_ids.append(indices)\n",
    "        attention_mask.append(attn)\n",
    "        labels.append(0 if label == 'neutral' else 1)\n",
    "\n",
    "    return (np.array(input_ids),\n",
    "            np.array(attention_mask),\n",
    "            np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3958d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, train_mask, y_train = prepare_transformer_data(clean_train_dataset, word2id, 124)\n",
    "x_test, test_mask, y_test = prepare_transformer_data(clean_test_dataset, word2id, 124)\n",
    "x_val, val_mask, y_val  = prepare_transformer_data(clean_validation_dataset, word2id, 124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ff92711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15329, 17393, 14064, 14062, 11000,  2278,  7466,   690,  6679,\n",
       "        5544, 22140,  6679, 17393, 14062, 11000,  2278, 14064, 20248,\n",
       "       10298, 14075,  5541, 22141, 22141, 22141, 22141, 22141, 22141,\n",
       "       22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141,\n",
       "       22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141,\n",
       "       22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141,\n",
       "       22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141,\n",
       "       22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141,\n",
       "       22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141,\n",
       "       22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141,\n",
       "       22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141,\n",
       "       22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141,\n",
       "       22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141, 22141,\n",
       "       22141, 22141, 22141, 22141, 22141, 22141, 22141])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0a2e16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22140"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8b1817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pluto', 'rotates', 'once', 'on', 'its', 'axis', 'every', '639', 'earth', 'days', '[SEP]', 'earth', 'rotates', 'on', 'its', 'axis', 'once', 'times', 'in', 'one', 'day', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print([id2word[idx] for idx in x_train[0]])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec046559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c717d4dd",
   "metadata": {},
   "source": [
    "Now that we have the data as the transformer expects it, lets move to the transformer.\n",
    "For the next steps i expect to build:\n",
    "\n",
    "- Embedding Layer: Use my FastText with nn.Embeddings for the transformer\n",
    "- Positional Encoding: Add the positional encoding to inject word order information.\n",
    "- Transformer Encoder Layer\n",
    "- Classification Head\n",
    "- Forward Pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5470c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters Transformer \n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 7\n",
    "LR = 0.00008\n",
    "NUM_CLASSES = 2\n",
    "N_HEADS = 2\n",
    "N_LAYERS = 4\n",
    "DROPOUT = 0.2\n",
    "WEIGHT_DECAY = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d18797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math \n",
    "\n",
    "# Positional Encoding Class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model) \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.pe[:,:x.size(1),:]\n",
    "        return x\n",
    "    \n",
    "# Transformer Class  \n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_classes):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        # Pre trained embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False)\n",
    "        # Train embeddings itself\n",
    "        #self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=MAX_WORD_LENGTH)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=N_HEADS, dropout=DROPOUT) \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=N_LAYERS) \n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        embedded = self.embedding(x) # [batch_size, seq_len, embed_dim]\n",
    "        embedded = self.pos_encoder(embedded) # Add positional encoding \n",
    "\n",
    "        # Create boolean padding mask\n",
    "        padding_mask = (attention_mask == 0).to(x.device)\n",
    "\n",
    "        # Transformer expects [seq_len, batch_size, embed_dim]\n",
    "        encoded = self.transformer_encoder(embedded.transpose(0,1), src_key_padding_mask=padding_mask)\n",
    "        # Without using the attention mask \n",
    "        #encoded = self.transformer_encoder(embedded.transpose(0,1))\n",
    "        #Mean pool over seq_len: output shape [batch_size, embed_dim]\n",
    "        pooled = encoded.mean(dim=0)\n",
    "\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d5f37",
   "metadata": {},
   "source": [
    "Now lets build the embedding matrix from the FastText using the word2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba94441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b672a758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DCCN9\\OneDrive - UWA\\Data Science\\Semester July-November 2025\\CITS4012 - Natural Language Processing\\Team Assignment 2 NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = fast_text_model.vector_size\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "#Initialize the embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, idx in word2id.items():\n",
    "    if word in fast_text_model.wv:\n",
    "        embedding_matrix[idx] = fast_text_model.wv[word]\n",
    "    else:\n",
    "        #Initialize unkown words\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# Convert the prepared input data 'x' to a PyTorch tensor\n",
    "x_tensor = torch.LongTensor(x_train[:8]) # example mini-batch\n",
    "\n",
    "#Instantiate your transformer Model\n",
    "transformer_model = TransformerClassifier(embedding_matrix, num_classes=NUM_CLASSES)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54b7236",
   "metadata": {},
   "source": [
    "For the last steps, lets define the optimizer and loss functin\n",
    "Create the trainning loop\n",
    "and build an evaluation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a6e75bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert the full datasets to PyTorch tensors\n",
    "# Convert the full datasets to PyTorch tensors, now including the masks\n",
    "train_dataset = TensorDataset(torch.LongTensor(x_train), torch.LongTensor(train_mask), torch.LongTensor(y_train))\n",
    "test_dataset = TensorDataset(torch.LongTensor(x_test), torch.LongTensor(test_mask), torch.LongTensor(y_test))\n",
    "val_dataset = TensorDataset(torch.LongTensor(x_val), torch.LongTensor(val_mask), torch.LongTensor(y_val))\n",
    "\n",
    "# The DataLoader setup doesn't need to change; it will now yield batches of three tensors.\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8293681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # 1. Unpack all three items from the dataloader\n",
    "    for inputs, masks, labels in dataloader:\n",
    "        # 2. Move all tensors to the selected device\n",
    "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 3. Pass both inputs and masks to the model\n",
    "        outputs = model(inputs, masks)\n",
    "        # Without the attention mask\n",
    "        #outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 1. Unpack all three items from the dataloader\n",
    "        for inputs, masks, labels in dataloader:\n",
    "            # 2. Move all tensors to the selected device\n",
    "            inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "            \n",
    "            # 3. Pass both inputs and masks to the model\n",
    "            outputs = model(inputs, masks)\n",
    "            # Without the attention mask\n",
    "            #outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951515b",
   "metadata": {},
   "source": [
    "Now we move to the trainning and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "25d4f19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1/7 - Train Loss: 0.6474, Train Acc: 0.6343 - Val Loss: 0.6970, Val Acc: 0.5661\n",
      "Epoch 2/7 - Train Loss: 0.5567, Train Acc: 0.7136 - Val Loss: 0.9614, Val Acc: 0.5998\n",
      "Epoch 3/7 - Train Loss: 0.5159, Train Acc: 0.7461 - Val Loss: 0.7488, Val Acc: 0.6559\n",
      "Epoch 4/7 - Train Loss: 0.4821, Train Acc: 0.7707 - Val Loss: 0.7780, Val Acc: 0.6544\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Main training loop\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     train_loss, train_acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     val_loss, val_acc = evaluate(transformer_model, val_loader, criterion, device)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     21\u001b[39m loss.backward()\n\u001b[32m     22\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * inputs.size(\u001b[32m0\u001b[39m)\n\u001b[32m     25\u001b[39m preds = outputs.argmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     26\u001b[39m correct += (preds == labels).sum().item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transformer_model.to(device)\n",
    "print(device)\n",
    "# Main training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(transformer_model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(transformer_model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016842b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL TEST - Loss: 0.7863, Acc: 0.7116\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(transformer_model, test_loader, criterion, device)\n",
    "print(f\"FINAL TEST - Loss: {test_loss:.4f}, Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01608727",
   "metadata": {},
   "source": [
    "## Evaluation of the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108729c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.71\n",
      "F1 Score:  0.55\n",
      "Precision: 0.73\n",
      "Recall:    0.43\n",
      "----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.70      0.90      0.79      1270\n",
      "     entails       0.73      0.43      0.55       842\n",
      "\n",
      "    accuracy                           0.71      2112\n",
      "   macro avg       0.72      0.66      0.67      2112\n",
      "weighted avg       0.72      0.71      0.69      2112\n",
      "\n",
      "----------------------------------------\n",
      "Hyperparameters\n",
      "Batch Size 64\n",
      "Num of Epochs 10\n",
      "Learning Rate 0.0001\n",
      "Num Classes 2\n",
      "Number of Heads 2\n",
      "Numer of Layere 2\n",
      "Embedding Dimensions 200\n",
      "Dropout 0.2\n",
      "Weight Decay 0.0005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "transformer_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in test_loader:\n",
    "        input_ids      = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels         = labels.to(device)\n",
    "\n",
    "        logits = transformer_model(input_ids, attention_mask)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy:  {accuracy_score(all_labels, all_preds):.2f}\")\n",
    "print(f\"F1 Score:  {f1_score(all_labels, all_preds):.2f}\")\n",
    "print(f\"Precision: {precision_score(all_labels, all_preds):.2f}\")\n",
    "print(f\"Recall:    {recall_score(all_labels, all_preds):.2f}\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"neutral\", \"entails\"]))\n",
    "\n",
    "\n",
    "\n",
    "print('-' * 40)\n",
    "print('Hyperparameters')\n",
    "print('Batch Size',BATCH_SIZE)\n",
    "print('Num of Epochs',NUM_EPOCHS)\n",
    "print('Learning Rate',LR)\n",
    "print('Num Classes',NUM_CLASSES)\n",
    "print('Number of Heads',N_HEADS)\n",
    "print('Numer of Layere',N_LAYERS)\n",
    "print('Embedding Dimensions',embedding_dim)\n",
    "print('Dropout',DROPOUT)\n",
    "print('Weight Decay',WEIGHT_DECAY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
