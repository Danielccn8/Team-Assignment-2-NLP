{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32yCsRUo8H33"
   },
   "source": [
    "# 2025 CITS4012 Project\n",
    "\n",
    "_Make sure you change the file name with your group id._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "CUDA version: None\n",
      "Number of GPUs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\msi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report  # ADD THIS LINE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "import nltk\n",
    "if not nltk.download('punkt'):\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "if not nltk.download('stopwords'):\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# Stemmer and Lemmatizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "if not nltk.download('wordnet'):\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sys.path.append(os.path.abspath('..')) \n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "sys.path.append(os.path.abspath('../')) # Points to the current folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "# Helper functions for training\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **We build a Configuration class to have all the hyperparameters together**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class construct for hyperparameters \n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.min_word_len = 3\n",
    "        self.max_word_len = 64\n",
    "        self.train_num_epoch = 1\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        self.embedding_dim = 200\n",
    "        self.hidden_dim = 64\n",
    "        self.max_combined_len = 130 #to join premise + hypothesis + cls + sep \n",
    "        self.patience = 5\n",
    "        #Embedding\n",
    "        self.embedding_method = \"\"\n",
    "        self.emb_lr = 0.0005\n",
    "        self.emb_epoch = 3\n",
    "        self.emb_window_size = 5\n",
    "        # Models\n",
    "        self.n_heads = 2\n",
    "        self.n_layers = 1\n",
    "        self.dropout = 0.3\n",
    "        self.num_classes = 2\n",
    "        self.lr = 0.00005\n",
    "        self.wd = 0.0005\n",
    "        self.train_num_epoch = 7\n",
    "        self.label_smoothing = 0.0\n",
    "\n",
    "    def display(self): # Display all the config parameters \n",
    "        print(\"=\" * 60)\n",
    "        print(\"CONFIGURATION PARAMETERS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nEmbedding Parameters:\")\n",
    "        print(f\"  Method: {self.embedding_method}\")\n",
    "        print(f\"  Max length: {self.max_word_len}\")\n",
    "        print(f\"  Learning rate: {self.emb_lr}\")\n",
    "        print(f\"  Embedding size: {self.embedding_dim}\")\n",
    "        print(f\"  Epochs: {self.emb_epoch}\")\n",
    "        print(f\"  Window size: {self.emb_window_size}\")\n",
    "        \n",
    "        print(\"\\nTransformer Parameters:\")\n",
    "        print(f\"  Batch size: {self.batch_size}\")\n",
    "        print(f\"  Number of heads: {self.n_heads}\")\n",
    "        print(f\"  Number of layers: {self.n_layers}\")\n",
    "        print(f\"  Dropout: {self.dropout}\")\n",
    "        print(f\"  Number of classes: {self.num_classes}\")\n",
    "        print(f\"  Learning rate: {self.lr}\")\n",
    "        print(f\"  Weight decay: {self.wd}\")\n",
    "        print(f\"  Training epochs: {self.train_num_epoch}\")\n",
    "\n",
    "        print(f\"  Label smoothing: {self.label_smoothing}\")\n",
    "        print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCybYoGz8YWQ"
   },
   "source": [
    "# Readme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6po98qVA8bJD"
   },
   "source": [
    "# 1.Dataset Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# These are common English contractions.\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
    "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "# Helper function for lemmatization with POS tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no match\n",
    "    \n",
    "# Allowed POS tags for filtering (example: nouns, verbs, adjectives, adverbs)\n",
    "allowed_pos_tags = {'NN', 'NNS', 'NNP', 'NNPS',   # Nouns\n",
    "                    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Verbs\n",
    "                    'JJ', 'JJR', 'JJS',           # Adjectives\n",
    "                    'RB', 'RBR', 'RBS'}           # Adverbs\n",
    "\n",
    "# Filter the desired POS tags\n",
    "def filter_tokens_by_pos(tokens):\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    filtered = [word for word, tag in tagged_tokens if tag in allowed_pos_tags]\n",
    "    return filtered\n",
    "\n",
    "def clean_dataset(dataset, min_word_len, max_word_len, method=['stem, lemmatize'], pos_filter=False, stop_w=False, clean_nums = False):\n",
    "    cleaned_premises = []\n",
    "    cleaned_hypotheses = []\n",
    "    cleaned_labels = []\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        premise = row['premise']\n",
    "        hypothesis = row['hypothesis']\n",
    "        label = row['label']\n",
    "\n",
    "        # Lowercase\n",
    "        premise = premise.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "        # Expand contractions\n",
    "        for contraction, full_form in contraction_dict.items():\n",
    "            premise = premise.replace(contraction, full_form)\n",
    "            hypothesis = hypothesis.replace(contraction, full_form)\n",
    "\n",
    "        # Remove punctuation/special chars\n",
    "        premise = re.sub(r'[^a-zA-Z0-9\\s.-]', ' ', premise)\n",
    "        hypothesis = re.sub(r'[^a-zA-Z0-9\\s.-]', ' ', hypothesis)\n",
    "\n",
    "        # Replace underscores/hyphens with spaces, then normalize whitespace\n",
    "        premise = re.sub(r'[-â€“â€”_]+', ' ', premise)\n",
    "        hypothesis = re.sub(r'[-â€“â€”_]+', ' ', hypothesis)\n",
    "\n",
    "        # Normalize whitespace\n",
    "        premise = re.sub(r'\\s+', ' ', premise).strip()\n",
    "        hypothesis = re.sub(r'\\s+', ' ', hypothesis).strip()\n",
    "\n",
    "        # Tokenization\n",
    "        premise_tokens = word_tokenize(premise)\n",
    "        hypothesis_tokens = word_tokenize(hypothesis)\n",
    "\n",
    "        # # Replace numbers with '<NUM>'\n",
    "        if clean_nums is True:\n",
    "            premise_tokens = ['[NUM]' if any(char.isdigit() for char in word) else word for word in premise_tokens]\n",
    "            hypothesis_tokens = ['[NUM]' if any(char.isdigit() for char in word) else word for word in hypothesis_tokens]\n",
    "\n",
    "\n",
    "        # Stemming/Lemmatization\n",
    "        if 'stem' in method:\n",
    "            stemmer = PorterStemmer()\n",
    "            premise_tokens = [stemmer.stem(word) for word in premise_tokens]\n",
    "            hypothesis_tokens = [stemmer.stem(word) for word in hypothesis_tokens]\n",
    "        elif 'lemmatize' in method:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            premise_pos_tags = pos_tag(premise_tokens)\n",
    "            hypothesis_pos_tags = pos_tag(hypothesis_tokens)\n",
    "            premise_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in premise_pos_tags]\n",
    "            hypothesis_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in hypothesis_pos_tags]\n",
    "\n",
    "        # POS Filtering\n",
    "        if pos_filter:\n",
    "            premise_tokens = filter_tokens_by_pos(premise_tokens)\n",
    "            hypothesis_tokens = filter_tokens_by_pos(hypothesis_tokens)\n",
    "\n",
    "        # Remove stop words\n",
    "        if stop_w:\n",
    "            premise_tokens = [word for word in premise_tokens if word not in stop_words]\n",
    "            hypothesis_tokens = [word for word in hypothesis_tokens if word not in stop_words]\n",
    "\n",
    "        # Now check token length AFTER cleaning\n",
    "        if (min_word_len <= len(premise_tokens) <= max_word_len and\n",
    "            min_word_len <= len(hypothesis_tokens) <= max_word_len):\n",
    "            cleaned_premises.append(premise_tokens)\n",
    "            cleaned_hypotheses.append(hypothesis_tokens)\n",
    "            cleaned_labels.append(label)\n",
    "        # else: skip row\n",
    "\n",
    "    # Build DataFrame from all cleaned token lists\n",
    "    new_dataset = pd.DataFrame({\n",
    "        'premise': cleaned_premises,\n",
    "        'hypothesis': cleaned_hypotheses,\n",
    "        'label': cleaned_labels\n",
    "    })\n",
    "\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Make the Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [pluto, rotates, axis, every, 6.39, earth, day...\n",
       "1    [glenn, per, day, earth, rotates, axis, ., ear...\n",
       "2    [geysers, periodic, gush, hot, water, surface,...\n",
       "3    [facts, liquid, water, droplets, changed, invi...\n",
       "4    [comparison, earth, rotates, axis, per, day, r...\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data\n",
    "train_df = pd.read_json('train.json')\n",
    "test_df = pd.read_json('test.json')\n",
    "validation_df = pd.read_json('validation.json')\n",
    "\n",
    "# Clean datasets. MAX_WORD_LENGTH set to 64 to remove very long texts\n",
    "clean_train_dataset = clean_dataset(train_df, hyperparameters.min_word_len, hyperparameters.max_word_len, stop_w=True, method=[], pos_filter=False)\n",
    "clean_test_dataset = clean_dataset(test_df, hyperparameters.min_word_len, hyperparameters.max_word_len,stop_w=True, method=[], pos_filter= False )\n",
    "clean_validation_dataset = clean_dataset(validation_df, hyperparameters.min_word_len, hyperparameters.max_word_len,stop_w=True, method=[], pos_filter= False )\n",
    "\n",
    "# Combine clean premises and hypotheses\n",
    "clean_t_dataset = clean_train_dataset['premise'] + clean_train_dataset['hypothesis']\n",
    "clean_t_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36685724185724183"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_df['label']=='entails') / (sum(train_df['label']!='entails') + sum(train_df['label']=='entails'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5038343558282209"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(validation_df['label']=='entails') / (sum(validation_df['label']!='entails') + sum(validation_df['label']=='entails'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39604891815616183"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_df['label']=='entails') / (sum(test_df['label']!='entails') + sum(test_df['label']=='entails'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique word list from the cleaned dataset\n",
    "unique_words = set()\n",
    "for sentence in clean_t_dataset: # Both Premise and Hypothesis \n",
    "    for word in sentence:\n",
    "        unique_words.add(word)\n",
    "        \n",
    "unique_words_list = sorted(list(unique_words))\n",
    "\n",
    "# Make dictionary of words and indices\n",
    "word2id = {w:i for i,w in enumerate(unique_words_list)}\n",
    "id2word = {i:w for i,w in enumerate(unique_words_list)}\n",
    "\n",
    "\n",
    "# Add special tokens to use later\n",
    "SPECIAL_TOKENS = ['[PAD]','[UNK]','[CLS]','[SEP]'] \n",
    "for tok in SPECIAL_TOKENS:\n",
    "    if tok not in word2id:\n",
    "        idx = len(word2id)\n",
    "        word2id[tok] = idx\n",
    "        id2word[idx] = tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_ID = word2id['[SEP]']\n",
    "UNK_ID = word2id['[UNK]']\n",
    "PAD_ID = word2id['[PAD]']\n",
    "CLS_ID = word2id['[CLS]']\n",
    "\n",
    "\n",
    "def prepare_indexed_data(df, word2id, max_len):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    label_map = {'neutral': 0, 'entails': 1}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        premise_toks = row['premise']\n",
    "        hypothesis_toks = row['hypothesis']\n",
    "        label = row['label']\n",
    "\n",
    "        # Add special tokens \n",
    "        tokens = [CLS_ID] \\\n",
    "                + [word2id.get(w,UNK_ID) for w in premise_toks] \\\n",
    "                + [SEP_ID] \\\n",
    "                + [word2id.get(w,UNK_ID) for w in hypothesis_toks] \n",
    "        # Truncate\n",
    "        tokens = tokens[:max_len] #to combine len of hypothesis and premise, plus cls and sep  \n",
    "\n",
    "        # Attention mask \n",
    "        attn = [1] * len(tokens)\n",
    "\n",
    "        # Pad\n",
    "        pad_len = max_len - len(tokens) # To fill the [PAD]\n",
    "        if pad_len > 0:\n",
    "            tokens += [PAD_ID] * pad_len\n",
    "            attn += [0] * pad_len # FLag positions as padding \n",
    "\n",
    "        input_ids.append(tokens)\n",
    "        attention_mask.append(attn)\n",
    "        labels.append(label_map[label])\n",
    "\n",
    "    return (torch.LongTensor(input_ids),\n",
    "            np.array(input_ids),\n",
    "            torch.LongTensor(attention_mask),\n",
    "            torch.LongTensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: torch.Size([22900, 130])\n",
      "Shape of train_masks: torch.Size([22900, 130])\n",
      "Shape of y_train: torch.Size([22900])\n",
      "\n",
      "--- Example ---\n",
      "First example real length (mask sum): 16\n",
      "First example PAD count: 114\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build train/test\n",
    "x_train, plain_train, train_masks, y_train = prepare_indexed_data(clean_train_dataset, word2id, hyperparameters.max_combined_len)\n",
    "x_test,  plain_test, test_masks,  y_test  = prepare_indexed_data(clean_test_dataset,  word2id, hyperparameters.max_combined_len)\n",
    "x_valid,  plain_valid, valid_masks,  y_valid  = prepare_indexed_data(clean_validation_dataset,  word2id, hyperparameters.max_combined_len)\n",
    "\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of train_masks:\", train_masks.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "\n",
    "# Sanity Check \n",
    "i = 0\n",
    "print(\"\\n--- Example ---\")\n",
    "print(\"First example real length (mask sum):\", int(train_masks[i].sum()))\n",
    "print(\"First example PAD count:\", int((train_masks[i]==0).sum()))\n",
    "\n",
    "# Check we didn't cut off too much\n",
    "sns.histplot(np.sum(train_masks.numpy(), axis=1), bins = 16)\n",
    "plt.axvline(hyperparameters.max_word_len, color='red')\n",
    "plt.show()\n",
    "\n",
    "# Prepare the datasets for model training\n",
    "train_ds = TensorDataset(x_train, train_masks, y_train)\n",
    "test_ds = TensorDataset(x_test, test_masks,y_test)\n",
    "valid_ds = TensorDataset(x_valid, valid_masks,y_valid)\n",
    "\n",
    "# Dataloaders (shuffle for train)\n",
    "train_loader = DataLoader(train_ds, batch_size=hyperparameters.batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=hyperparameters.batch_size, shuffle=False, num_workers=0)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=hyperparameters.batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first batch from the train_loader Get the first batch from the train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Make an embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix created with shape: (20264, 200)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fast_text_model = FastText(clean_t_dataset, # Both premise and Hypothesis \n",
    "                           vector_size=hyperparameters.embedding_dim,\n",
    "                           window=hyperparameters.emb_window_size,\n",
    "                           sg=1,\n",
    "                           epochs=hyperparameters.emb_epoch\n",
    "                           )\n",
    "\n",
    "fast_text_model.wv.most_similar('saturn', topn=10)\n",
    "\n",
    "def build_embedding_matrix(word2id, pretrained_vectors, embedding_size):\n",
    "    vocab_size = len(word2id)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size), dtype=np.float32)\n",
    "\n",
    "    # Fill the matrix with the pre-trained vectors\n",
    "    for word, idx in word2id.items():\n",
    "        if word == '[PAD]':# If [PAD] ignore \n",
    "            continue\n",
    "        try:\n",
    "            vec = pretrained_vectors[word] #Checks the size of the vector of each word\n",
    "            if vec.shape[0] == embedding_size:\n",
    "                embedding_matrix[idx] = vec.astype(np.float32)\n",
    "            else:\n",
    "                # fallback if dims donâ€™t match\n",
    "                embedding_matrix[idx] = np.random.normal(0.0, 0.02, size=(embedding_size,)).astype(np.float32)\n",
    "        except KeyError:\n",
    "            # special tokens or OOV start them with random values \n",
    "            embedding_matrix[idx] = np.random.normal(0.0, 0.02, size=(embedding_size,)).astype(np.float32)\n",
    "\n",
    "    print(\"Embedding matrix created with shape:\", embedding_matrix.shape)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix = build_embedding_matrix(word2id, fast_text_model.wv, hyperparameters.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedding matrix type: <class 'numpy.ndarray'>\n",
      "âœ… Embedding matrix device: numpy array\n"
     ]
    }
   ],
   "source": [
    "#check if it works\n",
    "# print(f\"âœ… Embedding matrix type: {type(embedding_matrix)}\")\n",
    "# print(f\"âœ… Embedding matrix device: {embedding_matrix.device if hasattr(embedding_matrix, 'device') else 'numpy array'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Creating embeddings...\n",
      "ðŸ”¤ Embedding matrix: torch.Size([20264, 200])\n",
      "ðŸ”¤ Embedding matrix type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 4: EMBEDDINGS & POSITIONAL ENCODING\n",
    "print(\"ðŸ”¤ Creating embeddings...\")\n",
    "def create_embedding_matrix(word2id, embedding_dim):\n",
    "    \"\"\"Create embedding matrix\"\"\"\n",
    "    vocab_size = len(word2id)\n",
    "    embedding_matrix = np.random.normal(0, 0.1, (vocab_size, embedding_dim))\n",
    "    # Set padding to zeros\n",
    "    embedding_matrix[PAD_ID] = np.zeros(embedding_dim)\n",
    "    # CONVERT TO PYTORCH TENSOR - THIS IS THE FIX\n",
    "    return torch.FloatTensor(embedding_matrix)  # Already correct!\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(word2id, hyperparameters.embedding_dim)\n",
    "print(f\"ðŸ”¤ Embedding matrix: {embedding_matrix.shape}\")\n",
    "print(f\"ðŸ”¤ Embedding matrix type: {type(embedding_matrix)}\")  # Should be torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " idx  token                 first 5 dims\n",
      "--------------------------------------------------------------------------------\n",
      "1619  antarctic             tensor([ 0.0413, -0.0466,  0.0563, -0.1083,  0.0254])\n",
      "12472  needlessly            tensor([-0.1340, -0.0400, -0.0625, -0.0684,  0.0397])\n",
      "15578  repetitive            tensor([-0.0374, -0.1523,  0.0493,  0.0365, -0.0651])\n",
      "12397  naplate               tensor([-0.0828, -0.0441,  0.0254,  0.1505,  0.0847])\n",
      "13917  phototroph            tensor([0.0149, 0.1098, 0.1413, 0.0998, 0.0361])\n"
     ]
    }
   ],
   "source": [
    "# Look at embedding matrix with random words\n",
    "candidates = [i for i in range(embedding_matrix.shape[0]) \n",
    "              if id2word.get(i) not in (None, \"[PAD]\")]\n",
    "\n",
    "rng = np.random\n",
    "chosen = rng.choice(candidates, size=min(5, len(candidates)), replace=False)\n",
    "\n",
    "print(f\"{'idx':>4}  {'token':<20}  first 5 dims\")\n",
    "print(\"-\" * 80)\n",
    "for i in chosen:\n",
    "    token = id2word[i]\n",
    "    print(f\"{i:>4}  {token:<20}  {embedding_matrix[i][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FA2ao2l8hOg"
   },
   "source": [
    "# 2. Model Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we have collected following parameters from the pre-processing:\n",
    "\n",
    "- VOCAB_SIZE\n",
    "- MAX_SEQ_LEN\n",
    "- BATCH_SIZE\n",
    "- embedding_dim\n",
    "  We have collected the objects:\n",
    "- embedding_matrix\n",
    "- fast_text_model\n",
    "- clean_train_dataset, clean_test_dataset, clean_validation_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We build a Configuration class to have all the hyperparameters together**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,max_len = 512, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0,max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.pe[:, :x.size(1), :] \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Different Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence_output):\n",
    "        scores = self.attention_net(sequence_output)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.sum(weights * sequence_output, dim=1)\n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cross_attention(source_seq, target_seq):\n",
    "    scores = torch.bmm(source_seq, target_seq.transpose(1, 2))\n",
    "    scores = scores / math.sqrt(source_seq.size(-1))\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    context = torch.bmm(weights, target_seq)\n",
    "    return context, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHead Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Transformer-style multi-head self-attention\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence, padding_mask=None):\n",
    "        output, weights = self.mha(\n",
    "            sequence, sequence, sequence,\n",
    "            key_padding_mask=padding_mask\n",
    "        )\n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model A: RNN with Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM With Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=2, dropout=0.3, num_layers = 2):\n",
    "        super(LSTMAttentionModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None, return_weights = False):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        context = self.dropout(context)\n",
    "\n",
    "        logits = self.fc(context)\n",
    "        if return_weights:\n",
    "            return logits, attn_weights\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gru with Additive Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUAdditiveAttention(nn.Module):\n",
    "    \"\"\"GRU with additive attention\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = AdditiveAttention(hidden_dim * 2)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        context, _ = self.attention(gru_out)\n",
    "        logits = self.classifier(context)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCrossAttention(nn.Module):\n",
    "    \"\"\"GRU with cross attention between premise and hypothesis\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.premise_gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.hypothesis_gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(hidden_dim * 4, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Split input\n",
    "        sep_pos = (x == SEP_ID).int().argmax(dim=1)\n",
    "        premise_mask = torch.arange(x.size(1), device=x.device).unsqueeze(0) < sep_pos.unsqueeze(1)\n",
    "        hypothesis_mask = ~premise_mask & (x != PAD_ID)\n",
    "        \n",
    "        premise = torch.where(premise_mask, x, PAD_ID)\n",
    "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
    "        \n",
    "        # Encode separately\n",
    "        prem_emb = self.embedding(premise)\n",
    "        hypo_emb = self.embedding(hypothesis)\n",
    "        \n",
    "        prem_encoded, _ = self.premise_gru(prem_emb)\n",
    "        hypo_encoded, _ = self.hypothesis_gru(hypo_emb)\n",
    "        \n",
    "        # Cross attention\n",
    "        cross_context, _ = apply_cross_attention(hypo_encoded, prem_encoded)\n",
    "        \n",
    "        # Pool and combine\n",
    "        hypo_pooled = hypo_encoded.mean(dim=1)\n",
    "        cross_pooled = cross_context.mean(dim=1)\n",
    "        combined = torch.cat([hypo_pooled, cross_pooled], dim=1)\n",
    "        \n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GRUMultiHeadAttention(nn.Module):\n",
    "    \"\"\"GRU with multi-head self-attention\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_ID)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.multihead_attn = MultiHeadSelfAttention(hidden_dim * 2, num_heads)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        attn_out, _ = self.multihead_attn(gru_out, padding_mask=(x == PAD_ID))\n",
    "        context = attn_out.mean(dim=1)\n",
    "        logits = self.classifier(context)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model B: Transformer with Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_classes, n_heads, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.d_model = embed_dim\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False, padding_idx=word2id['[PAD]'])\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=hyperparameters.max_combined_len, dropout=hyperparameters.dropout)\n",
    "\n",
    "        # Transformer Encoder Layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=n_heads,\n",
    "            dropout=hyperparameters.dropout,\n",
    "            dim_feedforward=(embed_dim*4),\n",
    "            batch_first=True \n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=n_layers)\n",
    "\n",
    "        # Final Classification Head\n",
    "        #self.classifier = nn.Linear(embed_dim,num_classes)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        padding_mask = (src_mask == 0)\n",
    "\n",
    "        # Apply embedding and positional encoding\n",
    "        embedded = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "\n",
    "        # Pass trough the transformer encoder\n",
    "        encoded = self.transformer_encoder(pos_encoded,src_key_padding_mask = padding_mask)\n",
    "\n",
    "        # Use the output of the [CLS] token for classification\n",
    "        cls_output = encoded[:,0,:]\n",
    "\n",
    "        # Get final logits from the classifier\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "    \n",
    "    # Get attention from the first layer\n",
    "    def get_attention_weights(self, src, src_mask):\n",
    "        # Get embeddings and positional encoding\n",
    "        padding_mask = (src_mask == 0)\n",
    "        embedded = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "\n",
    "        # Access the FIRST encoder layer \n",
    "        first_encoder_layer = self.transformer_encoder.layers[0]\n",
    "\n",
    "        # Call its self-attention module with need_weights=True\n",
    "        _, attn_weights = first_encoder_layer.self_attn(\n",
    "            pos_encoded, pos_encoded, pos_encoded,\n",
    "            key_padding_mask=padding_mask,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=False\n",
    "        )\n",
    "\n",
    "        return attn_weights.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAdditiveAttention(nn.Module):\n",
    "    \"\"\"Transformer with additive attention over all tokens\"\"\"\n",
    "    def __init__(self, embedding_matrix, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.d_model = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=PAD_ID)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, hyperparameters.max_combined_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        \n",
    "        self.additive_attn = AdditiveAttention(embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        padding_mask = (x == PAD_ID)\n",
    "        embedded = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "        encoded = self.transformer(pos_encoded, src_key_padding_mask=padding_mask)\n",
    "        context, _ = self.additive_attn(encoded)\n",
    "        logits = self.classifier(context)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerCrossAttention(nn.Module):\n",
    "    \"\"\"Transformer with cross attention between streams\"\"\"\n",
    "    def __init__(self, embedding_matrix, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.d_model = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=PAD_ID)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, hyperparameters.max_combined_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.premise_transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        self.hypothesis_transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        \n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=n_heads, batch_first=True\n",
    "        )\n",
    "        self.classifier = nn.Linear(embed_dim * 2, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Split input\n",
    "        sep_pos = (x == SEP_ID).int().argmax(dim=1)\n",
    "        premise_mask = torch.arange(x.size(1), device=x.device).unsqueeze(0) < sep_pos.unsqueeze(1)\n",
    "        hypothesis_mask = ~premise_mask & (x != PAD_ID)\n",
    "        \n",
    "        premise = torch.where(premise_mask, x, PAD_ID)\n",
    "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
    "        \n",
    "        # Encode separately\n",
    "        prem_emb = self.embedding(premise) * math.sqrt(self.d_model)\n",
    "        hypo_emb = self.embedding(hypothesis) * math.sqrt(self.d_model)\n",
    "        \n",
    "        prem_encoded = self.premise_transformer(\n",
    "            self.pos_encoder(prem_emb), \n",
    "            src_key_padding_mask=(premise == PAD_ID)\n",
    "        )\n",
    "        hypo_encoded = self.hypothesis_transformer(\n",
    "            self.pos_encoder(hypo_emb),\n",
    "            src_key_padding_mask=(hypothesis == PAD_ID)\n",
    "        )\n",
    "        \n",
    "        # Cross attention\n",
    "        cross_out, _ = self.cross_attn(\n",
    "            hypo_encoded, prem_encoded, prem_encoded,\n",
    "            key_padding_mask=(premise == PAD_ID)\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] tokens\n",
    "        prem_cls = prem_encoded[:, 0, :]\n",
    "        cross_cls = cross_out[:, 0, :]\n",
    "        combined = torch.cat([prem_cls, cross_cls], dim=1)\n",
    "        \n",
    "        logits = self.classifier(combined)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMultiHead(nn.Module):\n",
    "    \"\"\"Standard transformer with multi-head self-attention\"\"\"\n",
    "    def __init__(self, embedding_matrix, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.d_model = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=PAD_ID)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, hyperparameters.max_combined_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        self.classifier = nn.Linear(embed_dim, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        padding_mask = (x == PAD_ID)\n",
    "        embedded = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "        encoded = self.transformer(pos_encoded, src_key_padding_mask=padding_mask)\n",
    "        cls_output = encoded[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model C: Wide and Deep RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model seeks to build on Model A by splitting into three components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUThreeStreamNLI(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, wide_layers=2, prem_layers=2, hypo_layers=2, num_fc_layers=2, embedding_dropout=0.2, dropout=0.3, unk_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embed_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Encoders\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True,\n",
    "                          bidirectional=True, num_layers=wide_layers, dropout=dropout if wide_layers > 1 else 0.0)\n",
    "        self.prem_encoder = nn.GRU(\n",
    "            embedding_dim, hidden_dim, num_layers=prem_layers,\n",
    "            batch_first=True, bidirectional=True, dropout=dropout if prem_layers > 1 else 0.0\n",
    "        )\n",
    "        self.hypo_encoder = nn.GRU(\n",
    "            embedding_dim, hidden_dim, num_layers=hypo_layers,\n",
    "            batch_first=True, bidirectional=True, dropout=dropout if hypo_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
    "\n",
    "\n",
    "        # Self-attention for premise and hypothesis\n",
    "        self.self_attn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        # Cross-attention for the wide stream\n",
    "        self.cross_attn_query = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.cross_attn_key = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.cross_attn_value = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "        # Fusion and classification layers\n",
    "        fusion_dim = hidden_dim * (4 + 2 + 2)  # 4H from cross + 2H + 2H = 8H total\n",
    "        fc_layers = []\n",
    "        in_dim = fusion_dim\n",
    "        out_dim = hidden_dim * 2\n",
    "\n",
    "        for i in range(num_fc_layers):\n",
    "            fc_layers.extend([\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            # for next layer\n",
    "            in_dim = out_dim\n",
    "            # Optionally shrink layer width gradually:\n",
    "            out_dim = max(hidden_dim, out_dim // 2)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "        self.classifier = nn.Linear(in_dim, 2)\n",
    "        self.unk_id = UNK_ID\n",
    "        self.unk_prob = unk_prob\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Token corruption (word dropout)\n",
    "        if self.training and self.unk_prob > 0:\n",
    "            mask = (x != PAD_ID) & (x != SEP_ID) & (x != CLS_ID) & (x != UNK_ID)\n",
    "            replace_mask = (torch.rand_like(x.float()) < self.unk_prob) & mask\n",
    "            x = torch.where(replace_mask, torch.full_like(x, self.unk_id), x)\n",
    "\n",
    "        # Split into premise/hypothesis \n",
    "        sep_positions = (x == SEP_ID).int().argmax(dim=1)\n",
    "        batch_idx = torch.arange(x.size(0), device=x.device)\n",
    "        token_range = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        premise_mask = token_range < sep_positions.unsqueeze(1)\n",
    "        hypothesis_mask = token_range > sep_positions.unsqueeze(1)\n",
    "        premise = torch.where(premise_mask, x, PAD_ID)\n",
    "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
    "\n",
    "        #  Embeddings\n",
    "        embedded = self.embed_dropout(self.embedding(x).transpose(1,2)).transpose(1,2)\n",
    "        emb_prem = self.embed_dropout(self.embedding(premise).transpose(1,2)).transpose(1,2)\n",
    "        emb_hypo = self.embed_dropout(self.embedding(hypothesis).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "        # Encode all three streams\n",
    "        full_out, _ = self.gru(embedded)\n",
    "        prem_out, _ = self.gru(emb_prem)\n",
    "        hypo_out, _ = self.gru(emb_hypo)\n",
    "\n",
    "        full_out = self.norm(full_out)\n",
    "        prem_out, _ = self.prem_encoder(emb_prem)\n",
    "        hypo_out, _ = self.hypo_encoder(emb_hypo)\n",
    "\n",
    "        # # Wide stream (cross-attention over full sequence)\n",
    "        # Q = self.cross_attn_query(full_out)\n",
    "        # K = self.cross_attn_key(full_out)\n",
    "        # V = self.cross_attn_value(full_out)\n",
    "        # attn_scores = torch.bmm(Q, K.transpose(1, 2)) / (Q.size(-1) ** 0.5)\n",
    "        # attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        # cross_context = torch.bmm(attn_weights, V).mean(dim=1)  # (B, hidden_dim)\n",
    "\n",
    "        # --- Bidirectional cross-attention between premise and hypothesis ---\n",
    "        # Project to query/key/value spaces\n",
    "        Q_p = self.cross_attn_query(prem_out)   # (B, Lp, H)\n",
    "        K_h = self.cross_attn_key(hypo_out)\n",
    "        V_h = self.cross_attn_value(hypo_out)\n",
    "\n",
    "        Q_h = self.cross_attn_query(hypo_out)   # (B, Lh, H)\n",
    "        K_p = self.cross_attn_key(prem_out)\n",
    "        V_p = self.cross_attn_value(prem_out)\n",
    "\n",
    "        # Premise â†’ Hypothesis attention (H attends to P)\n",
    "        scores_h2p = torch.bmm(Q_h, K_p.transpose(1, 2)) / (Q_h.size(-1) ** 0.5)\n",
    "        weights_h2p = F.softmax(scores_h2p, dim=-1)\n",
    "        context_h2p = torch.bmm(weights_h2p, V_p)  # (B, Lh, H)\n",
    "\n",
    "        # Hypothesis â†’ Premise attention (P attends to H)\n",
    "        scores_p2h = torch.bmm(Q_p, K_h.transpose(1, 2)) / (Q_p.size(-1) ** 0.5)\n",
    "        weights_p2h = F.softmax(scores_p2h, dim=-1)\n",
    "        context_p2h = torch.bmm(weights_p2h, V_h)  # (B, Lp, H)\n",
    "\n",
    "        # Pool both directions\n",
    "        context_h2p = context_h2p.mean(dim=1)  # (B, H)\n",
    "        context_p2h = context_p2h.mean(dim=1)  # (B, H)\n",
    "\n",
    "        # Fuse bidirectional cross contexts\n",
    "        cross_context = torch.cat([\n",
    "            context_p2h,                   # Pâ†’H\n",
    "            context_h2p,                   # Hâ†’P\n",
    "            torch.abs(context_p2h - context_h2p),  # difference\n",
    "            context_p2h * context_h2p              # element-wise product\n",
    "        ], dim=1)  # (B, 4H)\n",
    "\n",
    "        # Deep streams (self-attention pooling)\n",
    "        def self_attention_pooling(out):\n",
    "            weights = F.softmax(self.self_attn(out), dim=1)  # (B, L, 1)\n",
    "            return torch.sum(weights * out, dim=1)           # (B, 2H)\n",
    "\n",
    "        prem_context = self_attention_pooling(prem_out)\n",
    "        hypo_context = self_attention_pooling(hypo_out)\n",
    "\n",
    "        # Fusion\n",
    "        fused = torch.cat([cross_context, prem_context, hypo_context], dim=1)\n",
    "        fused = self.fc_layers(fused)\n",
    "        logits = self.classifier(fused)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "contain_UNK = np.zeros(len(plain_valid), dtype=bool)\n",
    "for i, sent in enumerate(plain_valid):\n",
    "    if UNK_ID in sent:\n",
    "        contain_UNK[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAdditiveAttention(nn.Module):\n",
    "    \"\"\"Transformer with additive attention over all tokens\"\"\"\n",
    "    def __init__(self, embedding_matrix, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.d_model = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=PAD_ID)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, hyperparameters.max_combined_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        \n",
    "        self.additive_attn = AdditiveAttention(embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        padding_mask = (x == PAD_ID)\n",
    "        embedded = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "        encoded = self.transformer(pos_encoded, src_key_padding_mask=padding_mask)\n",
    "        context, _ = self.additive_attn(encoded)\n",
    "        logits = self.classifier(context)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerCrossAttention(nn.Module):\n",
    "    \"\"\"Transformer with cross attention between streams\"\"\"\n",
    "    def __init__(self, embedding_matrix, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.d_model = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=PAD_ID)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, hyperparameters.max_combined_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.premise_transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        self.hypothesis_transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        \n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=n_heads, batch_first=True\n",
    "        )\n",
    "        self.classifier = nn.Linear(embed_dim * 2, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Split input\n",
    "        sep_pos = (x == SEP_ID).int().argmax(dim=1)\n",
    "        premise_mask = torch.arange(x.size(1), device=x.device).unsqueeze(0) < sep_pos.unsqueeze(1)\n",
    "        hypothesis_mask = ~premise_mask & (x != PAD_ID)\n",
    "        \n",
    "        premise = torch.where(premise_mask, x, PAD_ID)\n",
    "        hypothesis = torch.where(hypothesis_mask, x, PAD_ID)\n",
    "        \n",
    "        # Encode separately\n",
    "        prem_emb = self.embedding(premise) * math.sqrt(self.d_model)\n",
    "        hypo_emb = self.embedding(hypothesis) * math.sqrt(self.d_model)\n",
    "        \n",
    "        prem_encoded = self.premise_transformer(\n",
    "            self.pos_encoder(prem_emb), \n",
    "            src_key_padding_mask=(premise == PAD_ID)\n",
    "        )\n",
    "        hypo_encoded = self.hypothesis_transformer(\n",
    "            self.pos_encoder(hypo_emb),\n",
    "            src_key_padding_mask=(hypothesis == PAD_ID)\n",
    "        )\n",
    "        \n",
    "        # Cross attention\n",
    "        cross_out, _ = self.cross_attn(\n",
    "            hypo_encoded, prem_encoded, prem_encoded,\n",
    "            key_padding_mask=(premise == PAD_ID)\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] tokens\n",
    "        prem_cls = prem_encoded[:, 0, :]\n",
    "        cross_cls = cross_out[:, 0, :]\n",
    "        combined = torch.cat([prem_cls, cross_cls], dim=1)\n",
    "        \n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMultiHead(nn.Module):\n",
    "    \"\"\"Standard transformer with multi-head self-attention\"\"\"\n",
    "    def __init__(self, embedding_matrix, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.d_model = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=PAD_ID)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, hyperparameters.max_combined_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        self.classifier = nn.Linear(embed_dim, 2)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        padding_mask = (x == PAD_ID)\n",
    "        embedded = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "        encoded = self.transformer(pos_encoded, src_key_padding_mask=padding_mask)\n",
    "        cls_output = encoded[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model, train_loader, optimizer, criterion):\n",
    "    \"\"\"Train for exactly 1 epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    \n",
    "    for batch_idx, (xb, mb, yb) in enumerate(train_loader):\n",
    "        xb, mb, yb = xb.to(device), mb.to(device), yb.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb, mb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        total_correct += (preds == yb).sum().item()\n",
    "        total_samples += yb.size(0)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            batch_acc = (preds == yb).float().mean().item()\n",
    "            print(f\"   Batch {batch_idx}: Loss = {loss.item():.4f}, Acc = {batch_acc:.4f}\")\n",
    "    \n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_acc = total_correct / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for xb, mb, yb in data_loader:\n",
    "        xb, mb, yb = xb.to(device), mb.to(device), yb.to(device)\n",
    "        logits = model(xb, mb)\n",
    "        loss = criterion(logits, yb)\n",
    "        \n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        total_correct += (preds == yb).sum().item()\n",
    "        total_samples += yb.size(0)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return total_loss / total_samples, accuracy, f1\n",
    "\n",
    "def test_model_performance(model, test_loader):\n",
    "    \"\"\"Comprehensive model testing\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, mb, yb in test_loader:\n",
    "            xb, mb, yb = xb.to(device), mb.to(device), yb.to(device)\n",
    "            logits = model(xb, mb)\n",
    "            preds = logits.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    print(f\"ðŸ“Š Test Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['neutral', 'entails']))\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ STARTING COMPLETE MODEL TRAINING (1 EPOCH EACH)\n",
      "\n",
      "============================================================\n",
      "ðŸš€ TRAINING: GRU_Additive\n",
      "============================================================\n",
      "   Batch 0: Loss = 0.6883, Acc = 0.7344\n",
      "   Batch 10: Loss = 0.6925, Acc = 0.6406\n",
      "   Batch 20: Loss = 0.6899, Acc = 0.6250\n",
      "   Batch 30: Loss = 0.6917, Acc = 0.6250\n",
      "   Batch 40: Loss = 0.6885, Acc = 0.7031\n",
      "   Batch 50: Loss = 0.6938, Acc = 0.5938\n",
      "   Batch 60: Loss = 0.6973, Acc = 0.5781\n",
      "   Batch 70: Loss = 0.6921, Acc = 0.5938\n",
      "   Batch 80: Loss = 0.7001, Acc = 0.4688\n",
      "   Batch 90: Loss = 0.6914, Acc = 0.5781\n",
      "   Batch 100: Loss = 0.6914, Acc = 0.7031\n",
      "   Batch 110: Loss = 0.6908, Acc = 0.7031\n",
      "   Batch 120: Loss = 0.6916, Acc = 0.6875\n",
      "   Batch 130: Loss = 0.6904, Acc = 0.6406\n",
      "   Batch 140: Loss = 0.6958, Acc = 0.5156\n",
      "   Batch 150: Loss = 0.6921, Acc = 0.5469\n",
      "   Batch 160: Loss = 0.6906, Acc = 0.6094\n",
      "   Batch 170: Loss = 0.6905, Acc = 0.5938\n",
      "   Batch 180: Loss = 0.6866, Acc = 0.6875\n",
      "   Batch 190: Loss = 0.6906, Acc = 0.6250\n",
      "   Batch 200: Loss = 0.6859, Acc = 0.7031\n",
      "   Batch 210: Loss = 0.6879, Acc = 0.6406\n",
      "   Batch 220: Loss = 0.6908, Acc = 0.4688\n",
      "   Batch 230: Loss = 0.6883, Acc = 0.6406\n",
      "   Batch 240: Loss = 0.6918, Acc = 0.5000\n",
      "   Batch 250: Loss = 0.6889, Acc = 0.5156\n",
      "   Batch 260: Loss = 0.6865, Acc = 0.5156\n",
      "   Batch 270: Loss = 0.6879, Acc = 0.4688\n",
      "   Batch 280: Loss = 0.6899, Acc = 0.4062\n",
      "   Batch 290: Loss = 0.6878, Acc = 0.4688\n",
      "   Batch 300: Loss = 0.6919, Acc = 0.5156\n",
      "   Batch 310: Loss = 0.6816, Acc = 0.5781\n",
      "   Batch 320: Loss = 0.6945, Acc = 0.4062\n",
      "   Batch 330: Loss = 0.6880, Acc = 0.5312\n",
      "   Batch 340: Loss = 0.6898, Acc = 0.4688\n",
      "   Batch 350: Loss = 0.6889, Acc = 0.4688\n",
      "âœ… GRU_Additive Results:\n",
      "   Train Loss: 0.6905, Train Acc: 0.5752\n",
      "   Val Loss: 0.6875, Val Acc: 0.5676, Val F1: 0.6298\n",
      "ðŸ“Š Test Accuracy: 0.5064, F1 Score: 0.5124\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.64      0.41      0.50      1275\n",
      "     entails       0.42      0.65      0.51       842\n",
      "\n",
      "    accuracy                           0.51      2117\n",
      "   macro avg       0.53      0.53      0.51      2117\n",
      "weighted avg       0.55      0.51      0.51      2117\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸš€ TRAINING: GRU_Cross\n",
      "============================================================\n",
      "   Batch 0: Loss = 0.6942, Acc = 0.6250\n",
      "   Batch 10: Loss = 0.6984, Acc = 0.5781\n",
      "   Batch 20: Loss = 0.6945, Acc = 0.5781\n",
      "   Batch 30: Loss = 0.6964, Acc = 0.5469\n",
      "   Batch 40: Loss = 0.6933, Acc = 0.5938\n",
      "   Batch 50: Loss = 0.6926, Acc = 0.6250\n",
      "   Batch 60: Loss = 0.6919, Acc = 0.5000\n",
      "   Batch 70: Loss = 0.6919, Acc = 0.6875\n",
      "   Batch 80: Loss = 0.6939, Acc = 0.5469\n",
      "   Batch 90: Loss = 0.6927, Acc = 0.6562\n",
      "   Batch 100: Loss = 0.6920, Acc = 0.6406\n",
      "   Batch 110: Loss = 0.6917, Acc = 0.6406\n",
      "   Batch 120: Loss = 0.6937, Acc = 0.5938\n",
      "   Batch 130: Loss = 0.6901, Acc = 0.6719\n",
      "   Batch 140: Loss = 0.6959, Acc = 0.5625\n",
      "   Batch 150: Loss = 0.6900, Acc = 0.7031\n",
      "   Batch 160: Loss = 0.6899, Acc = 0.6719\n",
      "   Batch 170: Loss = 0.6932, Acc = 0.5781\n",
      "   Batch 180: Loss = 0.6927, Acc = 0.6406\n",
      "   Batch 190: Loss = 0.6918, Acc = 0.6250\n",
      "   Batch 200: Loss = 0.6965, Acc = 0.5625\n",
      "   Batch 210: Loss = 0.6926, Acc = 0.6250\n",
      "   Batch 220: Loss = 0.6902, Acc = 0.6719\n",
      "   Batch 230: Loss = 0.6911, Acc = 0.6719\n",
      "   Batch 240: Loss = 0.6915, Acc = 0.6094\n",
      "   Batch 250: Loss = 0.6920, Acc = 0.6562\n",
      "   Batch 260: Loss = 0.6903, Acc = 0.6875\n",
      "   Batch 270: Loss = 0.6917, Acc = 0.5938\n",
      "   Batch 280: Loss = 0.6913, Acc = 0.6250\n",
      "   Batch 290: Loss = 0.6886, Acc = 0.6406\n",
      "   Batch 300: Loss = 0.6951, Acc = 0.2812\n",
      "   Batch 310: Loss = 0.6884, Acc = 0.4219\n",
      "   Batch 320: Loss = 0.6930, Acc = 0.3906\n",
      "   Batch 330: Loss = 0.6913, Acc = 0.4375\n",
      "   Batch 340: Loss = 0.6921, Acc = 0.3906\n",
      "   Batch 350: Loss = 0.6915, Acc = 0.4375\n",
      "âœ… GRU_Cross Results:\n",
      "   Train Loss: 0.6920, Train Acc: 0.5924\n",
      "   Val Loss: 0.6889, Val Acc: 0.6060, Val F1: 0.6739\n",
      "ðŸ“Š Test Accuracy: 0.5404, F1 Score: 0.5734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.72      0.38      0.50      1275\n",
      "     entails       0.45      0.78      0.57       842\n",
      "\n",
      "    accuracy                           0.54      2117\n",
      "   macro avg       0.59      0.58      0.54      2117\n",
      "weighted avg       0.62      0.54      0.53      2117\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸš€ TRAINING: GRU_MultiHead\n",
      "============================================================\n",
      "   Batch 0: Loss = 0.6954, Acc = 0.4062\n",
      "   Batch 10: Loss = 0.6878, Acc = 0.4531\n",
      "   Batch 20: Loss = 0.6903, Acc = 0.4531\n",
      "   Batch 30: Loss = 0.6946, Acc = 0.3594\n",
      "   Batch 40: Loss = 0.6967, Acc = 0.3594\n",
      "   Batch 50: Loss = 0.6867, Acc = 0.5156\n",
      "   Batch 60: Loss = 0.6909, Acc = 0.5000\n",
      "   Batch 70: Loss = 0.6975, Acc = 0.4219\n",
      "   Batch 80: Loss = 0.6967, Acc = 0.4688\n",
      "   Batch 90: Loss = 0.6892, Acc = 0.5000\n",
      "   Batch 100: Loss = 0.6887, Acc = 0.5156\n",
      "   Batch 110: Loss = 0.6869, Acc = 0.5156\n",
      "   Batch 120: Loss = 0.6884, Acc = 0.5625\n",
      "   Batch 130: Loss = 0.6869, Acc = 0.5000\n",
      "   Batch 140: Loss = 0.6849, Acc = 0.5781\n",
      "   Batch 150: Loss = 0.6874, Acc = 0.5938\n",
      "   Batch 160: Loss = 0.7082, Acc = 0.4062\n",
      "   Batch 170: Loss = 0.6826, Acc = 0.5938\n",
      "   Batch 180: Loss = 0.6782, Acc = 0.7188\n",
      "   Batch 190: Loss = 0.6845, Acc = 0.6406\n",
      "   Batch 200: Loss = 0.6813, Acc = 0.6562\n",
      "   Batch 210: Loss = 0.6797, Acc = 0.5938\n",
      "   Batch 220: Loss = 0.6848, Acc = 0.5781\n",
      "   Batch 230: Loss = 0.6686, Acc = 0.6406\n",
      "   Batch 240: Loss = 0.6902, Acc = 0.5000\n",
      "   Batch 250: Loss = 0.6740, Acc = 0.6250\n",
      "   Batch 260: Loss = 0.6921, Acc = 0.5000\n",
      "   Batch 270: Loss = 0.6859, Acc = 0.5625\n",
      "   Batch 280: Loss = 0.6767, Acc = 0.5781\n",
      "   Batch 290: Loss = 0.6564, Acc = 0.6406\n",
      "   Batch 300: Loss = 0.6640, Acc = 0.6719\n",
      "   Batch 310: Loss = 0.6667, Acc = 0.5781\n",
      "   Batch 320: Loss = 0.6846, Acc = 0.5781\n",
      "   Batch 330: Loss = 0.6827, Acc = 0.5156\n",
      "   Batch 340: Loss = 0.6606, Acc = 0.7344\n",
      "   Batch 350: Loss = 0.6550, Acc = 0.7500\n",
      "âœ… GRU_MultiHead Results:\n",
      "   Train Loss: 0.6842, Train Acc: 0.5444\n",
      "   Val Loss: 0.6778, Val Acc: 0.5883, Val F1: 0.5677\n",
      "ðŸ“Š Test Accuracy: 0.5456, F1 Score: 0.4407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.63      0.61      0.62      1275\n",
      "     entails       0.43      0.45      0.44       842\n",
      "\n",
      "    accuracy                           0.55      2117\n",
      "   macro avg       0.53      0.53      0.53      2117\n",
      "weighted avg       0.55      0.55      0.55      2117\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸš€ TRAINING: Transformer_Additive\n",
      "============================================================\n",
      "   Batch 0: Loss = 0.7040, Acc = 0.4844\n",
      "   Batch 10: Loss = 0.6935, Acc = 0.5625\n",
      "   Batch 20: Loss = 0.6802, Acc = 0.5312\n",
      "   Batch 30: Loss = 0.6741, Acc = 0.6406\n",
      "   Batch 40: Loss = 0.7099, Acc = 0.3125\n",
      "   Batch 50: Loss = 0.6802, Acc = 0.5469\n",
      "   Batch 60: Loss = 0.6830, Acc = 0.6250\n",
      "   Batch 70: Loss = 0.6617, Acc = 0.6250\n",
      "   Batch 80: Loss = 0.6647, Acc = 0.6406\n",
      "   Batch 90: Loss = 0.6760, Acc = 0.5938\n",
      "   Batch 100: Loss = 0.7026, Acc = 0.5625\n",
      "   Batch 110: Loss = 0.6656, Acc = 0.6406\n",
      "   Batch 120: Loss = 0.6733, Acc = 0.6406\n",
      "   Batch 130: Loss = 0.6843, Acc = 0.5312\n",
      "   Batch 140: Loss = 0.6741, Acc = 0.6250\n",
      "   Batch 150: Loss = 0.6549, Acc = 0.6562\n",
      "   Batch 160: Loss = 0.6467, Acc = 0.6562\n",
      "   Batch 170: Loss = 0.6338, Acc = 0.5781\n",
      "   Batch 180: Loss = 0.6327, Acc = 0.6406\n",
      "   Batch 190: Loss = 0.6695, Acc = 0.5469\n",
      "   Batch 200: Loss = 0.6631, Acc = 0.6250\n",
      "   Batch 210: Loss = 0.6693, Acc = 0.5312\n",
      "   Batch 220: Loss = 0.6382, Acc = 0.6250\n",
      "   Batch 230: Loss = 0.5664, Acc = 0.7031\n",
      "   Batch 240: Loss = 0.5937, Acc = 0.7031\n",
      "   Batch 250: Loss = 0.6661, Acc = 0.6250\n",
      "   Batch 260: Loss = 0.7293, Acc = 0.5000\n",
      "   Batch 270: Loss = 0.6812, Acc = 0.7031\n",
      "   Batch 280: Loss = 0.6446, Acc = 0.6406\n",
      "   Batch 290: Loss = 0.5859, Acc = 0.6875\n",
      "   Batch 300: Loss = 0.5855, Acc = 0.7969\n",
      "   Batch 310: Loss = 0.7207, Acc = 0.4844\n",
      "   Batch 320: Loss = 0.6296, Acc = 0.6250\n",
      "   Batch 330: Loss = 0.6140, Acc = 0.6406\n",
      "   Batch 340: Loss = 0.5328, Acc = 0.6875\n",
      "   Batch 350: Loss = 0.5724, Acc = 0.7344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msi\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transformer_Additive Results:\n",
      "   Train Loss: 0.6509, Train Acc: 0.6103\n",
      "   Val Loss: 0.6910, Val Acc: 0.5576, Val F1: 0.4331\n",
      "ðŸ“Š Test Accuracy: 0.5966, F1 Score: 0.3569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.63      0.80      0.71      1275\n",
      "     entails       0.49      0.28      0.36       842\n",
      "\n",
      "    accuracy                           0.60      2117\n",
      "   macro avg       0.56      0.54      0.53      2117\n",
      "weighted avg       0.57      0.60      0.57      2117\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸš€ TRAINING: Transformer_Cross\n",
      "============================================================\n",
      "   Batch 0: Loss = 0.7477, Acc = 0.4062\n",
      "   Batch 10: Loss = 0.6962, Acc = 0.6094\n",
      "   Batch 20: Loss = 0.7405, Acc = 0.3125\n",
      "   Batch 30: Loss = 0.6764, Acc = 0.5781\n",
      "   Batch 40: Loss = 0.6844, Acc = 0.5781\n",
      "   Batch 50: Loss = 0.6716, Acc = 0.5156\n",
      "   Batch 60: Loss = 0.6633, Acc = 0.6094\n",
      "   Batch 70: Loss = 0.6813, Acc = 0.5938\n",
      "   Batch 80: Loss = 0.6743, Acc = 0.5469\n",
      "   Batch 90: Loss = 0.6750, Acc = 0.4688\n",
      "   Batch 100: Loss = 0.6459, Acc = 0.6406\n",
      "   Batch 110: Loss = 0.6493, Acc = 0.6719\n",
      "   Batch 120: Loss = 0.6537, Acc = 0.7500\n",
      "   Batch 130: Loss = 0.6677, Acc = 0.5469\n",
      "   Batch 140: Loss = 0.6390, Acc = 0.6875\n",
      "   Batch 150: Loss = 0.6465, Acc = 0.7031\n",
      "   Batch 160: Loss = 0.6285, Acc = 0.6562\n",
      "   Batch 170: Loss = 0.6287, Acc = 0.7188\n",
      "   Batch 180: Loss = 0.6094, Acc = 0.7188\n",
      "   Batch 190: Loss = 0.6290, Acc = 0.6562\n",
      "   Batch 200: Loss = 0.6028, Acc = 0.6719\n",
      "   Batch 210: Loss = 0.6902, Acc = 0.5156\n",
      "   Batch 220: Loss = 0.5807, Acc = 0.6562\n",
      "   Batch 230: Loss = 0.6287, Acc = 0.6406\n",
      "   Batch 240: Loss = 0.6093, Acc = 0.6875\n",
      "   Batch 250: Loss = 0.6168, Acc = 0.6094\n",
      "   Batch 260: Loss = 0.5943, Acc = 0.7031\n",
      "   Batch 270: Loss = 0.7046, Acc = 0.5781\n",
      "   Batch 280: Loss = 0.5840, Acc = 0.7344\n",
      "   Batch 290: Loss = 0.5579, Acc = 0.6562\n",
      "   Batch 300: Loss = 0.6091, Acc = 0.6719\n",
      "   Batch 310: Loss = 0.6735, Acc = 0.5312\n",
      "   Batch 320: Loss = 0.6332, Acc = 0.6406\n",
      "   Batch 330: Loss = 0.5619, Acc = 0.7344\n",
      "   Batch 340: Loss = 0.5231, Acc = 0.7656\n",
      "   Batch 350: Loss = 0.5827, Acc = 0.6406\n",
      "âœ… Transformer_Cross Results:\n",
      "   Train Loss: 0.6361, Train Acc: 0.6193\n",
      "   Val Loss: 0.5909, Val Acc: 0.6690, Val F1: 0.6861\n",
      "ðŸ“Š Test Accuracy: 0.6268, F1 Score: 0.5940\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.74      0.59      0.65      1275\n",
      "     entails       0.52      0.69      0.59       842\n",
      "\n",
      "    accuracy                           0.63      2117\n",
      "   macro avg       0.63      0.64      0.62      2117\n",
      "weighted avg       0.65      0.63      0.63      2117\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸš€ TRAINING: Transformer_MultiHead\n",
      "============================================================\n",
      "   Batch 0: Loss = 0.7524, Acc = 0.5312\n",
      "   Batch 10: Loss = 0.7381, Acc = 0.5312\n",
      "   Batch 20: Loss = 0.7270, Acc = 0.4219\n",
      "   Batch 30: Loss = 0.7018, Acc = 0.6250\n",
      "   Batch 40: Loss = 0.6920, Acc = 0.4844\n",
      "   Batch 50: Loss = 0.6832, Acc = 0.4844\n",
      "   Batch 60: Loss = 0.6791, Acc = 0.5781\n",
      "   Batch 70: Loss = 0.6867, Acc = 0.4688\n",
      "   Batch 80: Loss = 0.6979, Acc = 0.5625\n",
      "   Batch 90: Loss = 0.7062, Acc = 0.4844\n",
      "   Batch 100: Loss = 0.7217, Acc = 0.4062\n",
      "   Batch 110: Loss = 0.6472, Acc = 0.6562\n",
      "   Batch 120: Loss = 0.6502, Acc = 0.6250\n",
      "   Batch 130: Loss = 0.6801, Acc = 0.5625\n",
      "   Batch 140: Loss = 0.6577, Acc = 0.6406\n",
      "   Batch 150: Loss = 0.6667, Acc = 0.5312\n",
      "   Batch 160: Loss = 0.6788, Acc = 0.5469\n",
      "   Batch 170: Loss = 0.6736, Acc = 0.5469\n",
      "   Batch 180: Loss = 0.6055, Acc = 0.7344\n",
      "   Batch 190: Loss = 0.6980, Acc = 0.6406\n",
      "   Batch 200: Loss = 0.6421, Acc = 0.6719\n",
      "   Batch 210: Loss = 0.6271, Acc = 0.6250\n",
      "   Batch 220: Loss = 0.6552, Acc = 0.6250\n",
      "   Batch 230: Loss = 0.6874, Acc = 0.6094\n",
      "   Batch 240: Loss = 0.5944, Acc = 0.7500\n",
      "   Batch 250: Loss = 0.5968, Acc = 0.7188\n",
      "   Batch 260: Loss = 0.7432, Acc = 0.5781\n",
      "   Batch 270: Loss = 0.5884, Acc = 0.6406\n",
      "   Batch 280: Loss = 0.6164, Acc = 0.5938\n",
      "   Batch 290: Loss = 0.6590, Acc = 0.6875\n",
      "   Batch 300: Loss = 0.6054, Acc = 0.7188\n",
      "   Batch 310: Loss = 0.5958, Acc = 0.7188\n",
      "   Batch 320: Loss = 0.6086, Acc = 0.6562\n",
      "   Batch 330: Loss = 0.7233, Acc = 0.6094\n",
      "   Batch 340: Loss = 0.5942, Acc = 0.6406\n",
      "   Batch 350: Loss = 0.5532, Acc = 0.7031\n",
      "âœ… Transformer_MultiHead Results:\n",
      "   Train Loss: 0.6519, Train Acc: 0.6088\n",
      "   Val Loss: 0.7231, Val Acc: 0.5845, Val F1: 0.5316\n",
      "ðŸ“Š Test Accuracy: 0.6117, F1 Score: 0.5000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.67      0.69      0.68      1275\n",
      "     entails       0.51      0.49      0.50       842\n",
      "\n",
      "    accuracy                           0.61      2117\n",
      "   macro avg       0.59      0.59      0.59      2117\n",
      "weighted avg       0.61      0.61      0.61      2117\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ðŸŽŠ FINAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "GRU_Additive         | Val Acc: 0.5676 | Test Acc: 0.5064 | Test F1: 0.5124\n",
      "GRU_Cross            | Val Acc: 0.6060 | Test Acc: 0.5404 | Test F1: 0.5734\n",
      "GRU_MultiHead        | Val Acc: 0.5883 | Test Acc: 0.5456 | Test F1: 0.4407\n",
      "Transformer_Additive | Val Acc: 0.5576 | Test Acc: 0.5966 | Test F1: 0.3569\n",
      "Transformer_Cross    | Val Acc: 0.6690 | Test Acc: 0.6268 | Test F1: 0.5940\n",
      "Transformer_MultiHead | Val Acc: 0.5845 | Test Acc: 0.6117 | Test F1: 0.5000\n"
     ]
    }
   ],
   "source": [
    "def train_all_models():\n",
    "    \"\"\"Train all 6 models for 1 epoch each\"\"\"\n",
    "    models = {\n",
    "        'GRU_Additive': GRUAdditiveAttention(vocab_size, hyperparameters.embedding_dim, hyperparameters.hidden_dim),\n",
    "        'GRU_Cross': GRUCrossAttention(vocab_size, hyperparameters.embedding_dim, hyperparameters.hidden_dim),\n",
    "        'GRU_MultiHead': GRUMultiHeadAttention(vocab_size, hyperparameters.embedding_dim, hyperparameters.hidden_dim),\n",
    "        'Transformer_Additive': TransformerAdditiveAttention(embedding_matrix, hyperparameters.n_heads, hyperparameters.n_layers),\n",
    "        'Transformer_Cross': TransformerCrossAttention(embedding_matrix, hyperparameters.n_heads, hyperparameters.n_layers),\n",
    "        'Transformer_MultiHead': TransformerMultiHead(embedding_matrix, hyperparameters.n_heads, hyperparameters.n_layers)\n",
    "    }\n",
    "    \n",
    "    # Calculate class weights for balanced loss\n",
    "    y_train_np = y_train.numpy()\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸš€ TRAINING: {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparameters.lr, weight_decay=hyperparameters.wd)\n",
    "        \n",
    "     \n",
    "        train_loss, train_acc = train_single_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc, val_f1 = evaluate_model(model, valid_loader, criterion)\n",
    "        \n",
    "        print(f\"âœ… {name} Results:\")\n",
    "        print(f\"   Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"   Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Test on test set\n",
    "        test_acc, test_f1 = test_model_performance(model, test_loader)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc, \n",
    "            'val_f1': val_f1,\n",
    "            'test_acc': test_acc,\n",
    "            'test_f1': test_f1\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the complete training\n",
    "print(\"ðŸŽ¯ STARTING COMPLETE MODEL TRAINING \")\n",
    "final_results = train_all_models()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ðŸŽŠ FINAL RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "for name, result in final_results.items():\n",
    "    print(f\"{name:20} | Val Acc: {result['val_acc']:.4f} | Test Acc: {result['test_acc']:.4f} | Test F1: {result['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘ï¸  Visualizing prediction for best model: Transformer_Cross\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m final_results[best_model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ‘ï¸  Visualizing prediction for best model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m     plot_attention_visualization(best_model, sample_combined, best_model_name)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Fallback: test with first available model\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ‘ï¸  Testing visualization with GRU_Additive model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m, in \u001b[0;36mplot_attention_visualization\u001b[1;34m(model, sample_text, model_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Visualize attention weights for a sample\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Convert sample text to model input\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tokens \u001b[38;5;241m=\u001b[39m clean_text(sample_text)\n\u001b[0;32m      5\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m [CLS_ID] \u001b[38;5;241m+\u001b[39m [word2id\u001b[38;5;241m.\u001b[39mget(token, UNK_ID) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens] \u001b[38;5;241m+\u001b[39m [SEP_ID]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Pad to max length\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "# STILL FIXING\n",
    "\n",
    "#def plot_attention_visualization(model, sample_text, model_name):\n",
    "#     \"\"\"Visualize attention weights for a sample\"\"\"\n",
    "#     # Convert sample text to model input\n",
    "#     tokens = clean_text(sample_text)\n",
    "#     token_ids = [CLS_ID] + [word2id.get(token, UNK_ID) for token in tokens] + [SEP_ID]\n",
    "    \n",
    "#     # Pad to max length\n",
    "#     if len(token_ids) < hyperparameters.max_combined_len:\n",
    "#         token_ids += [PAD_ID] * (hyperparameters.max_combined_len - len(token_ids))\n",
    "#     else:\n",
    "#         token_ids = token_ids[:hyperparameters.max_combined_len]\n",
    "    \n",
    "#     # Create model input\n",
    "#     input_tensor = torch.LongTensor(token_ids).unsqueeze(0).to(device)\n",
    "#     mask_tensor = torch.LongTensor([1] * len(token_ids) + [0] * (hyperparameters.max_combined_len - len(token_ids))).unsqueeze(0).to(device)\n",
    "    \n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # FIX: Our models don't have return_weights parameter\n",
    "#         logits = model(input_tensor, mask_tensor)\n",
    "#         attn_weights = None\n",
    "    \n",
    "#     # Simple visualization - just show prediction for now\n",
    "#     plt.figure(figsize=(10, 2))\n",
    "    \n",
    "#     # Create a simple bar chart showing prediction confidence\n",
    "#     probs = torch.softmax(logits, 1).squeeze().cpu().numpy()\n",
    "#     classes = ['neutral', 'entails']\n",
    "    \n",
    "#     plt.bar(classes, probs, color=['red', 'green'], alpha=0.7)\n",
    "#     plt.ylim(0, 1)\n",
    "#     plt.title(f'Prediction Confidence - {model_name}')\n",
    "#     plt.ylabel('Probability')\n",
    "    \n",
    "#     for i, prob in enumerate(probs):\n",
    "#         plt.text(i, prob + 0.02, f'{prob:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "#     predicted_class = 'entails' if logits.argmax(1).item() == 1 else 'neutral'\n",
    "#     plt.suptitle(f'Prediction: {predicted_class} | Confidence: {probs.max():.3f}')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Print tokenized input for reference\n",
    "#     print(f\"ðŸ“ Tokenized input: {tokens}\")\n",
    "#     print(f\"ðŸŽ¯ Raw logits: {logits.squeeze().cpu().numpy()}\")\n",
    "\n",
    "# # Example visualization\n",
    "# sample_premise = \"When waves of two different frequencies interfere, beating occurs.\"\n",
    "# sample_hypothesis = \"Beats occur when sound waves interfere\"\n",
    "# sample_combined = sample_premise + \" \" + sample_hypothesis\n",
    "\n",
    "# # Visualize for best model\n",
    "# if 'final_results' in locals() and final_results:\n",
    "#     best_model_name = max(final_results.items(), key=lambda x: x[1]['test_acc'])[0]\n",
    "#     best_model = final_results[best_model_name]['model']\n",
    "#     print(f\"\\nðŸ‘ï¸  Visualizing prediction for best model: {best_model_name}\")\n",
    "#     plot_attention_visualization(best_model, sample_combined, best_model_name)\n",
    "# else:\n",
    "#     # Fallback: test with first available model\n",
    "#     print(f\"\\nðŸ‘ï¸  Testing visualization with GRU_Additive model\")\n",
    "#     test_model = GRUAdditiveAttention(vocab_size, hyperparameters.embedding_dim, hyperparameters.hidden_dim).to(device)\n",
    "#     plot_attention_visualization(test_model, sample_combined, \"GRU_Additive (Untrained)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_models(gru=False, lstm=False, transformer=False, monstruosity=False):\n",
    "    \n",
    "    models = {} \n",
    "\n",
    "    # Build Gru Attention\n",
    "    if gru:\n",
    "        models['gru'] = GRUAttentionModel(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=hyperparameters.embedding_dim,\n",
    "            hidden_dim=hyperparameters.hidden_dim,\n",
    "            dropout=hyperparameters.dropout,\n",
    "            unk_prob=0.7,\n",
    "            num_layers=hyperparameters.n_layers\n",
    "        ).to(device)\n",
    "    \n",
    "    # Build LSTM with Attention\n",
    "    if lstm:\n",
    "        models['lstm'] = LSTMAttentionModel(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=hyperparameters.embedding_dim,\n",
    "            hidden_dim=hyperparameters.hidden_dim,\n",
    "            dropout=hyperparameters.dropout,\n",
    "            num_layers=hyperparameters.n_layers\n",
    "        ).to(device)\n",
    "\n",
    "    # Build Transformer \n",
    "    if transformer:\n",
    "        models['transformer'] = TransformerClassifier(\n",
    "            embedding_matrix=embedding_matrix,\n",
    "            num_classes=2,\n",
    "            n_heads=hyperparameters.n_heads,\n",
    "            n_layers=hyperparameters.n_layers,\n",
    "            dropout=hyperparameters.dropout\n",
    "        ).to(device) \n",
    "\n",
    "    if monstruosity:\n",
    "        models['monstruosity'] = GRUThreeStreamNLI(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=hyperparameters.embedding_dim,\n",
    "            hidden_dim=hyperparameters.hidden_dim, \n",
    "            wide_layers=2, \n",
    "            prem_layers=4, \n",
    "            hypo_layers=2, \n",
    "            num_fc_layers=2,\n",
    "            embedding_dropout=0.2,\n",
    "            dropout=0.5, \n",
    "            unk_prob=0.1\n",
    "        ).to(device)\n",
    "\n",
    "    return models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = instantiate_models(gru=True, lstm=True, transformer=True, monstruosity=True)\n",
    "#transformer_model = instantiate_models(transformer=True)\n",
    "#lstm_model = instantiate_models(lstm=True)\n",
    "#monstruosity_model = instantiate_models(monstruosity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Global Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for xb, mb, yb in loader: # x_train, train_mask, y_train\n",
    "        xb,mb,yb = xb.to(device), mb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb, mb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)# Gradient clipping, prevents gradients to become to large \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        total_correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return total_loss/total, total_correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for xb, mb, yb in loader:\n",
    "        xb, mb, yb = xb.to(device), mb.to(device), yb.to(device).long()\n",
    "        out = model(xb, mb)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "        loss = criterion(logits, yb)\n",
    "        batch_size = yb.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == yb).sum().item()\n",
    "        total += batch_size\n",
    "\n",
    "    return total_loss / total, total_correct / total\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, optimizer, n_epochs=10, patience=3,criterion=nn.CrossEntropyLoss()):\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer,criterion)\n",
    "        val_loss, val_acc = evaluate_model(model, valid_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - \"\n",
    "              f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f} - \"\n",
    "              f\"Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def train_models(models_to_train):\n",
    "    trained_models = {}\n",
    "    histories = {}\n",
    "\n",
    "    # Calculate class weights\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "    weights = compute_class_weight('balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float).to(device))\n",
    "\n",
    "    # Iterate through the models that were passed in\n",
    "    for name, model in models_to_train.items():\n",
    "        print(f\"--- Training {name.upper()} Model ---\")\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparameters.lr, weight_decay=hyperparameters.wd)\n",
    "        \n",
    "        trained_model, history = train_model(\n",
    "            model, \n",
    "            train_loader, \n",
    "            valid_loader, \n",
    "            optimizer, \n",
    "            n_epochs=hyperparameters.train_num_epoch, \n",
    "            patience=5, \n",
    "            criterion=criterion\n",
    "        )\n",
    "        \n",
    "        trained_models[name] = trained_model\n",
    "        histories[name] = history\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return trained_models, histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can train one like : train_models(gru=True)\n",
    "\n",
    "trained_models, histories = train_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model acuracy in samples with and without UNK tokens \n",
    "def analyze_unk_performance(model, data_loader, plain_data):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Get all predictions and labels\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks, labels in data_loader:\n",
    "            outputs = model(inputs.to(device), masks.to(device))\n",
    "            preds = torch.max(outputs, dim=1)[1]\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    correct = (predictions == all_labels)\n",
    "\n",
    "    # Check which original sentences contain the UNK_ID\n",
    "    contain_UNK = np.array([UNK_ID in sent for sent in plain_data])\n",
    "\n",
    "    # Calculate accuracy for sentences with UNK tokens\n",
    "    unk_correct = correct[contain_UNK]\n",
    "    unk_accuracy = unk_correct.mean() if len(unk_correct) > 0 else 0\n",
    "\n",
    "    # Calculate accuracy for sentences without UNK tokens\n",
    "    no_unk_correct = correct[~contain_UNK]\n",
    "    no_unk_accuracy = no_unk_correct.mean() if len(no_unk_correct) > 0 else 0\n",
    "\n",
    "    print(\"--- UNK Token Analysis ---\")\n",
    "    print(f\"Accuracy on samples WITH [UNK] tokens:   {unk_accuracy:.4f} ({len(unk_correct)} samples)\")\n",
    "    print(f\"Accuracy on samples WITHOUT [UNK] tokens: {no_unk_accuracy:.4f} ({len(no_unk_correct)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def model_validation(model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in test_loader:\n",
    "            input_ids      = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels         = labels.to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"Accuracy:\", round(accuracy_score(all_labels, all_preds), 2))\n",
    "    print(f\"F1 Score: {f1_score(all_labels, all_preds):.2f}\")\n",
    "    print(\"Precision:\", round(precision_score(all_labels, all_preds), 2))\n",
    "    print(\"Recall:\", round(recall_score(all_labels, all_preds), 2))\n",
    "    print(\"-\" * 40)\n",
    "    print(classification_report(all_labels, all_preds, target_names=[\"neutral\", \"entails\"]))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    analyze_unk_performance(model, test_loader, plain_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models(trained_models):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL MODEL EVALUATION ON TEST SET\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    for model_name, model in trained_models.items():\n",
    "        print(f\"--- Evaluating {model_name.upper()} Model ---\")\n",
    "        model_validation(model)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models that were trained\n",
    "evaluate_all_models(trained_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trainning loss vs the validation loss\n",
    "def plot_training_histories(histories):\n",
    "    if not histories:\n",
    "        print(\"No histories to plot.\")\n",
    "        return\n",
    "\n",
    "    for model_name, history in histories.items():\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(history['train_loss'], label='Training Loss', color='blue', marker='o')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss', color='orange', marker='o')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training & Validation Loss for {model_name.upper()} Model')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_histories(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Attention of the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rnn_attention(tokens, attention_weights):\n",
    "    # 1. Remove padding from tokens for a cleaner plot\n",
    "    try:\n",
    "        real_len = tokens.index('[PAD]')\n",
    "        tokens_no_padding = tokens[:real_len]\n",
    "        weights_no_padding = attention_weights.squeeze().cpu().numpy()[:real_len]\n",
    "    except ValueError:\n",
    "        tokens_no_padding = tokens\n",
    "        weights_no_padding = attention_weights.squeeze().cpu().numpy()\n",
    "\n",
    "    # 2. Create the bar plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=tokens_no_padding, y=weights_no_padding, palette='viridis')\n",
    "    plt.title('RNN Attention Weights')\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Attention Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific Plot for transformer\n",
    "def plot_attention_head(attention_weights, tokens, head_index):\n",
    "    \"\"\"Plots a readable heatmap for a single attention head, ignoring padding.\"\"\"\n",
    "    \n",
    "    # 1. Find the actual length of the sentence by finding the first [PAD] token\n",
    "    try:\n",
    "        real_len = tokens.index('[PAD]')\n",
    "    except ValueError:\n",
    "        real_len = len(tokens)\n",
    "\n",
    "    # 2. Slice the tokens and weights to remove padding\n",
    "    tokens_no_padding = tokens[:real_len]\n",
    "    head_weights = attention_weights[0, head_index][:real_len, :real_len]\n",
    "    \n",
    "    # 3. Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        head_weights, \n",
    "        xticklabels=tokens_no_padding, \n",
    "        yticklabels=tokens_no_padding, \n",
    "        cmap='viridis', \n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'Attention Head #{head_index}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master function to visualize attention\n",
    "def visualize_attention(model_name, model, sample_token_ids, sample_mask):\n",
    "\n",
    "    print(f\"--- Visualizing Attention for {model_name.upper()} Model ---\")\n",
    "    \n",
    "    # Convert token IDs back to words for plotting\n",
    "    plot_tokens = [id2word[int(token_id)] for token_id in sample_token_ids]\n",
    "    \n",
    "    model.eval() # Set model to evaluation mode\n",
    "    \n",
    "    # Add a batch dimension for model input\n",
    "    sample_token_ids = sample_token_ids.unsqueeze(0).to(device)\n",
    "    sample_mask = sample_mask.unsqueeze(0).to(device)\n",
    "\n",
    "    if model_name in ['gru', 'lstm']:\n",
    "        # For RNNs, call forward with return_weights=True\n",
    "        _, attention_weights = model(sample_token_ids, sample_mask, return_weights=True)\n",
    "        plot_rnn_attention(plot_tokens, attention_weights.detach())\n",
    "        \n",
    "    elif model_name == 'transformer':\n",
    "        # For Transformer, call the special get_attention_weights method\n",
    "        # First, ensure the method exists\n",
    "        if not hasattr(model, 'get_attention_weights'):\n",
    "            print(\"Error: The Transformer model needs a 'get_attention_weights' method.\")\n",
    "            return\n",
    "            \n",
    "        attention_weights = model.get_attention_weights(sample_token_ids, sample_mask)\n",
    "        \n",
    "        # Plot the attention for each head\n",
    "        num_heads = attention_weights.shape[1]\n",
    "        for i in range(num_heads):\n",
    "            plot_attention_head(attention_weights.detach().cpu().numpy(), plot_tokens, head_index=i)\n",
    "    else:\n",
    "        print(f\"Attention visualization for model type '{model_name}' is not supported yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample to visualize (e.g., the 5th sentence in the training set)\n",
    "sample_idx = 5\n",
    "sample_tokens = x_train[sample_idx]\n",
    "sample_mask = train_masks[sample_idx]\n",
    "\n",
    "# Visualize attention for the trained GRU model\n",
    "if 'gru' in trained_models:\n",
    "    visualize_attention('gru', trained_models['gru'], sample_tokens, sample_mask)\n",
    "\n",
    "if 'lstm' in trained_models:\n",
    "    visualize_attention('lstm', trained_models['lstm'], sample_tokens, sample_mask)\n",
    "\n",
    "# Visualize attention for the trained Transformer model\n",
    "if 'transformer' in trained_models:\n",
    "    visualize_attention('transformer', trained_models['transformer'], sample_tokens, sample_mask)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
